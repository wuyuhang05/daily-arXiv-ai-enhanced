{"id": "2507.17948", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17948", "abs": "https://arxiv.org/abs/2507.17948", "authors": ["Shubham Mohole", "Hongjun Choi", "Shusen Liu", "Christine Klymko", "Shashank Kushwaha", "Derek Shi", "Wesam Sakla", "Sainyam Galhotra", "Ruben Glatt"], "title": "VERIRAG: Healthcare Claim Verification via Statistical Audit in Retrieval-Augmented Generation", "comment": null, "summary": "Retrieval-augmented generation (RAG) systems are increasingly adopted in\nclinical decision support, yet they remain methodologically blind-they retrieve\nevidence but cannot vet its scientific quality. A paper claiming \"Antioxidant\nproteins decreased after alloferon treatment\" and a rigorous multi-laboratory\nreplication study will be treated as equally credible, even if the former\nlacked scientific rigor or was even retracted. To address this challenge, we\nintroduce VERIRAG, a framework that makes three notable contributions: (i) the\nVeritable, an 11-point checklist that evaluates each source for methodological\nrigor, including data integrity and statistical validity; (ii) a Hard-to-Vary\n(HV) Score, a quantitative aggregator that weights evidence by its quality and\ndiversity; and (iii) a Dynamic Acceptance Threshold, which calibrates the\nrequired evidence based on how extraordinary a claim is. Across four\ndatasets-comprising retracted, conflicting, comprehensive, and settled science\ncorpora-the VERIRAG approach consistently outperforms all baselines, achieving\nabsolute F1 scores ranging from 0.53 to 0.65, representing a 10 to 14 point\nimprovement over the next-best method in each respective dataset. We will\nrelease all materials necessary for reproducing our results.", "AI": {"tldr": "VERIRAG\u6846\u67b6\u901a\u8fc7\u8bc4\u4f30\u6bcf\u4e2a\u8bc1\u636e\u6765\u6e90\u7684\u65b9\u6cd5\u8bba\u4e25\u8c28\u6027\uff0c\u786c\u53d8\u6027\u8bc4\u5206\u548c\u52a8\u6001\u63a5\u53d7\u9608\u503c\uff0c\u6709\u6548\u63d0\u5347\u4e86RAG\u7cfb\u7edf\u5728\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u4e2d\u7684\u6027\u80fd\u3002\u5176\u6d4b\u8bd5\u7ed3\u679c\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u7684RAG\u7cfb\u7edf\u65e0\u6cd5\u8bc4\u4f30\u68c0\u7d22\u5230\u7684\u79d1\u5b66\u8bc1\u636e\u7684\u8d28\u91cf\uff0c\u5bfc\u81f4\u7f3a\u4e4f\u79d1\u5b66\u4e25\u8c28\u6027\u7684\u7814\u7a76\u4e0e\u9ad8\u8d28\u91cf\u7684\u7814\u7a76\u88ab\u7b49\u540c\u5bf9\u5f85\u3002", "method": "\u5f15\u5165\u4e09\u4e2a\u65b0\u7684\u673a\u5236\uff1a\uff08i\uff09\u4e00\u4e2a\u540d\u4e3aVeritable\u768411\u9879\u68c0\u67e5\u8868\uff0c\u7528\u4e8e\u8bc4\u4f30\u6bcf\u4e2a\u8bc1\u636e\u6765\u6e90\u7684\u65b9\u6cd5\u4e25\u683c\u6027\uff1b\uff08ii\uff09\u4e00\u4e2a\u79f0\u4e3aHard-to-Vary (HV) \u7684\u8bc4\u5206\u7cfb\u7edf\uff0c\u7528\u4e8e\u5b9a\u91cf\u805a\u5408\u548c\u52a0\u6743\u8bc1\u636e\u8d28\u91cf\u4e0e\u591a\u6837\u6027\uff1b\uff08iii\uff09\u4e00\u4e2a\u52a8\u6001\u63a5\u53d7\u9608\u503c\uff0c\u6839\u636e\u4e3b\u5f20\u7684\u4e0d\u540c\u5bfb\u5e38\u7a0b\u5ea6\u6821\u51c6\u6240\u9700\u8bc1\u636e\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cVERIRAG\u65b9\u6cd5\u7684F1\u5f97\u5206\u57280.53\u52300.65\u4e4b\u95f4\uff0c\u6bd4\u6b21\u4f18\u65b9\u6cd5\u63d0\u9ad8\u4e8610\u523014\u5206\u3002", "conclusion": "VERIRAG\u5728\u6240\u6709\u57fa\u51c6\u4e0a\u5747\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u8bc4\u4ef7\u8bc1\u636e\u79d1\u5b66\u8d28\u91cf\u7684\u6709\u6548\u6027\u548c\u91cd\u8981\u6027\u3002"}}
{"id": "2507.17976", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2507.17976", "abs": "https://arxiv.org/abs/2507.17976", "authors": ["Maria Vlachou"], "title": "Failure Prediction in Conversational Recommendation Systems", "comment": null, "summary": "In a Conversational Image Recommendation task, users can provide natural\nlanguage feedback on a recommended image item, which leads to an improved\nrecommendation in the next turn. While typical instantiations of this task\nassume that the user's target item will (eventually) be returned, this might\noften not be true, for example, the item the user seeks is not within the item\ncatalogue. Failing to return a user's desired item can lead to user\nfrustration, as the user needs to interact with the system for an increased\nnumber of turns. To mitigate this issue, in this paper, we introduce the task\nof Supervised Conversational Performance Prediction, inspired by Query\nPerformance Prediction (QPP) for predicting effectiveness in response to a\nsearch engine query. In this regard, we propose predictors for conversational\nperformance that detect conversation failures using multi-turn semantic\ninformation contained in the embedded representations of retrieved image items.\nSpecifically, our AutoEncoder-based predictor learns a compressed\nrepresentation of top-retrieved items of the train turns and uses the\nclassification labels to predict the evaluation turn. Our evaluation scenario\naddressed two recommendation scenarios, by differentiating between system\nfailure, where the system is unable to find the target, and catalogue failure,\nwhere the target does not exist in the item catalogue. In our experiments using\nthe Shoes and FashionIQ Dresses datasets, we measure the accuracy of predictors\nfor both system and catalogue failures. Our results demonstrate the promise of\nour proposed predictors for predicting system failures (existing evaluation\nscenario), while we detect a considerable decrease in predictive performance in\nthe case of catalogue failure prediction (when inducing a missing item\nscenario) compared to system failures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAutoEncoder\u7684\u9884\u6d4b\u5668\uff0c\u7528\u4e8e\u4f1a\u8bdd\u56fe\u50cf\u63a8\u8350\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u9884\u6d4b\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u7cfb\u7edf\u5931\u8d25\u9884\u6d4b\u4e0a\u6548\u679c\u4e0d\u9519\uff0c\u4f46\u5728\u76ee\u5f55\u5931\u8d25\u9884\u6d4b\u4e0a\u6548\u679c\u8f83\u5dee\u3002", "motivation": "\u5728\u4f1a\u8bdd\u56fe\u50cf\u63a8\u8350\u4efb\u52a1\u4e2d\uff0c\u7528\u6237\u53ef\u80fd\u4f1a\u9762\u4e34\u5bfb\u627e\u76ee\u6807\u7269\u54c1\u5931\u8d25\u7684\u60c5\u51b5\uff0c\u5bfc\u81f4\u7528\u6237\u9700\u8981\u8fdb\u884c\u66f4\u591a\u4ea4\u4e92\uff0c\u4ece\u800c\u9020\u6210\u632b\u8d25\u611f\u3002\u672c\u6587\u7684\u76ee\u6807\u662f\u8bbe\u8ba1\u4e00\u4e2a\u4efb\u52a1\u6765\u9884\u6d4b\u4f1a\u8bdd\u6027\u80fd\uff0c\u4ee5\u51cf\u5c11\u8fd9\u79cd\u60c5\u51b5\u7684\u53d1\u751f\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eAutoEncoder\u7684\u9884\u6d4b\u5668\uff0c\u901a\u8fc7\u5b66\u4e60\u8bad\u7ec3\u8f6e\u6b21\u4e2d\u6700\u9ad8\u68c0\u7d22\u9879\u76ee\u7684\u538b\u7f29\u8868\u793a\uff0c\u5e76\u7ed3\u5408\u5206\u7c7b\u6807\u7b7e\u6765\u9884\u6d4b\u8bc4\u4f30\u8f6e\u6b21\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u9884\u6d4b\u5668\u5728\u9884\u6d4b\u7cfb\u7edf\u5931\u8d25\u65f6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u662f\u5728\u9884\u6d4b\u76ee\u5f55\u5931\u8d25\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\u63d0\u51fa\u7684\u7528\u4e8e\u9884\u6d4b\u7cfb\u7edf\u5931\u8d25\u7684\u9884\u6d4b\u5668\u5177\u6709\u826f\u597d\u7684\u6548\u679c\uff0c\u4f46\u5728\u76ee\u5f55\u5931\u8d25\u7684\u60c5\u51b5\u4e0b\u9884\u6d4b\u6027\u80fd\u660e\u663e\u4e0b\u964d\u3002"}}
{"id": "2507.18017", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18017", "abs": "https://arxiv.org/abs/2507.18017", "authors": ["Maria Vlachou"], "title": "Fashion-AlterEval: A Dataset for Improved Evaluation of Conversational Recommendation Systems with Alternative Relevant Items", "comment": "arXiv admin note: substantial text overlap with arXiv:2401.05783", "summary": "In Conversational Recommendation Systems (CRS), a user provides feedback on\nrecommended items at each turn, leading the CRS towards improved\nrecommendations. Due to the need for a large amount of data, a user simulator\nis employed for both training and evaluation. Such user simulators critique the\ncurrent retrieved item based on knowledge of a single target item. However,\nsystem evaluation in offline settings with simulators is limited by the focus\non a single target item and their unlimited patience over a large number of\nturns. To overcome these limitations of existing simulators, we propose\nFashion-AlterEval, a new dataset that contains human judgments for a selection\nof alternative items by adding new annotations in common fashion CRS datasets.\nConsequently, we propose two novel meta-user simulators that use the collected\njudgments and allow simulated users not only to express their preferences about\nalternative items to their original target, but also to change their mind and\nlevel of patience. In our experiments using the Shoes and Fashion IQ as the\noriginal datasets and three CRS models, we find that using the knowledge of\nalternatives by the simulator can have a considerable impact on the evaluation\nof existing CRS models, specifically that the existing single-target evaluation\nunderestimates their effectiveness, and when simulatedusers are allowed to\ninstead consider alternative relevant items, the system can rapidly respond to\nmore quickly satisfy the user.", "AI": {"tldr": "\u901a\u8fc7\u65b0\u7684\u6570\u636e\u96c6\u548c\u7528\u6237\u6a21\u62df\u5668\uff0c\u6539\u8fdb\u4e86CRS\u7684\u8bc4\u4ef7\uff0c\u4f7f\u7cfb\u7edf\u66f4\u5feb\u6ee1\u8db3\u7528\u6237\u9700\u6c42\u3002", "motivation": "\u73b0\u6709\u7528\u6237\u6a21\u62df\u5668\u4ec5\u5173\u6ce8\u5355\u4e00\u76ee\u6807\u9879\u76ee\uff0c\u65e0\u6cd5\u51c6\u786e\u8bc4\u4f30CRS\u7cfb\u7edf\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u7528\u6237\u53ef\u8003\u8651\u591a\u4e2a\u66ff\u4ee3\u9879\u76ee\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u63d0\u51faFashion-AlterEval\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u589e\u52a0\u65b0\u7684\u6ce8\u91ca\u4ee5\u5305\u542b\u4eba\u7c7b\u5224\u65ad\uff0c\u5e76\u5f00\u53d1\u4e86\u4e24\u4e2a\u65b0\u9896\u7684\u5143\u7528\u6237\u6a21\u62df\u5668\uff0c\u4ee5\u66f4\u51c6\u786e\u5730\u6a21\u62df\u7528\u6237\u5728\u4e0d\u540c\u9879\u76ee\u4e2d\u7684\u504f\u597d\u548c\u8010\u5fc3\u53d8\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5229\u7528\u6a21\u62df\u5668\u7684\u66ff\u4ee3\u77e5\u8bc6\u53ef\u4ee5\u663e\u8457\u5f71\u54cdCRS\u6a21\u578b\u7684\u8bc4\u4ef7\uff0c\u73b0\u6709\u5355\u4e00\u76ee\u6807\u8bc4\u4ef7\u4f4e\u4f30\u4e86CRS\u7684\u6709\u6548\u6027\uff0c\u800c\u5141\u8bb8\u7528\u6237\u8003\u8651\u66ff\u4ee3\u76f8\u5173\u9879\u65f6\uff0c\u7cfb\u7edf\u80fd\u66f4\u5feb\u54cd\u5e94\u4ee5\u6ee1\u8db3\u7528\u6237\u3002", "conclusion": "\u901a\u8fc7\u91c7\u7528\u65b0\u7684Fashion-AlterEval\u6570\u636e\u96c6\u548c\u4e24\u79cd\u65b0\u9896\u7684\u5143\u7528\u6237\u6a21\u62df\u5668\uff0c\u5bf9CRS\u6a21\u578b\u7684\u8bc4\u4ef7\u5927\u5927\u63d0\u9ad8\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u6ee1\u8db3\u7528\u6237\u9700\u6c42\u7684\u80fd\u529b\u3002"}}
{"id": "2507.18365", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2507.18365", "abs": "https://arxiv.org/abs/2507.18365", "authors": ["Jiajie He", "Yuechun Gu", "Keke Chen"], "title": "RecPS: Privacy Risk Scoring for Recommender Systems", "comment": null, "summary": "Recommender systems (RecSys) have become an essential component of many web\napplications. The core of the system is a recommendation model trained on\nhighly sensitive user-item interaction data. While privacy-enhancing techniques\nare actively studied in the research community, the real-world model\ndevelopment still depends on minimal privacy protection, e.g., via controlled\naccess. Users of such systems should have the right to choose \\emph{not} to\nshare highly sensitive interactions. However, there is no method allowing the\nuser to know which interactions are more sensitive than others. Thus,\nquantifying the privacy risk of RecSys training data is a critical step to\nenabling privacy-aware RecSys model development and deployment. We propose a\nmembership-inference attack (MIA)- based privacy scoring method, RecPS, to\nmeasure privacy risks at both the interaction and user levels. The RecPS\ninteraction-level score definition is motivated and derived from differential\nprivacy, which is then extended to the user-level scoring method. A critical\ncomponent is the interaction-level MIA method RecLiRA, which gives high-quality\nmembership estimation. We have conducted extensive experiments on well-known\nbenchmark datasets and RecSys models to show the unique features and benefits\nof RecPS scoring in risk assessment and RecSys model unlearning. Our code is\navailable at https://anonymous.4open.science/r/RsLiRA-4BD3/readme.md.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRecPS\uff0c\u901a\u8fc7MIA\u6765\u91cf\u5316\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u9690\u79c1\u98ce\u9669\uff0c\u4ee5\u63d0\u9ad8\u9690\u79c1\u4fdd\u62a4\u80fd\u529b\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u98ce\u9669\u8bc4\u4f30\u548c\u6a21\u578b\u53bb\u5b66\u4e60\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u63a8\u8350\u7cfb\u7edf\u4f9d\u8d56\u4e8e\u654f\u611f\u7684\u7528\u6237-\u7269\u54c1\u4ea4\u4e92\u6570\u636e\uff0c\u7f3a\u4e4f\u91cf\u5316\u4e0d\u540c\u4ea4\u4e92\u654f\u611f\u6027\u7684\u65b9\u6cd5\u3002\u56e0\u6b64\uff0c\u91cf\u5316\u63a8\u8350\u7cfb\u7edf\u8bad\u7ec3\u6570\u636e\u7684\u9690\u79c1\u98ce\u9669\u5bf9\u9690\u79c1\u654f\u611f\u7684\u7cfb\u7edf\u5f00\u53d1\u548c\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7MIA\uff08membership-inference attack\uff09\u7684\u9690\u79c1\u8bc4\u5206\u65b9\u6cd5RecPS\u6d4b\u91cf\u4ea4\u4e92\u548c\u7528\u6237\u5c42\u9762\u7684\u9690\u79c1\u98ce\u9669\uff0c\u4ea4\u4e92\u5c42\u9762\u7684\u8bc4\u5206\u57fa\u4e8e\u5dee\u5206\u9690\u79c1\uff0c\u6269\u5c55\u5230\u7528\u6237\u5c42\u9762\u3002\u5173\u952e\u7ec4\u4ef6\u662fRecLiRA\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u8d28\u91cf\u7684\u6210\u5458\u4f30\u7b97\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRecPS\u8bc4\u5206\u5728\u98ce\u9669\u8bc4\u4f30\u548c\u63a8\u8350\u7cfb\u7edf\u6a21\u578b\u53bb\u5b66\u4e60\u65b9\u9762\u6709\u663e\u8457\u6548\u679c\u3002", "conclusion": "\u63d0\u51faRecPS\u65b9\u6cd5\uff0c\u901a\u8fc7MIA\u7684\u9690\u79c1\u8bc4\u5206\uff0c\u91cf\u5316\u548c\u8bc4\u4f30RecSys\u7684\u9690\u79c1\u98ce\u9669\uff0c\u4e3a\u7528\u6237\u63d0\u4f9b\u4e0d\u5171\u4eab\u654f\u611f\u6570\u636e\u7684\u9009\u62e9\u6743\u3002\u7ecf\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u5728\u98ce\u9669\u8bc4\u4f30\u548c\u6a21\u578b\u53bb\u5b66\u4e60\u65b9\u9762\u5177\u6709\u72ec\u7279\u4f18\u52bf\u3002"}}
{"id": "2507.18455", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18455", "abs": "https://arxiv.org/abs/2507.18455", "authors": ["Damith Premasiri", "Tharindu Ranasinghe", "Ruslan Mitkov"], "title": "LLM-based Embedders for Prior Case Retrieval", "comment": "Accepted in Recent Advancements in Natural Language Processing (RANLP\n  2025) conference", "summary": "In common law systems, legal professionals such as lawyers and judges rely on\nprecedents to build their arguments. As the volume of cases has grown massively\nover time, effectively retrieving prior cases has become essential. Prior case\nretrieval (PCR) is an information retrieval (IR) task that aims to\nautomatically identify the most relevant court cases for a specific query from\na large pool of potential candidates. While IR methods have seen several\nparadigm shifts over the last few years, the vast majority of PCR methods\ncontinue to rely on traditional IR methods, such as BM25. The state-of-the-art\ndeep learning IR methods have not been successful in PCR due to two key\nchallenges: i. Lengthy legal text limitation; when using the powerful\nBERT-based transformer models, there is a limit of input text lengths, which\ninevitably requires to shorten the input via truncation or division with a loss\nof legal context information. ii. Lack of legal training data; due to data\nprivacy concerns, available PCR datasets are often limited in size, making it\ndifficult to train deep learning-based models effectively. In this research, we\naddress these challenges by leveraging LLM-based text embedders in PCR.\nLLM-based embedders support longer input lengths, and since we use them in an\nunsupervised manner, they do not require training data, addressing both\nchallenges simultaneously. In this paper, we evaluate state-of-the-art\nLLM-based text embedders in four PCR benchmark datasets and show that they\noutperform BM25 and supervised transformer-based models.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u4f7f\u7528LLM-based\u6587\u672c\u5d4c\u5165\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u5148\u4f8b\u68c0\u7d22\u4e2d\u957f\u6587\u672c\u8f93\u5165\u548c\u7f3a\u4e4f\u8bad\u7ec3\u6570\u636e\u7684\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8eBM25\u548c\u4f20\u7edftransformer\u6a21\u578b\u3002", "motivation": "\u5728\u666e\u901a\u6cd5\u7cfb\u4e2d\uff0c\u6cd5\u5f8b\u4e13\u4e1a\u4eba\u58eb\u4f9d\u8d56\u5148\u4f8b\u6765\u6784\u5efa\u4ed6\u4eec\u7684\u8bba\u70b9\u3002\u968f\u7740\u6848\u4ef6\u6570\u91cf\u7684\u6025\u5267\u589e\u52a0\uff0c\u5982\u4f55\u6709\u6548\u5730\u68c0\u7d22\u8fc7\u5f80\u6848\u4f8b\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u76ee\u524d\u7684\u5927\u591a\u6570\u5148\u4f8b\u68c0\u7d22\uff08PCR\uff09\u65b9\u6cd5\u4f9d\u65e7\u4f9d\u8d56\u4e8e\u4f20\u7edf\u7684\u4fe1\u606f\u68c0\u7d22\u6280\u672f\u5982BM25\uff0c\u800c\u6700\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60\u4fe1\u606f\u68c0\u7d22\u65b9\u6cd5\u5728PCR\u4e2d\u672a\u80fd\u53d6\u5f97\u6210\u529f\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u5229\u7528\u57fa\u4e8e\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6587\u672c\u5d4c\u5165\u6280\u672f\u6765\u5e94\u5bf9PCR\u4e2d\u7684\u6311\u6218\uff0c\u8fd9\u4e9b\u5d4c\u5165\u5668\u80fd\u591f\u652f\u6301\u8f83\u957f\u7684\u8f93\u5165\u957f\u5ea6\uff0c\u5e76\u4e14\u53ef\u4ee5\u4ee5\u65e0\u76d1\u7763\u7684\u65b9\u5f0f\u4f7f\u7528\uff0c\u4e0d\u9700\u8981\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5728\u56db\u4e2aPCR\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u5148\u8fdb\u7684LLM-based\u6587\u672c\u5d4c\u5165\u5668\uff0c\u7ed3\u679c\u8868\u660e\u5b83\u4eec\u4f18\u4e8eBM25\u548c\u4ee5\u76d1\u7763\u65b9\u5f0f\u8bad\u7ec3\u7684\u57fa\u4e8etransformer\u7684\u6a21\u578b\u3002", "conclusion": "\u5229\u7528LLM-based\u6587\u672c\u5d4c\u5165\u5668\u53ef\u4ee5\u6709\u6548\u89e3\u51b3PCR\u4e2d\u7684\u957f\u6587\u672c\u8f93\u5165\u9650\u5236\u548c\u7f3a\u4e4f\u6cd5\u5f8b\u8bad\u7ec3\u6570\u636e\u7684\u95ee\u9898\uff0c\u4ece\u800c\u5728\u5148\u4f8b\u68c0\u7d22\u4efb\u52a1\u4e2d\u53d6\u5f97\u663e\u8457\u7684\u6548\u679c\u63d0\u5347\u3002"}}
{"id": "2507.18479", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2507.18479", "abs": "https://arxiv.org/abs/2507.18479", "authors": ["Ngoc Luyen Le", "Marie-H\u00e9l\u00e8ne Abel"], "title": "How Well Do LLMs Predict Prerequisite Skills? Zero-Shot Comparison to Expert-Defined Concepts", "comment": null, "summary": "Prerequisite skills - foundational competencies required before mastering\nmore advanced concepts - are important for supporting effective learning,\nassessment, and skill-gap analysis. Traditionally curated by domain experts,\nthese relationships are costly to maintain and difficult to scale. This paper\ninvestigates whether large language models (LLMs) can predict prerequisite\nskills in a zero-shot setting, using only natural language descriptions and\nwithout task-specific fine-tuning. We introduce ESCO-PrereqSkill, a benchmark\ndataset constructed from the ESCO taxonomy, comprising 3,196 skills and their\nexpert-defined prerequisite links. Using a standardized prompting strategy, we\nevaluate 13 state-of-the-art LLMs, including GPT-4, Claude 3, Gemini, LLaMA 4,\nQwen2, and DeepSeek, across semantic similarity, BERTScore, and inference\nlatency. Our results show that models such as LLaMA4-Maverick,\nClaude-3-7-Sonnet, and Qwen2-72B generate predictions that closely align with\nexpert ground truth, demonstrating strong semantic reasoning without\nsupervision. These findings highlight the potential of LLMs to support scalable\nprerequisite skill modeling for applications in personalized learning,\nintelligent tutoring, and skill-based recommender systems.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86LLMs\u5728\u96f6\u6837\u672c\u573a\u666f\u4e0b\u9884\u6d4b\u524d\u63d0\u6280\u80fd\u7684\u80fd\u529b\uff0c\u7ed3\u679c\u663e\u793a\u591a\u79cd\u5148\u8fdb\u7684LLMs\u80fd\u591f\u5728\u6ca1\u6709\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\uff0c\u51c6\u786e\u9884\u6d4b\u524d\u63d0\u6280\u80fd\uff0c\u8fd9\u53ef\u80fd\u652f\u6301\u4e2a\u6027\u5316\u5b66\u4e60\u548c\u667a\u80fd\u8f85\u5bfc\u7b49\u5e94\u7528\u3002", "motivation": "\u524d\u63d0\u6280\u80fd\u5bf9\u4e8e\u6709\u6548\u5b66\u4e60\u3001\u8bc4\u4f30\u548c\u6280\u80fd\u7f3a\u53e3\u5206\u6790\u975e\u5e38\u91cd\u8981\u3002\u7136\u800c\uff0c\u4f20\u7edf\u4e0a\u7531\u9886\u57df\u4e13\u5bb6\u7b56\u5212\u7684\u8fd9\u4e9b\u5173\u7cfb\u96be\u4ee5\u7ef4\u62a4\u4e14\u96be\u4ee5\u6269\u5c55\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u63a2\u8ba8LLMs\u80fd\u5426\u5728\u4e0d\u8fdb\u884c\u7279\u5b9a\u4efb\u52a1\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\uff0c\u4ec5\u5229\u7528\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u9884\u6d4b\u524d\u63d0\u6280\u80fd\u3002", "method": "\u4f7f\u7528\u6807\u51c6\u5316\u7684\u63d0\u793a\u7b56\u7565\uff0c\u5e76\u5229\u752813\u79cd\u6700\u5148\u8fdb\u7684LLMs\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5305\u62ecGPT-4\u3001Claude 3\u3001Gemini\u3001LLaMA 4\u3001Qwen2\u548cDeepSeek\uff0c\u8bc4\u4f30\u5176\u5728\u8bed\u4e49\u76f8\u4f3c\u6027\u3001BERTScore\u548c\u63a8\u7406\u5ef6\u8fdf\u65b9\u9762\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u50cfLLaMA4-Maverick\u3001Claude-3-7-Sonnet\u548cQwen2-72B\u7b49\u6a21\u578b\u80fd\u591f\u4ea7\u751f\u4e0e\u4e13\u5bb6\u5b9a\u4e49\u7684\u524d\u63d0\u6280\u80fd\u5173\u7cfb\u4e00\u81f4\u7684\u9884\u6d4b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0cLLMs\u53ef\u4ee5\u5728\u96f6\u6837\u672c\u573a\u666f\u4e0b\u9884\u6d4b\u524d\u63d0\u6280\u80fd\uff0c\u4e0e\u4e13\u5bb6\u5b9a\u4e49\u7684\u6807\u51c6\u76f8\u7b26\uff0c\u5c55\u793a\u51fa\u5728\u6ca1\u6709\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u7684\u5f3a\u5927\u8bed\u4e49\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2507.18489", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2507.18489", "abs": "https://arxiv.org/abs/2507.18489", "authors": ["Jinfeng Xu", "Zheyu Chen", "Shuo Yang", "Jinze Li", "Edith C. H. Ngai"], "title": "The Best is Yet to Come: Graph Convolution in the Testing Phase for Multimodal Recommendation", "comment": "Accepted by MM 2025", "summary": "The efficiency and scalability of graph convolution networks (GCNs) in\ntraining recommender systems remain critical challenges, hindering their\npractical deployment in real-world scenarios. In the multimodal recommendation\n(MMRec) field, training GCNs requires more expensive time and space costs and\nexacerbates the gap between different modalities, resulting in sub-optimal\nrecommendation accuracy. This paper critically points out the inherent\nchallenges associated with adopting GCNs during the training phase in MMRec,\nrevealing that GCNs inevitably create unhelpful and even harmful pairs during\nmodel optimization and isolate different modalities. To this end, we propose\nFastMMRec, a highly efficient multimodal recommendation framework that deploys\ngraph convolutions exclusively during the testing phase, bypassing their use in\ntraining. We demonstrate that adopting GCNs solely in the testing phase\nsignificantly improves the model's efficiency and scalability while alleviating\nthe modality isolation problem often caused by using GCNs during the training\nphase. We conduct extensive experiments on three public datasets, consistently\ndemonstrating the performance superiority of FastMMRec over competitive\nbaselines while achieving efficiency and scalability.", "AI": {"tldr": "\u63d0\u51faFastMMRec\u6846\u67b6\uff0c\u5728\u6d4b\u8bd5\u9636\u6bb5\u4f7f\u7528\u56fe\u5377\u79ef\u7f51\u7edc\uff0c\u63d0\u9ad8\u63a8\u8350\u7cfb\u7edf\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u56fe\u5377\u79ef\u7f51\u7edc\u5728\u63a8\u8350\u7cfb\u7edf\u8bad\u7ec3\u4e2d\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u4ee5\u53ca\u4e0d\u540c\u6a21\u6001\u95f4\u7684\u9694\u79bb\u95ee\u9898\u3002", "method": "\u63d0\u51faFastMMRec\u6846\u67b6\uff0c\u5e76\u5728\u6d4b\u8bd5\u9636\u6bb5\u4f7f\u7528\u56fe\u5377\u79ef\u7f51\u7edc\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFastMMRec\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u7ade\u4e89\u57fa\u51c6\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "FastMMRec\u5728\u6d4b\u8bd5\u9636\u6bb5\u4f7f\u7528\u56fe\u5377\u79ef\u7f51\u7edc\uff0c\u907f\u514d\u4e86\u8bad\u7ec3\u9636\u6bb5\u7684\u9ad8\u65f6\u95f4\u548c\u7a7a\u95f4\u6210\u672c\uff0c\u63d0\u9ad8\u4e86\u63a8\u8350\u7cfb\u7edf\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u4e0d\u540c\u6a21\u6001\u95f4\u7684\u9694\u79bb\u3002"}}
{"id": "2507.18518", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2507.18518", "abs": "https://arxiv.org/abs/2507.18518", "authors": ["Ruiqi He", "Zekun Fei", "Jiaqi Li", "Xinyuan Zhu", "Biao Yi", "Siyi Lv", "Weijie Liu", "Zheli Liu"], "title": "Transform Before You Query: A Privacy-Preserving Approach for Vector Retrieval with Embedding Space Alignment", "comment": null, "summary": "Vector Database (VDB) can efficiently index and search high-dimensional\nvector embeddings from unstructured data, crucially enabling fast semantic\nsimilarity search essential for modern AI applications like generative AI and\nrecommendation systems. Since current VDB service providers predominantly use\nproprietary black-box models, users are forced to expose raw query text to them\nvia API in exchange for the vector retrieval services. Consequently, if query\ntext involves confidential records from finance or healthcare domains, this\nmechanism inevitably leads to critical leakage of user's sensitive information.\nTo address this issue, we introduce STEER (\\textbf{S}ecure \\textbf{T}ransformed\n\\textbf{E}mbedding v\\textbf{E}ctor\\textbf{ R}etrieval), a private vector\nretrieval framework that leverages the alignment relationship between the\nsemantic spaces of different embedding models to derive approximate embeddings\nfor the query text. STEER performs the retrieval using the approximate\nembeddings within the original VDB and requires no modifications to the server\nside. Our theoretical and experimental analyses demonstrate that STEER\neffectively safeguards query text privacy while maintaining the retrieval\naccuracy. Even though approximate embeddings are approximations of the\nembeddings from proprietary models, they still prevent the providers from\nrecovering the query text through Embedding Inversion Attacks (EIAs). Extensive\nexperimental results show that Recall@100 of STEER can basically achieve a\ndecrease of less than 5\\%. Furthermore, even when searching within a text\ncorpus of millions of entries, STEER achieves a Recall@20 accuracy 20\\% higher\nthan current baselines.", "AI": {"tldr": "STEER\u6846\u67b6\u53ef\u4ee5\u5229\u7528\u8fd1\u4f3c\u5d4c\u5165\u4fdd\u62a4\u7528\u6237\u67e5\u8be2\u9690\u79c1\uff0c\u4e14\u5728\u51e0\u4e4e\u4e0d\u5f71\u54cd\u51c6\u786e\u6027\u7684\u57fa\u7840\u4e0a\uff0c\u5927\u5e45\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u5f53\u524d\u7684VDB\u670d\u52a1\u63d0\u4f9b\u5546\u4e3b\u8981\u4f7f\u7528\u4e13\u6709\u7684\u9ed1\u76d2\u6a21\u578b\uff0c\u7528\u6237\u5fc5\u987b\u901a\u8fc7API\u66b4\u9732\u539f\u59cb\u67e5\u8be2\u6587\u672c\u4ee5\u6362\u53d6\u5411\u91cf\u68c0\u7d22\u670d\u52a1\uff0c\u5c24\u5176\u662f\u5f53\u67e5\u8be2\u6d89\u53ca\u673a\u5bc6\u8bb0\u5f55\u65f6\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u654f\u611f\u4fe1\u606f\u6cc4\u9732\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u79f0\u4e3aSTEER\u7684\u79c1\u4eba\u5411\u91cf\u68c0\u7d22\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u4e0d\u540c\u5d4c\u5165\u6a21\u578b\u8bed\u4e49\u7a7a\u95f4\u4e4b\u95f4\u7684\u5bf9\u9f50\u5173\u7cfb\u6765\u6d3e\u751f\u67e5\u8be2\u6587\u672c\u7684\u8fd1\u4f3c\u5d4c\u5165\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cSTEER\u5728\u68c0\u7d22\u51c6\u786e\u6027\u65b9\u9762\u57fa\u672c\u4ec5\u4e0b\u964d\u4e0d\u52305%\u3002\u5373\u4f7f\u5728\u6570\u767e\u4e07\u6761\u76ee\u4e2d\u8fdb\u884c\u641c\u7d22\uff0cSTEER\u7684Recall@20\u51c6\u786e\u5ea6\u4e5f\u6bd4\u5f53\u524d\u57fa\u7ebf\u9ad8\u51fa20%\u3002", "conclusion": "STEER\u80fd\u5728\u4e0d\u4fee\u6539\u670d\u52a1\u5668\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u4fdd\u62a4\u67e5\u8be2\u6587\u672c\u7684\u9690\u79c1\uff0c\u540c\u65f6\u4fdd\u6301\u68c0\u7d22\u51c6\u786e\u6027\u3002\u5373\u4f7f\u8fd1\u4f3c\u5d4c\u5165\u662f\u4e13\u6709\u6a21\u578b\u5d4c\u5165\u7684\u8fd1\u4f3c\u503c\uff0c\u5b83\u4eec\u4ecd\u963b\u6b62\u63d0\u4f9b\u5546\u901a\u8fc7\u5d4c\u5165\u53cd\u6f14\u653b\u51fb\u6062\u590d\u67e5\u8be2\u6587\u672c\u3002"}}
{"id": "2507.18583", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18583", "abs": "https://arxiv.org/abs/2507.18583", "authors": ["Zhengyun Zhao", "Huaiyuan Ying", "Yue Zhong", "Sheng Yu"], "title": "DR.EHR: Dense Retrieval for Electronic Health Record with Knowledge Injection and Synthetic Data", "comment": "Model and code released upon acceptance", "summary": "Electronic Health Records (EHRs) are pivotal in clinical practices, yet their\nretrieval remains a challenge mainly due to semantic gap issues. Recent\nadvancements in dense retrieval offer promising solutions but existing models,\nboth general-domain and biomedical-domain, fall short due to insufficient\nmedical knowledge or mismatched training corpora. This paper introduces\n\\texttt{DR.EHR}, a series of dense retrieval models specifically tailored for\nEHR retrieval. We propose a two-stage training pipeline utilizing MIMIC-IV\ndischarge summaries to address the need for extensive medical knowledge and\nlarge-scale training data. The first stage involves medical entity extraction\nand knowledge injection from a biomedical knowledge graph, while the second\nstage employs large language models to generate diverse training data. We train\ntwo variants of \\texttt{DR.EHR}, with 110M and 7B parameters, respectively.\nEvaluated on the CliniQ benchmark, our models significantly outperforms all\nexisting dense retrievers, achieving state-of-the-art results. Detailed\nanalyses confirm our models' superiority across various match and query types,\nparticularly in challenging semantic matches like implication and abbreviation.\nAblation studies validate the effectiveness of each pipeline component, and\nsupplementary experiments on EHR QA datasets demonstrate the models'\ngeneralizability on natural language questions, including complex ones with\nmultiple entities. This work significantly advances EHR retrieval, offering a\nrobust solution for clinical applications.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e13\u4e3aEHR\u68c0\u7d22\u8bbe\u8ba1\u7684DR.EHR\u6a21\u578b\uff0c\u63d0\u51fa\u4e86\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u590d\u6742\u8bed\u4e49\u5339\u914d\u65f6\u3002", "motivation": "\u73b0\u6709\u7684\u5bc6\u96c6\u68c0\u7d22\u6a21\u578b\u5728EHR\u68c0\u7d22\u4e2d\u5b58\u5728\u533b\u5b66\u77e5\u8bc6\u4e0d\u8db3\u6216\u8bad\u7ec3\u8bed\u6599\u4e0d\u5339\u914d\u7684\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff0c\u9996\u5148\u8fdb\u884c\u533b\u5b66\u5b9e\u4f53\u63d0\u53d6\u548c\u6765\u81ea\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u7684\u77e5\u8bc6\u6ce8\u5165\uff0c\u7136\u540e\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u591a\u6837\u5316\u7684\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u6211\u4eec\u7684DR.EHR\u6a21\u578b\u5728CliniQ\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u8d85\u8d8a\u4e86\u6240\u6709\u73b0\u6709\u7684\u5bc6\u96c6\u68c0\u7d22\u5668\uff0c\u83b7\u5f97\u4e86\u6700\u65b0\u7684\u7ed3\u679c\uff0c\u5e76\u5728\u591a\u79cd\u5339\u914d\u548c\u67e5\u8be2\u7c7b\u578b\u4e0b\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u6211\u4eec\u5f00\u53d1\u7684\u5bc6\u96c6\u68c0\u7d22\u6a21\u578bDR.EHR\u5728EHR\u68c0\u7d22\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u68c0\u7d22\u6a21\u578b\uff0c\u5e76\u8bc1\u660e\u5176\u5728\u4e0d\u540c\u8bed\u4e49\u5339\u914d\u548c\u67e5\u8be2\u7c7b\u578b\u4e0a\u7684\u4f18\u8d8a\u6027\u3002"}}
