<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 89]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [A Quality-Guided Mixture of Score-Fusion Experts Framework for Human Recognition](https://arxiv.org/abs/2508.00053)
*Jie Zhu,Yiyang Su,Minchul Kim,Anil Jain,Xiaoming Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新的通过专家混合实现可学习评分融合的框架QME，有效提高了全身生物识别性能，并在多个数据集上达到了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的单独处理多个生物识别模式的模型容易忽视单个模式分数分布的差异性，限制了最终表现的提升。因此需要一种整合多种生物识别模式的方法，以克服单模系统的局限性。

Method: 提出了一种通过专家混合（MoE）进行可学习评分融合的方法，并引入伪质量损失和评分三重项损失来评估质量和提高度量性能。

Result: 各种全身生物识别数据集上的实验结果显示，该方法的有效性，并在多个指标上达到了最先进的结果，解决了相似度分数域中的模型不对齐和数据质量的变化等关键挑战。

Conclusion: 本文提出了一种能够提高全身生物识别性能的新框架QME，通过可学习的评分融合策略获知质量指导混合专家方法。实验表明，该方法在多个数据集上达到了优于现有方法的结果。

Abstract: Whole-body biometric recognition is a challenging multimodal task that
integrates various biometric modalities, including face, gait, and body. This
integration is essential for overcoming the limitations of unimodal systems.
Traditionally, whole-body recognition involves deploying different models to
process multiple modalities, achieving the final outcome by score-fusion (e.g.,
weighted averaging of similarity matrices from each model). However, these
conventional methods may overlook the variations in score distributions of
individual modalities, making it challenging to improve final performance. In
this work, we present \textbf{Q}uality-guided \textbf{M}ixture of score-fusion
\textbf{E}xperts (QME), a novel framework designed for improving whole-body
biometric recognition performance through a learnable score-fusion strategy
using a Mixture of Experts (MoE). We introduce a novel pseudo-quality loss for
quality estimation with a modality-specific Quality Estimator (QE), and a score
triplet loss to improve the metric performance. Extensive experiments on
multiple whole-body biometric datasets demonstrate the effectiveness of our
proposed approach, achieving state-of-the-art results across various metrics
compared to baseline methods. Our method is effective for multimodal and
multi-model, addressing key challenges such as model misalignment in the
similarity score domain and variability in data quality.

</details>


### [2] [Punching Bag vs. Punching Person: Motion Transferability in Videos](https://arxiv.org/abs/2508.00085)
*Raiyaan Abdullah,Jared Claypoole,Michael Cogswell,Ajay Divakaran,Yogesh Rawat*

Main category: cs.CV

TL;DR: 研究动作识别模型在不同上下文中转移高层次动作的能力，引入三种数据集进行评估，发现识别新上下文中动作时性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 探索模型在不同上下文中有效转移高层次动作概念的能力，尤其是识别新的动作变体。

Method: 引入三个数据集，并进行实验分析，评估13个先进模型在新上下文中识别高层次动作的表现。

Result: 在新上下文中识别高层次动作时性能显著下降，揭示了多模态模型在识别细粒度未知动作时的困难及模型在控制环境中的表现降低。

Conclusion: 本研究建立了评估动作识别中的运动可转移性的重要基准。

Abstract: Action recognition models demonstrate strong generalization, but can they
effectively transfer high-level motion concepts across diverse contexts, even
within similar distributions? For example, can a model recognize the broad
action "punching" when presented with an unseen variation such as "punching
person"? To explore this, we introduce a motion transferability framework with
three datasets: (1) Syn-TA, a synthetic dataset with 3D object motions; (2)
Kinetics400-TA; and (3) Something-Something-v2-TA, both adapted from natural
video datasets. We evaluate 13 state-of-the-art models on these benchmarks and
observe a significant drop in performance when recognizing high-level actions
in novel contexts. Our analysis reveals: 1) Multimodal models struggle more
with fine-grained unknown actions than with coarse ones; 2) The bias-free
Syn-TA proves as challenging as real-world datasets, with models showing
greater performance drops in controlled settings; 3) Larger models improve
transferability when spatial cues dominate but struggle with intensive temporal
reasoning, while reliance on object and background cues hinders generalization.
We further explore how disentangling coarse and fine motions can improve
recognition in temporally challenging datasets. We believe this study
establishes a crucial benchmark for assessing motion transferability in action
recognition. Datasets and relevant code:
https://github.com/raiyaan-abdullah/Motion-Transfer.

</details>


### [3] [The Monado SLAM Dataset for Egocentric Visual-Inertial Tracking](https://arxiv.org/abs/2508.00088)
*Mateo de Mayo,Daniel Cremers,Taihú Pire*

Main category: cs.CV

TL;DR: 文章介绍了Monado SLAM数据集以解决头戴设备场景中的跟踪挑战，并推动VIO/SLAM领域的发展。


<details>
  <summary>Details</summary>
Motivation: 当前的数据集未能充分覆盖头戴设备中的许多挑战性情况，如高强度运动、动态遮挡等。

Method: 提供一个名为Monado SLAM的数据集，包含来自多个虚拟现实头盔的序列。

Result: 发布了一个可用于VIO/SLAM研究的数据集，并以开源许可方式提供。

Conclusion: 现有的VIO和SLAM系统在头戴设备场景中的表现仍然不足。

Abstract: Humanoid robots and mixed reality headsets benefit from the use of
head-mounted sensors for tracking. While advancements in visual-inertial
odometry (VIO) and simultaneous localization and mapping (SLAM) have produced
new and high-quality state-of-the-art tracking systems, we show that these are
still unable to gracefully handle many of the challenging settings presented in
the head-mounted use cases. Common scenarios like high-intensity motions,
dynamic occlusions, long tracking sessions, low-textured areas, adverse
lighting conditions, saturation of sensors, to name a few, continue to be
covered poorly by existing datasets in the literature. In this way, systems may
inadvertently overlook these essential real-world issues. To address this, we
present the Monado SLAM dataset, a set of real sequences taken from multiple
virtual reality headsets. We release the dataset under a permissive CC BY 4.0
license, to drive advancements in VIO/SLAM research and development.

</details>


### [4] [Exploring the Feasibility of Deep Learning Techniques for Accurate Gender Classification from Eye Images](https://arxiv.org/abs/2508.00135)
*Basna Mohammed Salih Hasan,Ramadhan J. Mstafa*

Main category: cs.CV

TL;DR: 该研究提出了一种CNN模型，利用眼眶区域的颜色图像进行性别分类，取得了极高的准确率。


<details>
  <summary>Details</summary>
Motivation: 通过解决化妆和伪装对性别分类准确性影响的问题，研究眼眶区域对提高性别分类准确性的作用。

Method: 利用卷积神经网络（CNN）对眼眶区域的颜色图像进行分析，从中提取关键特征进行性别分类。

Result: 模型在未使用过的CVBL数据集上达到了99%的准确率，而在Female和Male数据集上则达到了96%的准确率。

Conclusion: 所提出的CNN模型通过对眼眶区域的颜色图像进行分类，证明了其在性别分类方面的有效性和应用潜力。

Abstract: Gender classification has emerged as a crucial aspect in various fields,
including security, human-machine interaction, surveillance, and advertising.
Nonetheless, the accuracy of this classification can be influenced by factors
such as cosmetics and disguise. Consequently, our study is dedicated to
addressing this concern by concentrating on gender classification using color
images of the periocular region. The periocular region refers to the area
surrounding the eye, including the eyelids, eyebrows, and the region between
them. It contains valuable visual cues that can be used to extract key features
for gender classification. This paper introduces a sophisticated Convolutional
Neural Network (CNN) model that utilizes color image databases to evaluate the
effectiveness of the periocular region for gender classification. To validate
the model's performance, we conducted tests on two eye datasets, namely CVBL
and (Female and Male). The recommended architecture achieved an outstanding
accuracy of 99% on the previously unused CVBL dataset while attaining a
commendable accuracy of 96% with a small number of learnable parameters
(7,235,089) on the (Female and Male) dataset. To ascertain the effectiveness of
our proposed model for gender classification using the periocular region, we
evaluated its performance through an extensive range of metrics and compared it
with other state-of-the-art approaches. The results unequivocally demonstrate
the efficacy of our model, thereby suggesting its potential for practical
application in domains such as security and surveillance.

</details>


### [5] [World Consistency Score: A Unified Metric for Video Generation Quality](https://arxiv.org/abs/2508.00144)
*Akshat Rakheja,Aarsh Ashdhir,Aryan Bhattacharjee,Vanshika Sharma*

Main category: cs.CV

TL;DR: 引入了一个新的视频生成评估指标WCS，专注于视频的一致性评估，整合多个子指标，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的视频评估指标通常只关注视觉保真度或提示对齐，而缺乏对生成视频内部世界一致性的评估。为了填补这一空白，引入了新的评价指标WCS。

Method: WCS包含四个可解释的子组件：物体持久性、关系稳定性、因果合规性和闪烁惩罚。这些子指标通过一个学习到的加权公式组合在一起，生成一个总的一致性评分。每个子指标通过使用开源工具（如跟踪器、动作识别器、CLIP 嵌入、光流）计算得出。

Result: 提出的WCS为评估视频生成模型在时间上维持连续一致的“世界”提供了一个全面、可解释的框架，并被证明与人类评价具有较好的相关性。

Conclusion: WCS提供了一个统一的评估视频生成模型的框架，能够在时间上保持视频的内部世界一致性，弥补了原有指标的不足。

Abstract: We introduce World Consistency Score (WCS), a novel unified evaluation metric
for generative video models that emphasizes internal world consistency of the
generated videos. WCS integrates four interpretable sub-components - object
permanence, relation stability, causal compliance, and flicker penalty - each
measuring a distinct aspect of temporal and physical coherence in a video.
These submetrics are combined via a learned weighted formula to produce a
single consistency score that aligns with human judgments. We detail the
motivation for WCS in the context of existing video evaluation metrics,
formalize each submetric and how it is computed with open-source tools
(trackers, action recognizers, CLIP embeddings, optical flow), and describe how
the weights of the WCS combination are trained using human preference data. We
also outline an experimental validation blueprint: using benchmarks like
VBench-2.0, EvalCrafter, and LOVE to test WCS's correlation with human
evaluations, performing sensitivity analyses, and comparing WCS against
established metrics (FVD, CLIPScore, VBench, FVMD). The proposed WCS offers a
comprehensive and interpretable framework for evaluating video generation
models on their ability to maintain a coherent "world" over time, addressing
gaps left by prior metrics focused only on visual fidelity or prompt alignment.

</details>


### [6] [GeoExplorer: Active Geo-localization with Curiosity-Driven Exploration](https://arxiv.org/abs/2508.00152)
*Li Mi,Manon Bechaz,Zeming Chen,Antoine Bosselut,Devis Tuia*

Main category: cs.CV

TL;DR: GeoExplorer是一种利用好奇心驱动的内在奖励来提升AGL任务探索效率的代理，在多样化的基准测试中显示出增强的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在遇到距离估计困难或未见过的目标和环境时，探索策略的可靠性较差，进而影响了鲁棒性和泛化能力。因此需要一种更可靠的探索策略。

Method: GeoExplorer是一种AGL代理，通过好奇心驱动的探索方式实现，该方式通过内在奖励来激励探索。这种奖励机制是无目标的，使得探索过程更加可靠、多样化和具有情境相关性。

Result: 在四个AGL基准上的广泛实验验证了GeoExplorer的有效性和泛化能力，尤其是在处理不熟悉的目标和环境时。

Conclusion: GeoExplorer在各种AGL基准上的性能和泛化能力得到了验证，特别是在定位不熟悉的目标和环境方面具有优势。

Abstract: Active Geo-localization (AGL) is the task of localizing a goal, represented
in various modalities (e.g., aerial images, ground-level images, or text),
within a predefined search area. Current methods approach AGL as a
goal-reaching reinforcement learning (RL) problem with a distance-based reward.
They localize the goal by implicitly learning to minimize the relative distance
from it. However, when distance estimation becomes challenging or when
encountering unseen targets and environments, the agent exhibits reduced
robustness and generalization ability due to the less reliable exploration
strategy learned during training. In this paper, we propose GeoExplorer, an AGL
agent that incorporates curiosity-driven exploration through intrinsic rewards.
Unlike distance-based rewards, our curiosity-driven reward is goal-agnostic,
enabling robust, diverse, and contextually relevant exploration based on
effective environment modeling. These capabilities have been proven through
extensive experiments across four AGL benchmarks, demonstrating the
effectiveness and generalization ability of GeoExplorer in diverse settings,
particularly in localizing unfamiliar targets and environments.

</details>


### [7] [Robust 3D Object Detection using Probabilistic Point Clouds from Single-Photon LiDARs](https://arxiv.org/abs/2508.00169)
*Bhavya Goyal,Felipe Gutierrez-Barragan,Wei Lin,Andreas Velten,Yin Li,Mohit Gupta*

Main category: cs.CV

TL;DR: 提出了一种加入测量不确定性的新型概率点云表示方法，显著提高了3D目标检测的性能。


<details>
  <summary>Details</summary>
Motivation: 现代LiDAR在长距离和低反射率物体场景下生成的稀疏或错误点云会影响下游感知模型的精度，因为传统的3D处理流程未能保留原始测量的不确定性信息。

Method: 提出了一种新的概率点云（PPC）表示方法，为每个点云数据增加了概率属性，反映原始数据的测量不确定性，同时设计了利用PPC进行3D目标检测的推理方法，可作为轻量级模块添加到3D推理流水线中。

Result: 通过模拟和真实场景验证，利用PPC的3D推理方法在室内和室外挑战场景下优于多种基线模型，尤其在处理小型、远处、低反射率物体及强光条件下表现突出。

Conclusion: 通过引入概率点云（PPC），增强了3D点云的测量不确定性，提升了点云在3D目标检测中的可靠性和精度。

Abstract: LiDAR-based 3D sensors provide point clouds, a canonical 3D representation
used in various scene understanding tasks. Modern LiDARs face key challenges in
several real-world scenarios, such as long-distance or low-albedo objects,
producing sparse or erroneous point clouds. These errors, which are rooted in
the noisy raw LiDAR measurements, get propagated to downstream perception
models, resulting in potentially severe loss of accuracy. This is because
conventional 3D processing pipelines do not retain any uncertainty information
from the raw measurements when constructing point clouds.
  We propose Probabilistic Point Clouds (PPC), a novel 3D scene representation
where each point is augmented with a probability attribute that encapsulates
the measurement uncertainty (or confidence) in the raw data. We further
introduce inference approaches that leverage PPC for robust 3D object
detection; these methods are versatile and can be used as computationally
lightweight drop-in modules in 3D inference pipelines. We demonstrate, via both
simulations and real captures, that PPC-based 3D inference methods outperform
several baselines using LiDAR as well as camera-LiDAR fusion models, across
challenging indoor and outdoor scenarios involving small, distant, and
low-albedo objects, as well as strong ambient light.
  Our project webpage is at https://bhavyagoyal.github.io/ppc .

</details>


### [8] [On the Risk of Misleading Reports: Diagnosing Textual Biases in Multimodal Clinical AI](https://arxiv.org/abs/2508.00171)
*David Restrepo,Ira Ktena,Maria Vakalopoulou,Stergios Christodoulidis,Enzo Ferrante*

Main category: cs.CV

TL;DR: 提出一种新方法来评估视觉语言模型对视觉和文本模态的依赖性，揭示出在医学成像任务中模型对文本信息的强烈偏见。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在整合分析医学图像和相关临床报告时，常常对一种模态产生偏见，倾向于文本信息而忽视关键的视觉线索。

Method: 提出了一种名为选择性模态转换（SMS）的扰动方法，用于量化模型在二元分类任务中对每种模态的依赖性。具体做法是系统性地交换具有相反标签的样本中的图像或文本，以揭示模态特定的偏见。

Result: 研究评估了六个开源VLM模型（四个通用模型和两个为医疗数据微调的模型）在MIMIC-CXR（胸部X射线）和FairVLMed（扫描激光检眼镜）两个不同模态的医疗成像数据集上的表现。结果揭示了模型对文本输入的显著依赖，即便存在互补的视觉信息。定性关注度分析进一步证实图像内容常被文本细节所遮蔽。

Conclusion: 本研究强调了多模态医疗模型在整合视觉和文本线索方面的重要性，而不是依赖单一模态信号。

Abstract: Clinical decision-making relies on the integrated analysis of medical images
and the associated clinical reports. While Vision-Language Models (VLMs) can
offer a unified framework for such tasks, they can exhibit strong biases toward
one modality, frequently overlooking critical visual cues in favor of textual
information. In this work, we introduce Selective Modality Shifting (SMS), a
perturbation-based approach to quantify a model's reliance on each modality in
binary classification tasks. By systematically swapping images or text between
samples with opposing labels, we expose modality-specific biases. We assess six
open-source VLMs-four generalist models and two fine-tuned for medical data-on
two medical imaging datasets with distinct modalities: MIMIC-CXR (chest X-ray)
and FairVLMed (scanning laser ophthalmoscopy). By assessing model performance
and the calibration of every model in both unperturbed and perturbed settings,
we reveal a marked dependency on text input, which persists despite the
presence of complementary visual information. We also perform a qualitative
attention-based analysis which further confirms that image content is often
overshadowed by text details. Our findings highlight the importance of
designing and evaluating multimodal medical models that genuinely integrate
visual and textual cues, rather than relying on single-modality signals.

</details>


### [9] [Graph Lineages and Skeletal Graph Products](https://arxiv.org/abs/2508.00197)
*Eric Mjolsness,Cory B. Scott*

Main category: cs.CV

TL;DR: 提出了一种新的层级图结构及其代数类型理论，能够高效应用于定义机器学习和计算科学中的层级模型架构。


<details>
  <summary>Details</summary>
Motivation: 动机在于构建一种新的层次化图结构，这种结构可以适用于机器学习和计算科学中的数学模型架构，尤其是能够实现模型的高效采样、搜索和优化。

Method: 论文通过定义结构化图的“世代”（按层级编号排序），构建了一种层级生长的图结构。利用双向图连接图的连续层级，并使用跨域映射定义图之间的距离度量，从而开发出低成本的“骨架”变体的代数图操作和类型构造器。

Result: 论文结果表明，通过这些新的代数操作和类型理论，可以创建多尺度图和搜索边界的图，提供了一种新的有效的层级模型架构定义方式，适用于神经网络和数值方法。

Conclusion: 本文提出了一种用于定义层级模型架构的代数类型理论，称为“分层架构”，能够有效地应用于深度神经网络和多网格数值方法。

Abstract: Graphs, and sequences of growing graphs, can be used to specify the
architecture of mathematical models in many fields including machine learning
and computational science. Here we define structured graph "lineages" (ordered
by level number) that grow in a hierarchical fashion, so that: (1) the number
of graph vertices and edges increases exponentially in level number; (2)
bipartite graphs connect successive levels within a graph lineage and, as in
multigrid methods, can constrain matrices relating successive levels; (3) using
prolongation maps within a graph lineage, process-derived distance measures
between graphs at successive levels can be defined; (4) a category of "graded
graphs" can be defined, and using it low-cost "skeletal" variants of standard
algebraic graph operations and type constructors (cross product, box product,
disjoint sum, and function types) can be derived for graded graphs and hence
hierarchical graph lineages; (5) these skeletal binary operators have similar
but not identical algebraic and category-theoretic properties to their standard
counterparts; (6) graph lineages and their skeletal product constructors can
approach continuum limit objects. Additional space-efficient unary operators on
graded graphs are also derived: thickening, which creates a graph lineage of
multiscale graphs, and escalation to a graph lineage of search frontiers
(useful as a generalization of adaptive grids and in defining "skeletal"
functions). The result is an algebraic type theory for graded graphs and
(hierarchical) graph lineages. The approach is expected to be well suited to
defining hierarchical model architectures - "hierarchitectures" - and local
sampling, search, or optimization algorithms on them. We demonstrate such
application to deep neural networks (including visual and feature scale spaces)
and to multigrid numerical methods.

</details>


### [10] [Learning Personalised Human Internal Cognition from External Expressive Behaviours for Real Personality Recognition](https://arxiv.org/abs/2508.00205)
*Xiangyu Kong,Hengde Zhu,Haoqin Sun,Zhihao Guo,Jiayan Gu,Xinyi Ni,Wei Zhang,Shizhe Liu,Siyang Song*

Main category: cs.CV

TL;DR: 提出了一种通过模拟个体的内部认知来识别真实性格的方案，使用二维图神经网络提高了识别质量。


<details>
  <summary>Details</summary>
Motivation: 大多数现有解决方案作为外部观察者，根据目标个体的表达行为推断观察者的性格印象，与实际性格存在显著偏差，导致较差的识别性能。

Method: 提出了一种新的真实性格识别（RPR）方法，该方法通过目标个体的外部音视频行为来模拟个性化的内部认知。模拟的认知被编码为一个包含二维节点和边特征矩阵的新图，并提出了新的二维图神经网络（2D-GNN）用于从中推断真实性格特征。

Result: 实现了个性化内在认知的有效模拟，并通过端到端的策略进行联合训练，提升了性格识别模块的性能。

Conclusion: 通过模拟个性化内部认知和结合二维图神经网络，可以更准确地识别真实性格特征。

Abstract: Automatic real personality recognition (RPR) aims to evaluate human real
personality traits from their expressive behaviours. However, most existing
solutions generally act as external observers to infer observers' personality
impressions based on target individuals' expressive behaviours, which
significantly deviate from their real personalities and consistently lead to
inferior recognition performance. Inspired by the association between real
personality and human internal cognition underlying the generation of
expressive behaviours, we propose a novel RPR approach that efficiently
simulates personalised internal cognition from easy-accessible external short
audio-visual behaviours expressed by the target individual. The simulated
personalised cognition, represented as a set of network weights that enforce
the personalised network to reproduce the individual-specific facial reactions,
is further encoded as a novel graph containing two-dimensional node and edge
feature matrices, with a novel 2D Graph Neural Network (2D-GNN) proposed for
inferring real personality traits from it. To simulate real personality-related
cognition, an end-to-end strategy is designed to jointly train our cognition
simulation, 2D graph construction, and personality recognition modules.

</details>


### [11] [SAM-PTx: Text-Guided Fine-Tuning of SAM with Parameter-Efficient, Parallel-Text Adapters](https://arxiv.org/abs/2508.00213)
*Shayan Jalilian,Abdul Bais*

Main category: cs.CV

TL;DR: 提出了一种参数高效的方法，通过CLIP文本嵌入对SAM进行调整，实现语义指导的分割性能提升。


<details>
  <summary>Details</summary>
Motivation: 探索语义文本提示在分割任务中的潜力，与传统的空间提示相比，语义文本提示未得到充分研究。

Method: 使用冻结的CLIP派生文本嵌入作为类级语义指导，通过轻量级适配器设计将文本嵌入注入SAM的图像编码器。

Result: 实验表明，在COD10K数据集以及COCO和ADE20K的低数据子集上，加入固定的文本嵌入可提高分割性能。

Conclusion: 通过使用文本提示可以提升图像分割的性能，且结合语义条件可以实现高效的适应性。

Abstract: The Segment Anything Model (SAM) has demonstrated impressive generalization
in prompt-based segmentation. Yet, the potential of semantic text prompts
remains underexplored compared to traditional spatial prompts like points and
boxes. This paper introduces SAM-PTx, a parameter-efficient approach for
adapting SAM using frozen CLIP-derived text embeddings as class-level semantic
guidance. Specifically, we propose a lightweight adapter design called
Parallel-Text that injects text embeddings into SAM's image encoder, enabling
semantics-guided segmentation while keeping most of the original architecture
frozen. Our adapter modifies only the MLP-parallel branch of each transformer
block, preserving the attention pathway for spatial reasoning. Through
supervised experiments and ablations on the COD10K dataset as well as low-data
subsets of COCO and ADE20K, we show that incorporating fixed text embeddings as
input improves segmentation performance over purely spatial prompt baselines.
To our knowledge, this is the first work to use text prompts for segmentation
on the COD10K dataset. These results suggest that integrating semantic
conditioning into SAM's architecture offers a practical and scalable path for
efficient adaptation with minimal computational complexity.

</details>


### [12] [Object-Centric Cropping for Visual Few-Shot Classification](https://arxiv.org/abs/2508.00218)
*Aymane Abdali,Bartosz Boguslawski,Lucas Drumetz,Vincent Gripon*

Main category: cs.CV

TL;DR: 通过额外的对象局部位置信息，使用Segment Anything Model或无监督方法来改善小样本图像分类。


<details>
  <summary>Details</summary>
Motivation: 小样本图像分类中，图像的模糊性（如多个对象或复杂背景）会显著影响性能，因此需要附加的局部位置信息来改善分类。

Method: 利用Segment Anything Model，仅需指出感兴趣对象的一个像素，或使用完全无监督的前景对象提取方法。

Result: 在现有基准测试上，新增的对象局部位置信息显著提升了分类性能。

Conclusion: 在小样本图像分类中，通过提供有关对象在图像中的局部位置信息，可以显著提升分类性能。

Abstract: In the domain of Few-Shot Image Classification, operating with as little as
one example per class, the presence of image ambiguities stemming from multiple
objects or complex backgrounds can significantly deteriorate performance. Our
research demonstrates that incorporating additional information about the local
positioning of an object within its image markedly enhances classification
across established benchmarks. More importantly, we show that a significant
fraction of the improvement can be achieved through the use of the Segment
Anything Model, requiring only a pixel of the object of interest to be pointed
out, or by employing fully unsupervised foreground object extraction methods.

</details>


### [13] [Guided Depth Map Super-Resolution via Multi-Scale Fusion U-shaped Mamba Network](https://arxiv.org/abs/2508.00248)
*Chenggang Guo,Hao Xu,XianMing Wan*

Main category: cs.CV

TL;DR: 提出了MSF-UM模型，通过融合Mamba的状态空间模型和多尺度结构，实现了更高效的深度图超分辨率，同时减少了模型参数，提高了精度。


<details>
  <summary>Details</summary>
Motivation: 传统的卷积神经网络在处理远程依赖时有局限性，无法充分建模深度图中的全局上下文信息；而变压器虽然可以建模全局依赖，但其计算复杂度和内存消耗较大，限制了处理高分辨率深度图的能力。

Method: 提出了一种多尺度融合U形Mamba（MSF-UM）模型，结合残差密集通道注意块和Mamba状态空间模块，采用多尺度跨模态融合策略，以色彩图像的高频纹理信息为指导进行深度图的超分辨率处理。

Result: 实验结果表明，MSF-UM模型在减少模型参数数量的同时，提高了重建精度，还展示了在大规模深度图超分辨率任务中的优秀泛化能力。

Conclusion: MSF-UM模型在大规模深度图超分辨率任务中表现出色的泛化能力，并且有效地减少了模型参数的数量，同时提高了重建精度。

Abstract: Depth map super-resolution technology aims to improve the spatial resolution
of low-resolution depth maps and effectively restore high-frequency detail
information. Traditional convolutional neural network has limitations in
dealing with long-range dependencies and are unable to fully model the global
contextual information in depth maps. Although transformer can model global
dependencies, its computational complexity and memory consumption are
quadratic, which significantly limits its ability to process high-resolution
depth maps. In this paper, we propose a multi-scale fusion U-shaped Mamba
(MSF-UM) model, a novel guided depth map super-resolution framework. The core
innovation of this model is to integrate Mamba's efficient state-space modeling
capabilities into a multi-scale U-shaped fusion structure guided by a color
image. The structure combining the residual dense channel attention block and
the Mamba state space module is designed, which combines the local feature
extraction capability of the convolutional layer with the modeling advantage of
the state space model for long-distance dependencies. At the same time, the
model adopts a multi-scale cross-modal fusion strategy to make full use of the
high-frequency texture information from the color image to guide the
super-resolution process of the depth map. Compared with existing mainstream
methods, the proposed MSF-UM significantly reduces the number of model
parameters while achieving better reconstruction accuracy. Extensive
experiments on multiple publicly available datasets validate the effectiveness
of the model, especially showing excellent generalization ability in the task
of large-scale depth map super-resolution.

</details>


### [14] [PointGauss: Point Cloud-Guided Multi-Object Segmentation for Gaussian Splatting](https://arxiv.org/abs/2508.00259)
*Wentao Sun,Hanqing Xu,Quanyun Wu,Dedong Zhang,Yiping Chen,Lingfei Ma,John S. Zelek,Jonathan Li*

Main category: cs.CV

TL;DR: PointGauss是一种新的点云引导的实时多目标分割框架，解决了现有方法的初始化时间长和多视图一致性差的问题，表现优于现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在初始化时间长及多视图一致性差的问题。为此，引入一个新框架PointGauss，以解决这些不足。

Method: 1. 点云导向的高斯原语解码器生成3D实例掩码；
2. GPU加速的2D掩码渲染系统确保多视图一致性。

Result: PointGauss在多视图mIoU上比之前的最先进方法有1.89到31.78%的性能提升，同时保持了较高的计算效率。

Conclusion: 该论文提出了一种新的实时多目标分割框架PointGauss，通过点云引导在高斯斑点表示中实现高效的3D分割。实验表明，其在多视图mIoU表现上有显著提升，并具有较高的计算效率。

Abstract: We introduce PointGauss, a novel point cloud-guided framework for real-time
multi-object segmentation in Gaussian Splatting representations. Unlike
existing methods that suffer from prolonged initialization and limited
multi-view consistency, our approach achieves efficient 3D segmentation by
directly parsing Gaussian primitives through a point cloud segmentation-driven
pipeline. The key innovation lies in two aspects: (1) a point cloud-based
Gaussian primitive decoder that generates 3D instance masks within 1 minute,
and (2) a GPU-accelerated 2D mask rendering system that ensures multi-view
consistency. Extensive experiments demonstrate significant improvements over
previous state-of-the-art methods, achieving performance gains of 1.89 to
31.78% in multi-view mIoU, while maintaining superior computational efficiency.
To address the limitations of current benchmarks (single-object focus,
inconsistent 3D evaluation, small scale, and partial coverage), we present
DesktopObjects-360, a novel comprehensive dataset for 3D segmentation in
radiance fields, featuring: (1) complex multi-object scenes, (2) globally
consistent 2D annotations, (3) large-scale training data (over 27 thousand 2D
masks), (4) full 360{\deg} coverage, and (5) 3D evaluation masks.

</details>


### [15] [Instruction-Grounded Visual Projectors for Continual Learning of Generative Vision-Language Models](https://arxiv.org/abs/2508.00260)
*Hyundong Jin,Hyung Jin Chang,Eunwoo Kim*

Main category: cs.CV

TL;DR: 本文提出了一种新框架，通过混合视觉投影仪和专家推荐策略，改进持续学习模型在视觉语言任务中的表现，解决语言指令被忽略的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的方法可能导致模型优先考虑视觉输入而忽略语言指令，尤其是在学习有重复性文字指令的任务时。我们提出一个新框架，通过基于指令的视觉信息翻译来解决这一问题。

Method: 我们引入了混合视觉投影仪，每个投影仪作为特定的视觉到语言的翻译专家，根据给定的指令上下文适应新任务。我们提出了一种专家推荐策略，用于在相似任务中重复使用专家，并引入专家修剪来减轻前期任务中累积激活的专家所造成的干扰。

Result: 我们的方法在各种视觉语言任务上表现出色，生成的响应符合指令要求，优于现有的持续学习方法。

Conclusion: 我们的方法通过生成符合指令的响应，在多种视觉语言任务上表现优于现有的持续学习方法。

Abstract: Continual learning enables pre-trained generative vision-language models
(VLMs) to incorporate knowledge from new tasks without retraining data from
previous ones. Recent methods update a visual projector to translate visual
information for new tasks, connecting pre-trained vision encoders with large
language models. However, such adjustments may cause the models to prioritize
visual inputs over language instructions, particularly learning tasks with
repetitive types of textual instructions. To address the neglect of language
instructions, we propose a novel framework that grounds the translation of
visual information on instructions for language models. We introduce a mixture
of visual projectors, each serving as a specialized visual-to-language
translation expert based on the given instruction context to adapt to new
tasks. To avoid using experts for irrelevant instruction contexts, we propose
an expert recommendation strategy that reuses experts for tasks similar to
those previously learned. Additionally, we introduce expert pruning to
alleviate interference from the use of experts that cumulatively activated in
previous tasks. Extensive experiments on diverse vision-language tasks
demonstrate that our method outperforms existing continual learning approaches
by generating instruction-following responses.

</details>


### [16] [Multimodal Referring Segmentation: A Survey](https://arxiv.org/abs/2508.00265)
*Henghui Ding,Song Tang,Shuting He,Chang Liu,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 本文综述多模态指代分割领域，重点介绍该领域背景、方法以及在图像、视频、三维场景中的应用。


<details>
  <summary>Details</summary>
Motivation: 随着卷积神经网络、Transformer和大型语言模型的进步，多模态指代分割的多模态感知能力显著提高，实用性增强，因此成为研究热点。

Method: 本文总结了一种统一的元架构用于指代分割，并回顾了在图像、视频、3D 场景中的代表性方法。同时讨论了一般化指代表达方法以应对复杂的现实世界场景。

Result: 提供了标准基准上的广泛性能比较，显示了不同方法在各场景下的优劣。

Conclusion: 通过综述该领域的发展历程和现有技术，发现多模态指代分割在处理真实世界复杂性方面已取得一定进展，同时揭示了其在相关任务和实际应用中的潜力。

Abstract: Multimodal referring segmentation aims to segment target objects in visual
scenes, such as images, videos, and 3D scenes, based on referring expressions
in text or audio format. This task plays a crucial role in practical
applications requiring accurate object perception based on user instructions.
Over the past decade, it has gained significant attention in the multimodal
community, driven by advances in convolutional neural networks, transformers,
and large language models, all of which have substantially improved multimodal
perception capabilities. This paper provides a comprehensive survey of
multimodal referring segmentation. We begin by introducing this field's
background, including problem definitions and commonly used datasets. Next, we
summarize a unified meta architecture for referring segmentation and review
representative methods across three primary visual scenes, including images,
videos, and 3D scenes. We further discuss Generalized Referring Expression
(GREx) methods to address the challenges of real-world complexity, along with
related tasks and practical applications. Extensive performance comparisons on
standard benchmarks are also provided. We continually track related works at
https://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation.

</details>


### [17] [Towards Robust Semantic Correspondence: A Benchmark and Insights](https://arxiv.org/abs/2508.00272)
*Wenyue Chong*

Main category: cs.CV

TL;DR: 本文建立了一个新的基准数据集，用于评估语义对应在不利条件下的稳健性，发现现有方法在这样的条件下性能明显下降，而DINO模型和Stable Diffusion的融合在绝对稳健性上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 语义对应在受控高质量条件下取得显著性能，但其在挑战场景中的稳健性研究较少，因此设立一个新的基准以评估其在不利条件下的表现。

Method: 通过建立新的基准数据集，该数据集包含14个反映常见成像问题的不同挑战场景，进行广泛评估以提供对语义对应方法稳健性的重要见解，并评估通用稳健性增强策略。

Result: 通过全面评估，发现语义对应方法在不利条件下性能明显下降，并且通用数据增强无效，这凸显了任务专用设计的必要性。DINO模型在相对稳健性方面表现优于Stable Diffusion，二者的融合实现了更好的绝对稳健性。

Conclusion: 现有的所有方法在不利条件下都会明显性能下降，使用大规模视觉模型可以增强总体稳健性，但对这些模型进行微调会导致相对稳健性下降。DINO模型在相对稳健性方面表现优于Stable Diffusion，二者的融合实现了更好的绝对稳健性。

Abstract: Semantic correspondence aims to identify semantically meaningful
relationships between different images and is a fundamental challenge in
computer vision. It forms the foundation for numerous tasks such as 3D
reconstruction, object tracking, and image editing. With the progress of
large-scale vision models, semantic correspondence has achieved remarkable
performance in controlled and high-quality conditions. However, the robustness
of semantic correspondence in challenging scenarios is much less investigated.
In this work, we establish a novel benchmark for evaluating semantic
correspondence in adverse conditions. The benchmark dataset comprises 14
distinct challenging scenarios that reflect commonly encountered imaging
issues, including geometric distortion, image blurring, digital artifacts, and
environmental occlusion. Through extensive evaluations, we provide several key
insights into the robustness of semantic correspondence approaches: (1) All
existing methods suffer from noticeable performance drops under adverse
conditions; (2) Using large-scale vision models can enhance overall robustness,
but fine-tuning on these models leads to a decline in relative robustness; (3)
The DINO model outperforms the Stable Diffusion in relative robustness, and
their fusion achieves better absolute robustness; Moreover, We evaluate common
robustness enhancement strategies for semantic correspondence and find that
general data augmentations are ineffective, highlighting the need for
task-specific designs. These results are consistent across both our dataset and
real-world benchmarks.

</details>


### [18] [Privacy-Preserving Driver Drowsiness Detection with Spatial Self-Attention and Federated Learning](https://arxiv.org/abs/2508.00287)
*Tran Viet Khoa,Do Hai Son,Mohammad Abu Alsheikh,Yibeltal F Alem,Dinh Thai Hoang*

Main category: cs.CV

TL;DR: 本文提出了一种基于空间自注意力和LSTM网络的新方法，用于检测驾驶员困倦状态，具有较高的准确性和鲁棒性。同时，通过梯度相似性比较和自定义工具提高了数据处理效率，在联合学习设置中表现优异。


<details>
  <summary>Details</summary>
Motivation: 准确检测司机的困倦状态是减少交通事故和交通相关死亡的重要因素。然而，由于面部数据的多样性和分散性，特别是在真实世界环境中，这项任务面临挑战。

Method: 本文提出了一种新颖的困倦检测框架，采用空间自注意力（SSA）机制和长短时记忆（LSTM）网络，以改善面部特征提取和检测性能。此外，使用梯度相似性比较（GSC）为联合学习选择最相关的训练模型，并开发了一个定制工具自动处理视频数据。

Result: 在联合学习环境下，该框架实现了89.9%的检测准确率，优于现有方法，并在各种部署场景下表现出色。

Conclusion: 本文提出的框架在处理多样化的真实世界数据时表现出色，能够在智能交通系统中实现早期和可靠的困倦检测，增强道路安全性。

Abstract: Driver drowsiness is one of the main causes of road accidents and is
recognized as a leading contributor to traffic-related fatalities. However,
detecting drowsiness accurately remains a challenging task, especially in
real-world settings where facial data from different individuals is
decentralized and highly diverse. In this paper, we propose a novel framework
for drowsiness detection that is designed to work effectively with
heterogeneous and decentralized data. Our approach develops a new Spatial
Self-Attention (SSA) mechanism integrated with a Long Short-Term Memory (LSTM)
network to better extract key facial features and improve detection
performance. To support federated learning, we employ a Gradient Similarity
Comparison (GSC) that selects the most relevant trained models from different
operators before aggregation. This improves the accuracy and robustness of the
global model while preserving user privacy. We also develop a customized tool
that automatically processes video data by extracting frames, detecting and
cropping faces, and applying data augmentation techniques such as rotation,
flipping, brightness adjustment, and zooming. Experimental results show that
our framework achieves a detection accuracy of 89.9% in the federated learning
settings, outperforming existing methods under various deployment scenarios.
The results demonstrate the effectiveness of our approach in handling
real-world data variability and highlight its potential for deployment in
intelligent transportation systems to enhance road safety through early and
reliable drowsiness detection.

</details>


### [19] [TITAN-Guide: Taming Inference-Time AligNment for Guided Text-to-Video Diffusion Models](https://arxiv.org/abs/2508.00289)
*Christian Simon,Masato Ishii,Akio Hayakawa,Zhi Zhong,Shusuke Takahashi,Takashi Shibuya,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: TITAN-Guide是一种新方法，解决了无训练指导框架的内存问题，提升了T2V扩散模型的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的无训练指导框架要么内存需求大，要么由于估计粗糙，控制效果不佳，限制了扩散模型的应用。本文旨在解决这些不足。

Method: 开发了一种高效的方法，通过无反向传播优化扩散潜变量，并研究了在指导扩散任务中使用前向梯度下降的方法。

Result: 实验结果表明，提出的方法在潜变量优化时有效管理内存，并显著提升了在一系列扩散指导基准测试中的T2V性能。

Conclusion: 提出的TITAN-Guide方法有效解决了现有无训练指导框架的内存占用和控制精度问题，显著提升了T2V扩散模型的性能。

Abstract: In the recent development of conditional diffusion models still require heavy
supervised fine-tuning for performing control on a category of tasks.
Training-free conditioning via guidance with off-the-shelf models is a
favorable alternative to avoid further fine-tuning on the base model. However,
the existing training-free guidance frameworks either have heavy memory
requirements or offer sub-optimal control due to rough estimation. These
shortcomings limit the applicability to control diffusion models that require
intense computation, such as Text-to-Video (T2V) diffusion models. In this
work, we propose Taming Inference Time Alignment for Guided Text-to-Video
Diffusion Model, so-called TITAN-Guide, which overcomes memory space issues,
and provides more optimal control in the guidance process compared to the
counterparts. In particular, we develop an efficient method for optimizing
diffusion latents without backpropagation from a discriminative guiding model.
In particular, we study forward gradient descents for guided diffusion tasks
with various options on directional directives. In our experiments, we
demonstrate the effectiveness of our approach in efficiently managing memory
during latent optimization, while previous methods fall short. Our proposed
approach not only minimizes memory requirements but also significantly enhances
T2V performance across a range of diffusion guidance benchmarks. Code, models,
and demo are available at https://titanguide.github.io.

</details>


### [20] [AniMer+: Unified Pose and Shape Estimation Across Mammalia and Aves via Family-Aware Transformer](https://arxiv.org/abs/2508.00298)
*Jin Lyu,Liang An,Li Lin,Pujin Cheng,Yebin Liu,Xiaoying Tang*

Main category: cs.CV

TL;DR: 本文介绍了AniMer+框架，通过高容量的视觉变压器和数据生成技术，实现了对哺乳动物和鸟类姿态和形状的准确估计，在多个基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 旨在通过一个单一的网络实现对不同动态对象的统一理解，特别是在生物研究中进行准确的动物姿态和形状估计，但由于现有方法的网络容量有限以及多物种数据集的缺乏，这个主题仍未得到充分探索。

Method: 引入了AniMer+框架，该框架采用了高容量、家庭感知的视觉变压器（ViT），并结合多专家设计，将网络层分为特定类群组件（哺乳动物和鸟类）和共享类群组件。同时，采用了基于扩散的条件图像生成管道来生成两个大规模合成数据集，分别针对四足动物和鸟类进行训练。

Result: 在综合收集了41.3k哺乳动物图片和12.4k鸟类图片（包括真实数据和合成数据）后，该方法在广泛的基准上显示出优于现有方法的性能，包括在具有挑战性的外域动物王国数据集上的出色表现。消融研究证实了我们的新颖网络架构和合成数据集在提升现实应用性能方面的效果。

Conclusion: AniMer+框架实现了不同动态对象的统一理解，并在动物姿态和形状估计方面提供了增强的性能。

Abstract: In the era of foundation models, achieving a unified understanding of
different dynamic objects through a single network has the potential to empower
stronger spatial intelligence. Moreover, accurate estimation of animal pose and
shape across diverse species is essential for quantitative analysis in
biological research. However, this topic remains underexplored due to the
limited network capacity of previous methods and the scarcity of comprehensive
multi-species datasets. To address these limitations, we introduce AniMer+, an
extended version of our scalable AniMer framework. In this paper, we focus on a
unified approach for reconstructing mammals (mammalia) and birds (aves). A key
innovation of AniMer+ is its high-capacity, family-aware Vision Transformer
(ViT) incorporating a Mixture-of-Experts (MoE) design. Its architecture
partitions network layers into taxa-specific components (for mammalia and aves)
and taxa-shared components, enabling efficient learning of both distinct and
common anatomical features within a single model. To overcome the critical
shortage of 3D training data, especially for birds, we introduce a
diffusion-based conditional image generation pipeline. This pipeline produces
two large-scale synthetic datasets: CtrlAni3D for quadrupeds and CtrlAVES3D for
birds. To note, CtrlAVES3D is the first large-scale, 3D-annotated dataset for
birds, which is crucial for resolving single-view depth ambiguities. Trained on
an aggregated collection of 41.3k mammalian and 12.4k avian images (combining
real and synthetic data), our method demonstrates superior performance over
existing approaches across a wide range of benchmarks, including the
challenging out-of-domain Animal Kingdom dataset. Ablation studies confirm the
effectiveness of both our novel network architecture and the generated
synthetic datasets in enhancing real-world application performance.

</details>


### [21] [Controllable Pedestrian Video Editing for Multi-View Driving Scenarios via Motion Sequence](https://arxiv.org/abs/2508.00299)
*Danzhen Fu,Jiagao Hu,Daiguo Zhou,Fei Wang,Zepeng Wang,Wenhua Liao*

Main category: cs.CV

TL;DR: 提出了一种新框架，通过视频修补和人体动作控制技术实现多视角驾驶场景中的可控行人视频编辑，提升了行人检测的鲁棒性，并在数据增强和场景模拟中具有潜力。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统中行人检测模型常因训练数据集中缺乏对危险行人情景的适当表征而表现出鲁棒性不足。

Method: 提出了一种新颖的框架，通过整合视频修补和人体动作控制技术进行多视角驾驶场景中的可控行人视频编辑。该方法从多视角摄像头中识别行人兴趣区域，扩展检测边界框并重设尺寸，将这些区域整合到统一画布中，同时保持跨视角的空间关系。

Result: 大量实验证明，该框架实现了高质量的行人编辑，具备较强的视觉真实感、时空一致性和跨视角一致性。

Conclusion: 所提出的方法被确立为一种稳健且多用途的多视角行人视频生成解决方案，在数据增强和自动驾驶场景模拟中具有广泛的应用潜力。

Abstract: Pedestrian detection models in autonomous driving systems often lack
robustness due to insufficient representation of dangerous pedestrian scenarios
in training datasets. To address this limitation, we present a novel framework
for controllable pedestrian video editing in multi-view driving scenarios by
integrating video inpainting and human motion control techniques. Our approach
begins by identifying pedestrian regions of interest across multiple camera
views, expanding detection bounding boxes with a fixed ratio, and resizing and
stitching these regions into a unified canvas while preserving cross-view
spatial relationships. A binary mask is then applied to designate the editable
area, within which pedestrian editing is guided by pose sequence control
conditions. This enables flexible editing functionalities, including pedestrian
insertion, replacement, and removal. Extensive experiments demonstrate that our
framework achieves high-quality pedestrian editing with strong visual realism,
spatiotemporal coherence, and cross-view consistency. These results establish
the proposed method as a robust and versatile solution for multi-view
pedestrian video generation, with broad potential for applications in data
augmentation and scenario simulation in autonomous driving.

</details>


### [22] [Exploring Fourier Prior and Event Collaboration for Low-Light Image Enhancement](https://arxiv.org/abs/2508.00308)
*Chunyan She,Fujun Han,Chengyu Fang,Shukai Duan,Lidan Wang*

Main category: cs.CV

TL;DR: 提出了一种事件相机的低光图像增强方法，通过两阶段管道提升性能，实验优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有事件相机方法未充分利用模态特定优势，直接将帧和事件输入单一模型，表现受限。

Method: 在可见性恢复阶段，设计了傅里叶空间中幅值-相位纠缠的可见性恢复网络；在结构优化阶段，提出了动态对齐的融合策略，并利用空间频率插值生成对比损失的负样本。

Result: 实验结果表明，所提出的方法优于现有最先进模型。

Conclusion: 本研究提出了一种具有两个阶段的低光图像增强新方法，有效提升了现有事件相机方法的性能。

Abstract: The event camera, benefiting from its high dynamic range and low latency,
provides performance gain for low-light image enhancement. Unlike frame-based
cameras, it records intensity changes with extremely high temporal resolution,
capturing sufficient structure information. Currently, existing event-based
methods feed a frame and events directly into a single model without fully
exploiting modality-specific advantages, which limits their performance.
Therefore, by analyzing the role of each sensing modality, the enhancement
pipeline is decoupled into two stages: visibility restoration and structure
refinement. In the first stage, we design a visibility restoration network with
amplitude-phase entanglement by rethinking the relationship between amplitude
and phase components in Fourier space. In the second stage, a fusion strategy
with dynamic alignment is proposed to mitigate the spatial mismatch caused by
the temporal resolution discrepancy between two sensing modalities, aiming to
refine the structure information of the image enhanced by the visibility
restoration network. In addition, we utilize spatial-frequency interpolation to
simulate negative samples with diverse illumination, noise and artifact
degradations, thereby developing a contrastive loss that encourages the model
to learn discriminative representations. Experiments demonstrate that the
proposed method outperforms state-of-the-art models.

</details>


### [23] [DocTron-Formula: Generalized Formula Recognition in Complex and Structured Scenarios](https://arxiv.org/abs/2508.00311)
*Yufeng Zhong,Zhixiong Zeng,Lei Chen,Longrong Yang,Liming Zheng,Jing Huang,Siqi Yang,Lin Ma*

Main category: cs.CV

TL;DR: 提出DocTron-Formula框架和CSFormula数据集，通过简单微调实现了复杂数学公式的OCR，在准确性和鲁棒性上超过专门化模型。


<details>
  <summary>Details</summary>
Motivation: 数学公式的光学字符识别对科学文献的智能分析至关重要。然而，任务特定和通用的视觉语言模型通常难以处理数学内容中固有的结构多样性、复杂性和真实世界的变异性。

Method: 研究中提出了一种基于通用视觉语言模型的统一框架，名为DocTron-Formula，并引入了一个大规模且具有挑战性的CSFormula数据集。

Result: 实验结果表明，所提出的方法不仅在准确性和鲁棒性方面超过了专门化模型，并且为复杂科学文档的自动化理解设立了新标准。

Conclusion: 本研究通过简单的监督微调方法实现了在多种风格、科学领域和复杂布局上的最先进性能。它在准确性和鲁棒性方面超过了专门化模型，并为复杂科学文档的自动化理解建立了新的范式。

Abstract: Optical Character Recognition (OCR) for mathematical formula is essential for
the intelligent analysis of scientific literature. However, both task-specific
and general vision-language models often struggle to handle the structural
diversity, complexity, and real-world variability inherent in mathematical
content. In this work, we present DocTron-Formula, a unified framework built
upon general vision-language models, thereby eliminating the need for
specialized architectures. Furthermore, we introduce CSFormula, a large-scale
and challenging dataset that encompasses multidisciplinary and structurally
complex formulas at the line, paragraph, and page levels. Through
straightforward supervised fine-tuning, our approach achieves state-of-the-art
performance across a variety of styles, scientific domains, and complex
layouts. Experimental results demonstrate that our method not only surpasses
specialized models in terms of accuracy and robustness, but also establishes a
new paradigm for the automated understanding of complex scientific documents.

</details>


### [24] [GV-VAD : Exploring Video Generation for Weakly-Supervised Video Anomaly Detection](https://arxiv.org/abs/2508.00312)
*Suhang Cai,Xiaohao Peng,Chong Wang,Xiaojie Cai,Jiangbo Qian*

Main category: cs.CV

TL;DR: 提出了一种生成性视频增强弱监督视频异常检测框架，通过合成视频加强训练数据，提升性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中异常事件的稀少性、不可预测性和高标注成本，使得难以扩展VAD数据集，限制了现有模型的性能和泛化能力。

Method: 生成性视频增强弱监督视频异常检测框架（GV-VAD），通过文本条件视频生成模型生成语义可控且物理合理的合成视频，并用于增强低成本训练数据。

Result: 实验表明，该框架在UCF-Crime数据集上优于最新的方法。

Conclusion: GV-VAD框架通过使用合成视频和损失缩放策略提高训练效率和精度，能够有效增强视频异常检测性能。

Abstract: Video anomaly detection (VAD) plays a critical role in public safety
applications such as intelligent surveillance. However, the rarity,
unpredictability, and high annotation cost of real-world anomalies make it
difficult to scale VAD datasets, which limits the performance and
generalization ability of existing models. To address this challenge, we
propose a generative video-enhanced weakly-supervised video anomaly detection
(GV-VAD) framework that leverages text-conditioned video generation models to
produce semantically controllable and physically plausible synthetic videos.
These virtual videos are used to augment training data at low cost. In
addition, a synthetic sample loss scaling strategy is utilized to control the
influence of generated synthetic samples for efficient training. The
experiments show that the proposed framework outperforms state-of-the-art
methods on UCF-Crime datasets. The code is available at
https://github.com/Sumutan/GV-VAD.git.

</details>


### [25] [Steering Guidance for Personalized Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.00319)
*Sunghyun Park,Seokeon Choi,Hyoungwoo Park,Sungrack Yun*

Main category: cs.CV

TL;DR: 提出个性化指导以改善文本对齐和目标分布保真度，无需额外计算开销，并能够与多种微调策略结合。


<details>
  <summary>Details</summary>
Motivation: 在个性化文本到图像扩散模型中，微调预训练模型以适应特定目标概念是至关重要的，因为这样能够实现多样化的图像生成。然而，使用少量图像进行微调时，存在在与目标分布对齐（比如主题保真度）和保留原始模型的广泛知识（比如文本编辑能力）之间的固有权衡。为了应对这些局限性，本文提出了一种称为个性化指导的方法，以改善输出的文本对齐和目标分布保真度。

Method: 本文提出了一种称为个性化指导的方法，该方法利用未学习的弱模型，并在推理过程中在预训练和微调模型之间进行权重插值，动态控制弱模型中未学习的程度。在实验中证明，该方法无需额外的计算开销，即可显式引导输出朝向平衡的潜在空间。

Result: 实验结果表明，提出的个性化指导方法能够改善文本对齐和目标分布的保真度，并可与各种微调策略顺利结合。

Conclusion: 个性化指导方法能够在维持计算效率的前提下，显著提高文本对齐和目标分布保真度，并且能够与不同的微调策略无缝集成。

Abstract: Personalizing text-to-image diffusion models is crucial for adapting the
pre-trained models to specific target concepts, enabling diverse image
generation. However, fine-tuning with few images introduces an inherent
trade-off between aligning with the target distribution (e.g., subject
fidelity) and preserving the broad knowledge of the original model (e.g., text
editability). Existing sampling guidance methods, such as classifier-free
guidance (CFG) and autoguidance (AG), fail to effectively guide the output
toward well-balanced space: CFG restricts the adaptation to the target
distribution, while AG compromises text alignment. To address these
limitations, we propose personalization guidance, a simple yet effective method
leveraging an unlearned weak model conditioned on a null text prompt. Moreover,
our method dynamically controls the extent of unlearning in a weak model
through weight interpolation between pre-trained and fine-tuned models during
inference. Unlike existing guidance methods, which depend solely on guidance
scales, our method explicitly steers the outputs toward a balanced latent space
without additional computational overhead. Experimental results demonstrate
that our proposed guidance can improve text alignment and target distribution
fidelity, integrating seamlessly with various fine-tuning strategies.

</details>


### [26] [Spectral Sensitivity Estimation with an Uncalibrated Diffraction Grating](https://arxiv.org/abs/2508.00330)
*Lilika Makabe,Hiroaki Santo,Fumio Okura,Michael S. Brown,Yasuyuki Matsushita*

Main category: cs.CV

TL;DR: 本研究提出了一种使用衍射光栅进行摄像机光谱灵敏度校准的新方法，简化了操作并提升了准确性，在合成及真实数据试验中优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 准确校准摄像机光谱灵敏度对包括颜色校正、光照估计和材料分析在内的多种计算机视觉任务至关重要。

Method: 该方法通过衍射光栅来进行校准，无需专门的窄带滤波器或已知光谱反射目标，仅需一个未经校准的衍射光栅片，通过捕捉直接光照和通过光栅片的衍射图案来闭合估算摄像机光谱灵敏度及衍射光栅参数。

Result: 实验结果显示，该方法在合成数据和真实数据上优于传统参考目标基础的方法，证明了其有效性和实用性。

Conclusion: 本论文引入了一种实用且准确的摄像机光谱灵敏度校准方法，该方法采用衍射光栅，比传统方法更加有效实用。

Abstract: This paper introduces a practical and accurate calibration method for camera
spectral sensitivity using a diffraction grating. Accurate calibration of
camera spectral sensitivity is crucial for various computer vision tasks,
including color correction, illumination estimation, and material analysis.
Unlike existing approaches that require specialized narrow-band filters or
reference targets with known spectral reflectances, our method only requires an
uncalibrated diffraction grating sheet, readily available off-the-shelf. By
capturing images of the direct illumination and its diffracted pattern through
the grating sheet, our method estimates both the camera spectral sensitivity
and the diffraction grating parameters in a closed-form manner. Experiments on
synthetic and real-world data demonstrate that our method outperforms
conventional reference target-based methods, underscoring its effectiveness and
practicality.

</details>


### [27] [Analyze-Prompt-Reason: A Collaborative Agent-Based Framework for Multi-Image Vision-Language Reasoning](https://arxiv.org/abs/2508.00356)
*Angelos Vlachos,Giorgos Filandrianos,Maria Lymperaiou,Nikolaos Spanos,Ilias Mitsouras,Vasileios Karampinis,Athanasios Voulodimos*

Main category: cs.CV

TL;DR: 提出了一种用于多图像推理的协作代理框架，通过双代理系统（PromptEngineer和VisionReasoner）实现高效推理，取得了优异的评估结果。


<details>
  <summary>Details</summary>
Motivation: 解决在不同数据集和任务格式上交织的多模态推理的挑战。

Method: 采用双代理系统，其中语言基础的PromptEngineer生成上下文感知的任务特定提示，VisionReasoner进行最终推理。该框架是全自动的、模块化和免训练的。

Result: 在2025 MIRAGE挑战赛（Track A）的18个数据集上进行评估，结果表明Claude 3.7在TQA、DocVQA和MMCoQA等任务中取得了接近顶峰的性能。

Conclusion: LVLMs能够在指导性提示下有效地进行多图像推理，取得了优异的性能。

Abstract: We present a Collaborative Agent-Based Framework for Multi-Image Reasoning.
Our approach tackles the challenge of interleaved multimodal reasoning across
diverse datasets and task formats by employing a dual-agent system: a
language-based PromptEngineer, which generates context-aware, task-specific
prompts, and a VisionReasoner, a large vision-language model (LVLM) responsible
for final inference. The framework is fully automated, modular, and
training-free, enabling generalization across classification, question
answering, and free-form generation tasks involving one or multiple input
images. We evaluate our method on 18 diverse datasets from the 2025 MIRAGE
Challenge (Track A), covering a broad spectrum of visual reasoning tasks
including document QA, visual comparison, dialogue-based understanding, and
scene-level inference. Our results demonstrate that LVLMs can effectively
reason over multiple images when guided by informative prompts. Notably, Claude
3.7 achieves near-ceiling performance on challenging tasks such as TQA (99.13%
accuracy), DocVQA (96.87%), and MMCoQA (75.28 ROUGE-L). We also explore how
design choices-such as model selection, shot count, and input length-influence
the reasoning performance of different LVLMs.

</details>


### [28] [Stable at Any Speed: Speed-Driven Multi-Object Tracking with Learnable Kalman Filtering](https://arxiv.org/abs/2508.00358)
*Yan Gong,Mengjun Chen,Hao Liu,Gao Yongsheng,Lei Yang,Naibang Wang,Ziying Song,Haoqun Ma*

Main category: cs.CV

TL;DR: 本文提出的SG-LKF通过考虑自车速度变化，提高了多目标跟踪在动态场景中的表现。


<details>
  <summary>Details</summary>
Motivation: 提高多目标跟踪在动态、高速场景下的稳定性和准确性。

Method: 提出了速度引导的可学习Kalman滤波器（SG-LKF），结合MotionScaleNet和自监督轨迹一致性损失。

Result: SG-LKF在KITTI 2D MOT和3D MOT上表现优异，并在nuScenes 3D MOT上优于SimpleTrack。

Conclusion: 将运动速度引入多目标跟踪的误差建模可以显著提升动态场景下的稳定性和准确性。

Abstract: Multi-object tracking (MOT) enables autonomous vehicles to continuously
perceive dynamic objects, supplying essential temporal cues for prediction,
behavior understanding, and safe planning. However, conventional
tracking-by-detection methods typically rely on static coordinate
transformations based on ego-vehicle poses, disregarding ego-vehicle
speed-induced variations in observation noise and reference frame changes,
which degrades tracking stability and accuracy in dynamic, high-speed
scenarios. In this paper, we investigate the critical role of ego-vehicle speed
in MOT and propose a Speed-Guided Learnable Kalman Filter (SG-LKF) that
dynamically adapts uncertainty modeling to ego-vehicle speed, significantly
improving stability and accuracy in highly dynamic scenarios. Central to SG-LKF
is MotionScaleNet (MSNet), a decoupled token-mixing and channel-mixing MLP that
adaptively predicts key parameters of SG-LKF. To enhance inter-frame
association and trajectory continuity, we introduce a self-supervised
trajectory consistency loss jointly optimized with semantic and positional
constraints. Extensive experiments show that SG-LKF ranks first among all
vision-based methods on KITTI 2D MOT with 79.59% HOTA, delivers strong results
on KITTI 3D MOT with 82.03% HOTA, and outperforms SimpleTrack by 2.2% AMOTA on
nuScenes 3D MOT.

</details>


### [29] [CoST: Efficient Collaborative Perception From Unified Spatiotemporal Perspective](https://arxiv.org/abs/2508.00359)
*Zongheng Tang,Yi Liu,Yifan Sun,Yulu Gao,Jinyu Chen,Runsheng Xu,Si Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新型协作感知方法CoST，能有效提高效率和准确性，并兼容先前大部分方法。


<details>
  <summary>Details</summary>
Motivation: 以往方法将多代理融合和多时间融合分为两个连续步骤，而本文希望通过将多代理和多时间融合合并到统一时空聚合中，提高感知的性能。

Method: 提出了一种协作感知的方法，即协作感知时空变换器（CoST），可以同时聚合不同代理的观察（空间）和不同时间的观察，形成统一的时空空间。

Result: CoST在感知性能上取得了效率和精度的提升，同时减少了传输带宽的需求。

Conclusion: CoST方法改进了协作感知的效率和准确性，同时不依赖于特定方法，兼容绝大多数先前方法。

Abstract: Collaborative perception shares information among different agents and helps
solving problems that individual agents may face, e.g., occlusions and small
sensing range. Prior methods usually separate the multi-agent fusion and
multi-time fusion into two consecutive steps. In contrast, this paper proposes
an efficient collaborative perception that aggregates the observations from
different agents (space) and different times into a unified spatio-temporal
space simultanesouly. The unified spatio-temporal space brings two benefits,
i.e., efficient feature transmission and superior feature fusion. 1) Efficient
feature transmission: each static object yields a single observation in the
spatial temporal space, and thus only requires transmission only once (whereas
prior methods re-transmit all the object features multiple times). 2) superior
feature fusion: merging the multi-agent and multi-time fusion into a unified
spatial-temporal aggregation enables a more holistic perspective, thereby
enhancing perception performance in challenging scenarios. Consequently, our
Collaborative perception with Spatio-temporal Transformer (CoST) gains
improvement in both efficiency and accuracy. Notably, CoST is not tied to any
specific method and is compatible with a majority of previous methods,
enhancing their accuracy while reducing the transmission bandwidth.

</details>


### [30] [Honey Classification using Hyperspectral Imaging and Machine Learning](https://arxiv.org/abs/2508.00361)
*Mokhtar A. Al-Awadhi,Ratnadeep R. Deshmukh*

Main category: cs.CV

TL;DR: 提出了一种基于机器学习的方法，自动分类蜂蜜植物来源。通过数据集准备、特征提取（LDA）和分类（SVM和KNN）步骤，达到了95.13%的高分类准确率。


<details>
  <summary>Details</summary>
Motivation: 论文的动机是为了自动分类蜂蜜植物来源，提升分类的准确性和效率。

Method: 该论文方法的核心步骤包括数据集准备、特征提取和分类。数据集准备阶段采用类转换方法以最大化类间可分性；特征提取阶段使用线性判别分析（LDA）技术提取相关特征并减少维度；分类阶段使用支持向量机（SVM）和K近邻（KNN）模型将蜂蜜样本的提取特征分类到其植物来源。

Result: 实验结果表明，所提出的系统在该标准数据集上的表现达到了当前最先进的水平，基于高光谱图像的分类准确率为95.13%，基于高光谱实例的分类准确率为92.80%。

Conclusion: 该论文提出的方法在标准蜂蜜高光谱成像（HSI）数据集上表现出色，实现了基于高光谱图像的95.13%的分类准确率和基于高光谱实例的92.80%的分类准确率，达到了当前最先进的结果。

Abstract: In this paper, we propose a machine learning-based method for automatically
classifying honey botanical origins. Dataset preparation, feature extraction,
and classification are the three main steps of the proposed method. We use a
class transformation method in the dataset preparation phase to maximize the
separability across classes. The feature extraction phase employs the Linear
Discriminant Analysis (LDA) technique for extracting relevant features and
reducing the number of dimensions. In the classification phase, we use Support
Vector Machines (SVM) and K-Nearest Neighbors (KNN) models to classify the
extracted features of honey samples into their botanical origins. We evaluate
our system using a standard honey hyperspectral imaging (HSI) dataset.
Experimental findings demonstrate that the proposed system produces
state-of-the-art results on this dataset, achieving the highest classification
accuracy of 95.13% for hyperspectral image-based classification and 92.80% for
hyperspectral instance-based classification.

</details>


### [31] [SparseRecon: Neural Implicit Surface Reconstruction from Sparse Views with Feature and Depth Consistencies](https://arxiv.org/abs/2508.00366)
*Liang Han,Xu Zhang,Haichuan Song,Kanle Shi,Yu-Shen Liu,Zhizhong Han*

Main category: cs.CV

TL;DR: SparseRecon 提供了一种新的稀疏视图神经隐式重建方法，通过特征一致性损失和不确定性引导深度约束提升了重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有的稀疏视图重建方法在未见过的视图或几何线索有限的情况下表现不佳，因此需要一种新的方法来提升稀疏视图重建的质量。

Method: SparseRecon 使用基于体渲染的特征一致性和不确定性引导深度约束的方法。包括跨视图的特征一致性损失及不确定性引导的深度约束。

Result: 实验结果表明，SparseRecon 能够在稀疏视图条件下生成高质量的几何形状，超越了现有的最先进方法。

Conclusion: SparseRecon 比现有的方法在稀疏视图的几何重建中表现更佳，尤其是在重叠视图较少的场景中。

Abstract: Surface reconstruction from sparse views aims to reconstruct a 3D shape or
scene from few RGB images. The latest methods are either generalization-based
or overfitting-based. However, the generalization-based methods do not
generalize well on views that were unseen during training, while the
reconstruction quality of overfitting-based methods is still limited by the
limited geometry clues. To address this issue, we propose SparseRecon, a novel
neural implicit reconstruction method for sparse views with volume
rendering-based feature consistency and uncertainty-guided depth constraint.
Firstly, we introduce a feature consistency loss across views to constrain the
neural implicit field. This design alleviates the ambiguity caused by
insufficient consistency information of views and ensures completeness and
smoothness in the reconstruction results. Secondly, we employ an
uncertainty-guided depth constraint to back up the feature consistency loss in
areas with occlusion and insignificant features, which recovers geometry
details for better reconstruction quality. Experimental results demonstrate
that our method outperforms the state-of-the-art methods, which can produce
high-quality geometry with sparse-view input, especially in the scenarios with
small overlapping views. Project page: https://hanl2010.github.io/SparseRecon/.

</details>


### [32] [Representation Shift: Unifying Token Compression with FlashAttention](https://arxiv.org/abs/2508.00367)
*Joonmyung Choi,Sanghyeok Lee,Byungoh Ko,Eunseo Kim,Jihyung Kil,Hyunwoo J. Kim*

Main category: cs.CV

TL;DR: 提出了表示转换方法，与FlashAttention兼容，实现了有效的token压缩，提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 增加的任务复杂性导致更大的模型和更多的tokens，提高了自注意力的计算成本和GPU内存访问开销。

Method: 提出了一种训练无关、与模型无关的度量——表示转换，测量每个token表示的变化程度，无需注意力图或重新训练。

Result: 表示转换实现了与FlashAttention兼容的有效token压缩，在视频文本检索和视频问答中分别实现了显著的速度提升，高达5.5%和4.4%。

Conclusion: 表示转换方法无缝整合了token压缩与FlashAttention，为Transformer、CNN和状态空间模型的效率提升提供了新的思路。

Abstract: Transformers have demonstrated remarkable success across vision, language,
and video. Yet, increasing task complexity has led to larger models and more
tokens, raising the quadratic cost of self-attention and the overhead of GPU
memory access. To reduce the computation cost of self-attention, prior work has
proposed token compression techniques that drop redundant or less informative
tokens. Meanwhile, fused attention kernels such as FlashAttention have been
developed to alleviate memory overhead by avoiding attention map construction
and its associated I/O to HBM. This, however, makes it incompatible with most
training-free token compression methods, which rely on attention maps to
determine token importance. Here, we propose Representation Shift, a
training-free, model-agnostic metric that measures the degree of change in each
token's representation. This seamlessly integrates token compression with
FlashAttention, without attention maps or retraining. Our method further
generalizes beyond Transformers to CNNs and state space models. Extensive
experiments show that Representation Shift enables effective token compression
compatible with FlashAttention, yielding significant speedups of up to 5.5% and
4.4% in video-text retrieval and video QA, respectively. Code is available at
https://github.com/mlvlab/Representation-Shift.

</details>


### [33] [Bidirectional Action Sequence Learning for Long-term Action Anticipation with Large Language Models](https://arxiv.org/abs/2508.00374)
*Yuji Sato,Yasunori Ishii,Takayoshi Yamashita*

Main category: cs.CV

TL;DR: BiAnt是一种结合前向和后向预测的新方法，改善了长期动作预测的性能。


<details>
  <summary>Details</summary>
Motivation: 为了在自动驾驶和机器人等领域进行早期风险检测，有必要进行视频基础的长期动作预测。传统的方法通过编码器提取过往行为的特征，并通过解码器进行未来事件的预测。

Method: 提出了一种称为BiAnt的新方法，它结合了前向预测和使用大语言模型的后向预测。

Result: 实验结果显示，BiAnt在Ego4D数据集上相较于基线方法在编辑距离方面的表现有所提升。

Conclusion: BiAnt方法解决了传统方法在场景中捕捉语义上不同子动作的困难，提升了性能。

Abstract: Video-based long-term action anticipation is crucial for early risk detection
in areas such as automated driving and robotics. Conventional approaches
extract features from past actions using encoders and predict future events
with decoders, which limits performance due to their unidirectional nature.
These methods struggle to capture semantically distinct sub-actions within a
scene. The proposed method, BiAnt, addresses this limitation by combining
forward prediction with backward prediction using a large language model.
Experimental results on Ego4D demonstrate that BiAnt improves performance in
terms of edit distance compared to baseline methods.

</details>


### [34] [Advancing Welding Defect Detection in Maritime Operations via Adapt-WeldNet and Defect Detection Interpretability Analysis](https://arxiv.org/abs/2508.00381)
*Kamal Basha S,Athira Nambiar*

Main category: cs.CV

TL;DR: 本研究提出“Adapt-WeldNet”框架，优化焊接缺陷检测，并通过可解释性分析增强系统透明度与可靠性。


<details>
  <summary>Details</summary>
Motivation: 焊接缺陷检测对于确保石油和天然气行业中管道系统的安全性和可靠性至关重要，特别是在具有挑战性的海洋和海上环境中。传统的无损检测方法常常无法检测到微妙或内部缺陷，导致潜在故障和昂贵的停机。而现有的基于神经网络的缺陷分类方法常常依赖于随意选择的预训练架构，缺乏可解释性，导致部署安全性问题。

Method: 引入了一种名为“Adapt-WeldNet”的自适应框架，系统地评估各种预训练架构、迁移学习策略和自适应优化器，识别最佳性能模型和超参数。此外，引入了一种新的缺陷检测可解释性分析（DDIA）框架，利用可解释AI技术（如Grad-CAM和LIME）以及经过认证的ASNT NDE Level II专业人员的领域特定评估，增强系统透明度。

Result: 提出的“Adapt-WeldNet”框架有效优化了缺陷检测，为系统透明度提供了可操作的见解。通过引入人机交互和可信AI原则，DDIA确保了缺陷检测系统的可靠性、公正性和可问责性，通过专家验证提高了对自动化决策的信心。

Conclusion: 通过改进性能和可解释性，这项工作提高了焊接缺陷检测系统的信任、安全性和可靠性，支持海上和海洋环境中的关键操作。

Abstract: Weld defect detection is crucial for ensuring the safety and reliability of
piping systems in the oil and gas industry, especially in challenging marine
and offshore environments. Traditional non-destructive testing (NDT) methods
often fail to detect subtle or internal defects, leading to potential failures
and costly downtime. Furthermore, existing neural network-based approaches for
defect classification frequently rely on arbitrarily selected pretrained
architectures and lack interpretability, raising safety concerns for
deployment. To address these challenges, this paper introduces
``Adapt-WeldNet", an adaptive framework for welding defect detection that
systematically evaluates various pre-trained architectures, transfer learning
strategies, and adaptive optimizers to identify the best-performing model and
hyperparameters, optimizing defect detection and providing actionable insights.
Additionally, a novel Defect Detection Interpretability Analysis (DDIA)
framework is proposed to enhance system transparency. DDIA employs Explainable
AI (XAI) techniques, such as Grad-CAM and LIME, alongside domain-specific
evaluations validated by certified ASNT NDE Level II professionals.
Incorporating a Human-in-the-Loop (HITL) approach and aligning with the
principles of Trustworthy AI, DDIA ensures the reliability, fairness, and
accountability of the defect detection system, fostering confidence in
automated decisions through expert validation. By improving both performance
and interpretability, this work enhances trust, safety, and reliability in
welding defect detection systems, supporting critical operations in offshore
and marine environments.

</details>


### [35] [$MV_{Hybrid}$: Improving Spatial Transcriptomics Prediction with Hybrid State Space-Vision Transformer Backbone in Pathology Vision Foundation Models](https://arxiv.org/abs/2508.00383)
*Won June Cho,Hongjun Yoon,Daeky Jeong,Hyeongyeol Lim,Yosep Chong*

Main category: cs.CV

TL;DR: $MV_{Hybrid}$通过组合SSMs和ViT，超越现有ViT模型，在基因表达预测及其他任务中表现出色，提供了一种实用的替代方案，代码已公开。


<details>
  <summary>Details</summary>
Motivation: 常规组织病理学图像预测空间基因表达提供了一种实用的替代方案，但现有基于ViT的视觉基础模型(VFMs)在临床标准下表现不佳。我们假设超越ViTs的架构创新可能更好地捕捉与分子表型相关的低频微妙形态模式。

Method: 提出了一种称为$MV_{Hybrid}$的新型混合背骨架构，结合了状态空间模型(SSMs)和ViT，并通过DINOv2自监督学习方法在相同的结直肠癌数据集上预训练。使用随机分割和留一研究(Loso)的设置对所有预训练模型进行评估。

Result: 在Loso评估中，$MV_{Hybrid}$比最佳的ViT高出57%的相关性，并在基因表达预测中显示出43%更小的性能降级。在分类、补丁检索和生存预测任务中，$MV_{Hybrid}$的下游性能与ViT相等或更好。

Conclusion: $MV_{Hybrid}$展示出优越的性能和鲁棒性，在基因表达预测中显著优于现有的ViT模型，并在分类、补丁检索和生存预测任务中表现得同样或更好。

Abstract: Spatial transcriptomics reveals gene expression patterns within tissue
context, enabling precision oncology applications such as treatment response
prediction, but its high cost and technical complexity limit clinical adoption.
Predicting spatial gene expression (biomarkers) from routine histopathology
images offers a practical alternative, yet current vision foundation models
(VFMs) in pathology based on Vision Transformer (ViT) backbones perform below
clinical standards. Given that VFMs are already trained on millions of diverse
whole slide images, we hypothesize that architectural innovations beyond ViTs
may better capture the low-frequency, subtle morphological patterns correlating
with molecular phenotypes. By demonstrating that state space models initialized
with negative real eigenvalues exhibit strong low-frequency bias, we introduce
$MV_{Hybrid}$, a hybrid backbone architecture combining state space models
(SSMs) with ViT. We compare five other different backbone architectures for
pathology VFMs, all pretrained on identical colorectal cancer datasets using
the DINOv2 self-supervised learning method. We evaluate all pretrained models
using both random split and leave-one-study-out (LOSO) settings of the same
biomarker dataset. In LOSO evaluation, $MV_{Hybrid}$ achieves 57% higher
correlation than the best-performing ViT and shows 43% smaller performance
degradation compared to random split in gene expression prediction,
demonstrating superior performance and robustness, respectively. Furthermore,
$MV_{Hybrid}$ shows equal or better downstream performance in classification,
patch retrieval, and survival prediction tasks compared to that of ViT, showing
its promise as a next-generation pathology VFM backbone. Our code is publicly
available at: https://github.com/deepnoid-ai/MVHybrid.

</details>


### [36] [Cued-Agent: A Collaborative Multi-Agent System for Automatic Cued Speech Recognition](https://arxiv.org/abs/2508.00391)
*Guanjie Huang,Danny H. K. Tsang,Shan Yang,Guangzhi Lei,Li Liu*

Main category: cs.CV

TL;DR: 提出多智能体系统Cued-Agent，通过结合多模态大语言模型、Transformer等方法，提升ACSR性能，实验结果优于现有先进方法。


<details>
  <summary>Details</summary>
Motivation: 由于手势和口型运动的时间异步性，现有方法在数据限制下无法充分训练多模态融合机制，导致性能不佳。多智能体系统在数据有限的复杂任务处理中显示出潜力，因此提出Cued-Agent来解决这一问题。

Method: 本文提出了第一个用于ACSR的协作多智能体系统Cued-Agent，包含四个子智能体，分别为基于大规模语言模型的手部识别智能体、预训练的Transformer口唇识别智能体、手势提示解码智能体和自我校正音素到词语转换智能体。

Result: 扩展了Mandarin CS数据集，进行了大量实验，并验证了Cued-Agent在各类场景中的优越性。

Conclusion: 研究表明，Cued-Agent在正常和听障场景下的表现均优于现有最先进的方法，展示了其优越的性能。

Abstract: Cued Speech (CS) is a visual communication system that combines lip-reading
with hand coding to facilitate communication for individuals with hearing
impairments. Automatic CS Recognition (ACSR) aims to convert CS hand gestures
and lip movements into text via AI-driven methods. Traditionally, the temporal
asynchrony between hand and lip movements requires the design of complex
modules to facilitate effective multimodal fusion. However, constrained by
limited data availability, current methods demonstrate insufficient capacity
for adequately training these fusion mechanisms, resulting in suboptimal
performance. Recently, multi-agent systems have shown promising capabilities in
handling complex tasks with limited data availability. To this end, we propose
the first collaborative multi-agent system for ACSR, named Cued-Agent. It
integrates four specialized sub-agents: a Multimodal Large Language Model-based
Hand Recognition agent that employs keyframe screening and CS expert prompt
strategies to decode hand movements, a pretrained Transformer-based Lip
Recognition agent that extracts lip features from the input video, a Hand
Prompt Decoding agent that dynamically integrates hand prompts with lip
features during inference in a training-free manner, and a Self-Correction
Phoneme-to-Word agent that enables post-process and end-to-end conversion from
phoneme sequences to natural language sentences for the first time through
semantic refinement. To support this study, we expand the existing Mandarin CS
dataset by collecting data from eight hearing-impaired cuers, establishing a
mixed dataset of fourteen subjects. Extensive experiments demonstrate that our
Cued-Agent performs superbly in both normal and hearing-impaired scenarios
compared with state-of-the-art methods. The implementation is available at
https://github.com/DennisHgj/Cued-Agent.

</details>


### [37] [Decouple before Align: Visual Disentanglement Enhances Prompt Tuning](https://arxiv.org/abs/2508.00395)
*Fei Zhang,Tianfei Zhou,Jiangchao Yao,Ya Zhang,Ivor W. Tsang,Yanfeng Wang*

Main category: cs.CV

TL;DR: 提出DAPT框架解决PT信息不对称问题，显著提高视觉语言模型性能，适用于少样本和数据高效学习。


<details>
  <summary>Details</summary>
Motivation: PT未被关注的信息不对称问题，其中视觉模态传递的上下文信息比对象导向的文本模态更多。粗略地对齐这两种模态可能导致偏倚注意力，使模型只是专注于上下文区域。

Method: DAPT，通过粗分-精分视觉分割线索，将视觉模态显式解耦为前景和背景表示，然后与原始前景文本和手工制作的背景类别对齐。为前景-背景模式量身定制视觉拉推正则化，推动原始视觉表示向无偏注意力的目标对象靠拢。

Result: 少样本学习、基础到新颖泛化和数据高效学习中表现优异。

Conclusion: DAPT显著提高PT视觉语言模型任务特定传递性和性能。

Abstract: Prompt tuning (PT), as an emerging resource-efficient fine-tuning paradigm,
has showcased remarkable effectiveness in improving the task-specific
transferability of vision-language models. This paper delves into a previously
overlooked information asymmetry issue in PT, where the visual modality mostly
conveys more context than the object-oriented textual modality.
Correspondingly, coarsely aligning these two modalities could result in the
biased attention, driving the model to merely focus on the context area. To
address this, we propose DAPT, an effective PT framework based on an intuitive
decouple-before-align concept. First, we propose to explicitly decouple the
visual modality into the foreground and background representation via
exploiting coarse-and-fine visual segmenting cues, and then both of these
decoupled patterns are aligned with the original foreground texts and the
hand-crafted background classes, thereby symmetrically strengthening the modal
alignment. To further enhance the visual concentration, we propose a visual
pull-push regularization tailored for the foreground-background patterns,
directing the original visual representation towards unbiased attention on the
region-of-interest object. We demonstrate the power of architecture-free DAPT
through few-shot learning, base-to-novel generalization, and data-efficient
learning, all of which yield superior performance across prevailing benchmarks.
Our code will be released at https://github.com/Ferenas/DAPT.

</details>


### [38] [Video Forgery Detection with Optical Flow Residuals and Spatial-Temporal Consistency](https://arxiv.org/abs/2508.00397)
*Xi Xue,Kunio Suzuki,Nabarun Goswami,Takuya Shintate*

Main category: cs.CV

TL;DR: 提出了一种结合RGB与光流残差的双分支检测框架，能有效检测高保真伪造视频中的运动异常，并在多种生成模型上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的方法往往难以捕捉细颗粒度的时间不一致性，尤其是在具有高视觉保真度和连贯运动的AI生成视频中。本文旨在通过结合空间-时间一致性特征，解决视频伪造检测中的新挑战。

Method: 本文采用双分支架构，其中一个分支分析RGB帧以检测外观级伪影，另一个分支处理光流残差以揭示由于时间合成不完善而引发的微妙运动异常。

Result: 在文本到视频和图像到视频任务的大量实验中，针对十种不同生成模型，所提出的方法展示了其鲁棒性和强大的泛化能力。

Conclusion: 本文提出的检测框架通过结合RGB外观特征与光流残差来有效检测广泛的伪造视频。通过整合这些互补特征，该方法在检测AI生成视频中的微妙运动异常方面表现出色。

Abstract: The rapid advancement of diffusion-based video generation models has led to
increasingly realistic synthetic content, presenting new challenges for video
forgery detection. Existing methods often struggle to capture fine-grained
temporal inconsistencies, particularly in AI-generated videos with high visual
fidelity and coherent motion. In this work, we propose a detection framework
that leverages spatial-temporal consistency by combining RGB appearance
features with optical flow residuals. The model adopts a dual-branch
architecture, where one branch analyzes RGB frames to detect appearance-level
artifacts, while the other processes flow residuals to reveal subtle motion
anomalies caused by imperfect temporal synthesis. By integrating these
complementary features, the proposed method effectively detects a wide range of
forged videos. Extensive experiments on text-to-video and image-to-video tasks
across ten diverse generative models demonstrate the robustness and strong
generalization ability of the proposed approach.

</details>


### [39] [iSafetyBench: A video-language benchmark for safety in industrial environment](https://arxiv.org/abs/2508.00399)
*Raiyaan Abdullah,Yogesh Singh Rawat,Shruti Vyas*

Main category: cs.CV

TL;DR: iSafetyBench是一个用于评估视听语言模型在工业环境中的表现的视频语言基准，揭示了现有模型在安全关键场景中的性能不足。


<details>
  <summary>Details</summary>
Motivation: 探索视听语言模型在高风险工业领域的能力，特别是在需要识别日常操作和安全关键异常的情况下。

Method: 引入了iSafetyBench基准，包括从工业环境中收集的1100个视频剪辑，并附有多选题进行模型性能评估。

Result: 评估了八个先进的视频语言模型，结果表明这些模型在识别危险活动和处理多标签场景时表现不佳。

Conclusion: 现有的视听语言模型在识别工业环境中的危险活动和多标签场景时存在显著性能差距，需开发更安全多模态的模型。

Abstract: Recent advances in vision-language models (VLMs) have enabled impressive
generalization across diverse video understanding tasks under zero-shot
settings. However, their capabilities in high-stakes industrial domains-where
recognizing both routine operations and safety-critical anomalies is
essential-remain largely underexplored. To address this gap, we introduce
iSafetyBench, a new video-language benchmark specifically designed to evaluate
model performance in industrial environments across both normal and hazardous
scenarios. iSafetyBench comprises 1,100 video clips sourced from real-world
industrial settings, annotated with open-vocabulary, multi-label action tags
spanning 98 routine and 67 hazardous action categories. Each clip is paired
with multiple-choice questions for both single-label and multi-label
evaluation, enabling fine-grained assessment of VLMs in both standard and
safety-critical contexts. We evaluate eight state-of-the-art video-language
models under zero-shot conditions. Despite their strong performance on existing
video benchmarks, these models struggle with iSafetyBench-particularly in
recognizing hazardous activities and in multi-label scenarios. Our results
reveal significant performance gaps, underscoring the need for more robust,
safety-aware multimodal models for industrial applications. iSafetyBench
provides a first-of-its-kind testbed to drive progress in this direction. The
dataset is available at: https://github.com/raiyaan-abdullah/iSafety-Bench.

</details>


### [40] [Sari Sandbox: A Virtual Retail Store Environment for Embodied AI Agents](https://arxiv.org/abs/2508.00400)
*Janika Deborah Gajo,Gerarld Paul Merales,Jerome Escarcha,Brenden Ashley Molina,Gian Nartea,Emmanuel G. Maminta,Juan Carlos Roldan,Rowel O. Atienza*

Main category: cs.CV

TL;DR: Sari Sandbox是高保真3D零售店模拟环境，助于具身代理训练和评估，与人类表现进行基准测试。


<details>
  <summary>Details</summary>
Motivation: 填补零售特定模拟环境用于具身代理训练的空白，以便在购物任务中对具身代理与人类表现进行基准测试。

Method: Sari Sandbox 提供 API 控制的高保真、逼真的 3D 零售商店模拟。支持虚拟现实（VR）和视觉语言模型（VLM）驱动的具身代理进行交互。

Result: 开发了 SariBench 数据集，包含不同任务难度的人工演示注释。代理能够在模拟环境中导航、检查和操纵零售物品，并提供与人类表现相比较的基线。

Conclusion: 研究提供了基准、性能分析以及提高现实性和可扩展性的建议。

Abstract: We present Sari Sandbox, a high-fidelity, photorealistic 3D retail store
simulation for benchmarking embodied agents against human performance in
shopping tasks. Addressing a gap in retail-specific sim environments for
embodied agent training, Sari Sandbox features over 250 interactive grocery
items across three store configurations, controlled via an API. It supports
both virtual reality (VR) for human interaction and a vision language model
(VLM)-powered embodied agent. We also introduce SariBench, a dataset of
annotated human demonstrations across varied task difficulties. Our sandbox
enables embodied agents to navigate, inspect, and manipulate retail items,
providing baselines against human performance. We conclude with benchmarks,
performance analysis, and recommendations for enhancing realism and
scalability. The source code can be accessed via
https://github.com/upeee/sari-sandbox-env.

</details>


### [41] [PMR: Physical Model-Driven Multi-Stage Restoration of Turbulent Dynamic Videos](https://arxiv.org/abs/2508.00406)
*Tao Wu,Jingyuan Ye,Ying Fu*

Main category: cs.CV

TL;DR: 引入动态效率指数和多阶段视频恢复框架，有效解决高湍流和复杂动态场景下的失真问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在强湍流和复杂动态条件下恢复边缘细节并消除混合失真。

Method: 引入动态效率指数（DEI）和物理模型驱动多阶段视频恢复框架（PMR），包括三个阶段：去倾斜、运动分割增强和去模糊。

Result: 新方法有效抑制运动拖影伪影，恢复边缘细节，尤其在高湍流和复杂动态的实际场景中表现出强大的泛化能力。

Conclusion: 提出的方法在长距离动态场景视频的恢复中表现出色，将公开代码和数据集。

Abstract: Geometric distortions and blurring caused by atmospheric turbulence degrade
the quality of long-range dynamic scene videos. Existing methods struggle with
restoring edge details and eliminating mixed distortions, especially under
conditions of strong turbulence and complex dynamics. To address these
challenges, we introduce a Dynamic Efficiency Index ($DEI$), which combines
turbulence intensity, optical flow, and proportions of dynamic regions to
accurately quantify video dynamic intensity under varying turbulence conditions
and provide a high-dynamic turbulence training dataset. Additionally, we
propose a Physical Model-Driven Multi-Stage Video Restoration ($PMR$) framework
that consists of three stages: \textbf{de-tilting} for geometric stabilization,
\textbf{motion segmentation enhancement} for dynamic region refinement, and
\textbf{de-blurring} for quality restoration. $PMR$ employs lightweight
backbones and stage-wise joint training to ensure both efficiency and high
restoration quality. Experimental results demonstrate that the proposed method
effectively suppresses motion trailing artifacts, restores edge details and
exhibits strong generalization capability, especially in real-world scenarios
characterized by high-turbulence and complex dynamics. We will make the code
and datasets openly available.

</details>


### [42] [Sortblock: Similarity-Aware Feature Reuse for Diffusion Model](https://arxiv.org/abs/2508.00412)
*Hanqi Chen,Xu Zhang,Xiaoliu Guan,Lielin Jiang,Guanzhong Wang,Zeyu Chen,Yi Liu*

Main category: cs.CV

TL;DR: 提出了一种用于加速生成模型推理的框架Sortblock，能够在保证质量的情况下实现显著加速。


<details>
  <summary>Details</summary>
Motivation: 现有的加速方法没有考虑不同时间步长之间的语义变化，为了解决这一问题，提出了Sortblock框架。

Method: Sortblock使用动态缓存块级特征的方法，并结合轻量级线性预测机制。

Result: 实验结果表明，Sortblock能够有效加速推断过程，且输出质量几乎没有下降。

Conclusion: Sortblock框架可以在保持生成质量的同时，实现超过2倍的推理加速。

Abstract: Diffusion Transformers (DiTs) have demonstrated remarkable generative
capabilities, particularly benefiting from Transformer architectures that
enhance visual and artistic fidelity. However, their inherently sequential
denoising process results in high inference latency, limiting their deployment
in real-time scenarios. Existing training-free acceleration approaches
typically reuse intermediate features at fixed timesteps or layers, overlooking
the evolving semantic focus across denoising stages and Transformer blocks.To
address this, we propose Sortblock, a training-free inference acceleration
framework that dynamically caches block-wise features based on their similarity
across adjacent timesteps. By ranking the evolution of residuals, Sortblock
adaptively determines a recomputation ratio, selectively skipping redundant
computations while preserving generation quality. Furthermore, we incorporate a
lightweight linear prediction mechanism to reduce accumulated errors in skipped
blocks.Extensive experiments across various tasks and DiT architectures
demonstrate that Sortblock achieves over 2$\times$ inference speedup with
minimal degradation in output quality, offering an effective and generalizable
solution for accelerating diffusion-based generative models.

</details>


### [43] [DC-AE 1.5: Accelerating Diffusion Model Convergence with Structured Latent Space](https://arxiv.org/abs/2508.00413)
*Junyu Chen,Dongyun Zou,Wenkun He,Junsong Chen,Enze Xie,Song Han,Han Cai*

Main category: cs.CV

TL;DR: DC-AE 1.5改进了高分辨率扩散模型的重建质量和生成速度，使用结构化潜在空间和增强扩散训练策略，在ImageNet上实现了更好的图像生成质量，同时速度提高了四倍。


<details>
  <summary>Details</summary>
Motivation: 提高高分辨率扩散模型的重建质量，同时解决因自编码器的潜在通道数量增加而导致的扩散模型收敛慢问题。

Method: 提出了两项关键创新：结构化潜在空间和增强扩散训练策略。结构化潜在空间通过训练在潜在空间中施加期望的通道结构，增强扩散训练在目标潜在通道上引入额外的扩散训练目标以加速收敛。

Result: DC-AE 1.5比DC-AE在收敛速度和扩散缩放效果上有更好的表现，在ImageNet 512x512上，DC-AE-1.5-f64c128实现了比DC-AE-f32c32更好的图像生成质量，并且速度提高了四倍。

Conclusion: 通过结构化潜在空间和增强扩散训练策略，成功解决了自编码器潜在通道增加导致的扩散模型收敛慢问题，实现了更快的收敛和更好的图像生成质量。

Abstract: We present DC-AE 1.5, a new family of deep compression autoencoders for
high-resolution diffusion models. Increasing the autoencoder's latent channel
number is a highly effective approach for improving its reconstruction quality.
However, it results in slow convergence for diffusion models, leading to poorer
generation quality despite better reconstruction quality. This issue limits the
quality upper bound of latent diffusion models and hinders the employment of
autoencoders with higher spatial compression ratios. We introduce two key
innovations to address this challenge: i) Structured Latent Space, a
training-based approach to impose a desired channel-wise structure on the
latent space with front latent channels capturing object structures and latter
latent channels capturing image details; ii) Augmented Diffusion Training, an
augmented diffusion training strategy with additional diffusion training
objectives on object latent channels to accelerate convergence. With these
techniques, DC-AE 1.5 delivers faster convergence and better diffusion scaling
results than DC-AE. On ImageNet 512x512, DC-AE-1.5-f64c128 delivers better
image generation quality than DC-AE-f32c32 while being 4x faster. Code:
https://github.com/dc-ai-projects/DC-Gen.

</details>


### [44] [IN2OUT: Fine-Tuning Video Inpainting Model for Video Outpainting Using Hierarchical Discriminator](https://arxiv.org/abs/2508.00418)
*Sangwoo Youn,Minji Lee,Nokap Tony Park,Yeonggyoo Jeon,Taeyoung Na*

Main category: cs.CV

TL;DR: 提出了一种视频outpainting方法，通过层次判别器和专门损失函数提升扩展场景质量，优于当前最佳方法。


<details>
  <summary>Details</summary>
Motivation: 现有的视频outpainting方法多聚焦于背景生成，而忽视了对象流学习和重构，导致效果模糊。为解决这一问题，需设计能够有效评估扩展区域视觉质量的判别器。

Method: 提出了一种新的层次判别器，并利用局部和全局特征制定了专门的outpainting损失函数，通过对抗性损失函数进行微调以增强生成效果。

Result: 我们的方法在定量和定性上均优于最新的方法。补充材料包括演示视频和代码在SigPort上可用。

Conclusion: 我们提出的使用层次判别器和专门的outpainting损失函数的方法，提高了生成器在视觉上吸引人的全局一致场景扩展能力，优于目前最先进的方法。

Abstract: Video outpainting presents a unique challenge of extending the borders while
maintaining consistency with the given content. In this paper, we suggest the
use of video inpainting models that excel in object flow learning and
reconstruction in outpainting rather than solely generating the background as
in existing methods. However, directly applying or fine-tuning inpainting
models to outpainting has shown to be ineffective, often leading to blurry
results. Our extensive experiments on discriminator designs reveal that a
critical component missing in the outpainting fine-tuning process is a
discriminator capable of effectively assessing the perceptual quality of the
extended areas. To tackle this limitation, we differentiate the objectives of
adversarial training into global and local goals and introduce a hierarchical
discriminator that meets both objectives. Additionally, we develop a
specialized outpainting loss function that leverages both local and global
features of the discriminator. Fine-tuning on this adversarial loss function
enhances the generator's ability to produce both visually appealing and
globally coherent outpainted scenes. Our proposed method outperforms
state-of-the-art methods both quantitatively and qualitatively. Supplementary
materials including the demo video and the code are available in SigPort.

</details>


### [45] [UIS-Mamba: Exploring Mamba for Underwater Instance Segmentation via Dynamic Tree Scan and Hidden State Weaken](https://arxiv.org/abs/2508.00421)
*Runmin Cong,Zongji Yu,Hao Fang,Haoyan Sun,Sam Kwong*

Main category: cs.CV

TL;DR: The paper presents UIS-Mamba, a novel model for underwater instance segmentation, overcoming challenges with innovative scanning and state mechanisms; achieving leading results on key datasets.


<details>
  <summary>Details</summary>
Motivation: Address challenges such as color distortion and complex backgrounds in underwater scenes using Mamba for underwater instance segmentation.

Method: Proposes UIS-Mamba, incorporating Dynamic Tree Scan (DTS) for dynamic offsets and scaling, and Hidden State Weaken (HSW) to suppress background interference.

Result: The proposed UIS-Mamba model demonstrated state-of-the-art results on UIIS and USIS10K datasets with efficiency in terms of parameters and computational load.

Conclusion: UIS-Mamba achieves state-of-the-art performance on major underwater instance segmentation benchmarks with low computational complexity.

Abstract: Underwater Instance Segmentation (UIS) tasks are crucial for underwater
complex scene detection. Mamba, as an emerging state space model with
inherently linear complexity and global receptive fields, is highly suitable
for processing image segmentation tasks with long sequence features. However,
due to the particularity of underwater scenes, there are many challenges in
applying Mamba to UIS. The existing fixed-patch scanning mechanism cannot
maintain the internal continuity of scanned instances in the presence of
severely underwater color distortion and blurred instance boundaries, and the
hidden state of the complex underwater background can also inhibit the
understanding of instance objects. In this work, we propose the first
Mamba-based underwater instance segmentation model UIS-Mamba, and design two
innovative modules, Dynamic Tree Scan (DTS) and Hidden State Weaken (HSW), to
migrate Mamba to the underwater task. DTS module maintains the continuity of
the internal features of the instance objects by allowing the patches to
dynamically offset and scale, thereby guiding the minimum spanning tree and
providing dynamic local receptive fields. HSW module suppresses the
interference of complex backgrounds and effectively focuses the information
flow of state propagation to the instances themselves through the Ncut-based
hidden state weakening mechanism. Experimental results show that UIS-Mamba
achieves state-of-the-art performance on both UIIS and USIS10K datasets, while
maintaining a low number of parameters and computational complexity. Code is
available at https://github.com/Maricalce/UIS-Mamba.

</details>


### [46] [Contact-Aware Amodal Completion for Human-Object Interaction via Multi-Regional Inpainting](https://arxiv.org/abs/2508.00427)
*Seunggeun Chi,Enna Sachdeva,Pin-Hao Huang,Kwonjoon Lee*

Main category: cs.CV

TL;DR: 我们的方法结合了物理先验和多区域补全技术，显著提高了在复杂HOI场景中推断被遮挡物体外观的准确性和真实性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在动态场景中生成合理的

Method: 我们的方法使用了物理先验知识和一种专门为HOI设计的多区域补全技术。通过结合人体拓扑和接触信息的物理约束，我们定义了两个不同的区域：主要区域，物体被遮挡的部分可能出现在这里；次要区域，物体被遮挡的可能性较小。这一多区域补全方法在扩散模型中通过定制的去噪策略来改善形状和视觉细节的补全准确性。

Result: 实验结果表明，我们的方法在HOI场景下显著优于现有方法，并且即使在没有真实接触标注的情况下，依然表现出很好的鲁棒性。这使得方法在3D重建和新视点/姿态合成等任务中具备广泛的适用性。

Conclusion: 我们的方法使得机器感知在动态环境中更接近人类的理解。

Abstract: Amodal completion, which is the process of inferring the full appearance of
objects despite partial occlusions, is crucial for understanding complex
human-object interactions (HOI) in computer vision and robotics. Existing
methods, such as those that use pre-trained diffusion models, often struggle to
generate plausible completions in dynamic scenarios because they have a limited
understanding of HOI. To solve this problem, we've developed a new approach
that uses physical prior knowledge along with a specialized multi-regional
inpainting technique designed for HOI. By incorporating physical constraints
from human topology and contact information, we define two distinct regions:
the primary region, where occluded object parts are most likely to be, and the
secondary region, where occlusions are less probable. Our multi-regional
inpainting method uses customized denoising strategies across these regions
within a diffusion model. This improves the accuracy and realism of the
generated completions in both their shape and visual detail. Our experimental
results show that our approach significantly outperforms existing methods in
HOI scenarios, moving machine perception closer to a more human-like
understanding of dynamic environments. We also show that our pipeline is robust
even without ground-truth contact annotations, which broadens its applicability
to tasks like 3D reconstruction and novel view/pose synthesis.

</details>


### [47] [Reducing the gap between general purpose data and aerial images in concentrated solar power plants](https://arxiv.org/abs/2508.00440)
*M. A. Pérez-Cutiño,J. Valverde,J. Capitán,J. M. Díaz-Báñez*

Main category: cs.CV

TL;DR: AerialCSP是一个为CSP工厂设计的高质量合成数据集，可以减少训练模型所需的大量人工数据标注，显著提高瑕疵检测能力。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型在CSP太阳能场景中的表现不佳，原因是这些场景具有高度反射性表面和域特定的元素，而这些在传统计算机视觉基准中是很少见的，导致需要大量人工标注数据进行重新训练，这非常耗时且成本高。

Method: 提出了AerialCSP虚拟数据集，通过生成模拟真实世界条件的合成数据，进行模型的预训练以减少人工标注需求。对多个模型进行基准测试以建立相关任务的基线。

Result: 通过预训练，能够显著提高模型在真实环境中检测瑕疵的能力，特别是在识别罕见和小缺陷时效果显著，减少了大量人工标注工作的需求。提供了一个高质量、可公开获取的合成数据集。

Conclusion: 使用AerialCSP进行预训练可以显著提高实际缺陷检测的效果，尤其是在检测罕见和小型缺陷方面，同时减少了对大量人工标注的需求。

Abstract: In the context of Concentrated Solar Power (CSP) plants, aerial images
captured by drones present a unique set of challenges. Unlike urban or natural
landscapes commonly found in existing datasets, solar fields contain highly
reflective surfaces, and domain-specific elements that are uncommon in
traditional computer vision benchmarks. As a result, machine learning models
trained on generic datasets struggle to generalize to this setting without
extensive retraining and large volumes of annotated data. However, collecting
and labeling such data is costly and time-consuming, making it impractical for
rapid deployment in industrial applications.
  To address this issue, we propose a novel approach: the creation of
AerialCSP, a virtual dataset that simulates aerial imagery of CSP plants. By
generating synthetic data that closely mimic real-world conditions, our
objective is to facilitate pretraining of models before deployment,
significantly reducing the need for extensive manual labeling. Our main
contributions are threefold: (1) we introduce AerialCSP, a high-quality
synthetic dataset for aerial inspection of CSP plants, providing annotated data
for object detection and image segmentation; (2) we benchmark multiple models
on AerialCSP, establishing a baseline for CSP-related vision tasks; and (3) we
demonstrate that pretraining on AerialCSP significantly improves real-world
fault detection, particularly for rare and small defects, reducing the need for
extensive manual labeling. AerialCSP is made publicly available at
https://mpcutino.github.io/aerialcsp/.

</details>


### [48] [TopoTTA: Topology-Enhanced Test-Time Adaptation for Tubular Structure Segmentation](https://arxiv.org/abs/2508.00442)
*Jiale Zhou,Wenhan Wang,Shikun Li,Xiaolei Qu,Xin Guo,Yizhong Liu,Wenzhong Tang,Xun Lin,Yefeng Zheng*

Main category: cs.CV

TL;DR: 提出了针对管状结构分割的测试时适应框架TopoTTA，通过处理拓扑结构差异来适应不同域，并在实验中取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 由于域移导致的性能下降仍是管状结构分割中的主要挑战，特别是拓扑结构的改变会损害分割完整性，而区分前景与背景的局部特征变化（如纹理和对比度）可能进一步破坏拓扑连续性。

Method: 提出了一种名为拓扑增强测试时适应（TopoTTA）的框架，包括两个阶段：第一阶段通过拓扑元差卷积（TopoMDCs）来适应跨域的拓扑差异；第二阶段通过拓扑硬样本生成策略（TopoHG）和预测校准策略来改善拓扑连续性。

Result: 在四种场景和十个数据集的广泛实验中，TopoTTA在处理拓扑分布偏移方面展现了其效果，平均在clDice指标上提高了31.81%。

Conclusion: TopoTTA significantly improves the performance of tubular structure segmentation across different domains, effectively handling topological distribution shifts.

Abstract: Tubular structure segmentation (TSS) is important for various applications,
such as hemodynamic analysis and route navigation. Despite significant progress
in TSS, domain shifts remain a major challenge, leading to performance
degradation in unseen target domains. Unlike other segmentation tasks, TSS is
more sensitive to domain shifts, as changes in topological structures can
compromise segmentation integrity, and variations in local features
distinguishing foreground from background (e.g., texture and contrast) may
further disrupt topological continuity. To address these challenges, we propose
Topology-enhanced Test-Time Adaptation (TopoTTA), the first test-time
adaptation framework designed specifically for TSS. TopoTTA consists of two
stages: Stage 1 adapts models to cross-domain topological discrepancies using
the proposed Topological Meta Difference Convolutions (TopoMDCs), which enhance
topological representation without altering pre-trained parameters; Stage 2
improves topological continuity by a novel Topology Hard sample Generation
(TopoHG) strategy and prediction alignment on hard samples with pseudo-labels
in the generated pseudo-break regions. Extensive experiments across four
scenarios and ten datasets demonstrate TopoTTA's effectiveness in handling
topological distribution shifts, achieving an average improvement of 31.81% in
clDice. TopoTTA also serves as a plug-and-play TTA solution for CNN-based TSS
models.

</details>


### [49] [SDMatte: Grafting Diffusion Models for Interactive Matting](https://arxiv.org/abs/2508.00443)
*Longfei Huang,Yu Liang,Hao Zhang,Jinwei Chen,Wei Dong,Lunde Chen,Wanyu Liu,Bo Li,Pengtao Jiang*

Main category: cs.CV

TL;DR: SDMatte是一种基于扩散模型的交互式抠图方法，利用视觉提示交互、坐标嵌入及掩模自注意力机制，从而在多个数据集上取得优异表现。代码已开放访问。


<details>
  <summary>Details</summary>
Motivation: 现有的交互式抠图方法在捕捉物体主要区域上表现良好，但在提取边缘细节方面不足。扩散模型在复杂数据分布建模和合成真实纹理细节方面表现出色，且具有鲁棒的文本驱动交互能力，这使得它们成为交互式抠图的有吸引力的解决方案。

Method: 1. 利用扩散模型的强大先验，将文本驱动的交互能力转变为视觉提示驱动的交互能力，实现交互式抠图。2. 在U-Net中整合视觉提示的坐标嵌入和目标对象的不透明度嵌入，增强SDMatte对空间位置信息和不透明度信息的敏感性。3. 提出了一种掩模自注意力机制，使模型能够专注于视觉提示指定的区域，从而提高性能。

Result: SDMatte方法在多个数据集上通过了广泛的实验测试，表现出优越的性能。

Conclusion: SDMatte方法在多个数据集上的实验中表现出色，验证了其在交互式抠图中的有效性。

Abstract: Recent interactive matting methods have shown satisfactory performance in
capturing the primary regions of objects, but they fall short in extracting
fine-grained details in edge regions. Diffusion models trained on billions of
image-text pairs, demonstrate exceptional capability in modeling highly complex
data distributions and synthesizing realistic texture details, while exhibiting
robust text-driven interaction capabilities, making them an attractive solution
for interactive matting. To this end, we propose SDMatte, a diffusion-driven
interactive matting model, with three key contributions. First, we exploit the
powerful priors of diffusion models and transform the text-driven interaction
capability into visual prompt-driven interaction capability to enable
interactive matting. Second, we integrate coordinate embeddings of visual
prompts and opacity embeddings of target objects into U-Net, enhancing
SDMatte's sensitivity to spatial position information and opacity information.
Third, we propose a masked self-attention mechanism that enables the model to
focus on areas specified by visual prompts, leading to better performance.
Extensive experiments on multiple datasets demonstrate the superior performance
of our method, validating its effectiveness in interactive matting. Our code
and model are available at https://github.com/vivoCameraResearch/SDMatte.

</details>


### [50] [AutoDebias: Automated Framework for Debiasing Text-to-Image Models](https://arxiv.org/abs/2508.00445)
*Hongyi Cai,Mohammad Mahdinur Rahman,Mingkang Dong,Jie Li,Muxin Pu,Zhili Fang,Yinan Peng,Hanjun Luo,Yang Liu*

Main category: cs.CV

TL;DR: AutoDebias automatically identifies and reduces biases in Text-to-Image models, achieving high accuracy and maintaining original image quality.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address unintended social biases in Text-to-Image models that existing debiasing methods struggle with, especially in subtle or overlapping cases.

Method: AutoDebias constructs fairness guides using vision-language models to identify biased patterns and creates alternative prompts for balanced representations. It employs CLIP-guided training to achieve fairer outputs.

Result: AutoDebias detected harmful patterns with 91.6% accuracy and successfully reduced the occurrence of biased outputs from 90% to negligible levels.

Conclusion: AutoDebias is an effective solution for removing biases in Text-to-Image models, maintaining image quality while significantly reducing biased outputs.

Abstract: Text-to-Image (T2I) models generate high-quality images from text prompts but
often exhibit unintended social biases, such as gender or racial stereotypes,
even when these attributes are not mentioned. Existing debiasing methods work
well for simple or well-known cases but struggle with subtle or overlapping
biases. We propose AutoDebias, a framework that automatically identifies and
mitigates harmful biases in T2I models without prior knowledge of specific bias
types. Specifically, AutoDebias leverages vision-language models to detect
biased visual patterns and constructs fairness guides by generating inclusive
alternative prompts that reflect balanced representations. These guides drive a
CLIP-guided training process that promotes fairer outputs while preserving the
original model's image quality and diversity. Unlike existing methods,
AutoDebias effectively addresses both subtle stereotypes and multiple
interacting biases. We evaluate the framework on a benchmark covering over 25
bias scenarios, including challenging cases where multiple biases occur
simultaneously. AutoDebias detects harmful patterns with 91.6% accuracy and
reduces biased outputs from 90% to negligible levels, while preserving the
visual fidelity of the original model.

</details>


### [51] [CLIPTime: Time-Aware Multimodal Representation Learning from Images and Text](https://arxiv.org/abs/2508.00447)
*Anju Rani,Daniel Ortiz-Arroyo,Petar Durdevic*

Main category: cs.CV

TL;DR: 本文提出了CLIPTime模型，通过视觉和文本输入预测真菌生长的阶段和时间，能够有效建模生物进展，且无需明确的时间输入。


<details>
  <summary>Details</summary>
Motivation: 目前的视觉-语言模型在捕捉时间进程方面效果有限，因此提出了一种新框架来改善这种局限性。

Method: 基于CLIP架构，CLIPTime模型学习联合的视觉-文本嵌入，能够在无需明确时间输入的情况下进行时间感知推理，通过联合执行分类和回归任务来预测发展阶段和对应的时间戳。

Result: 实验结果表明，CLIPTime有效地建模了生物进程，并能产生解释性强的时间基础输出。

Conclusion: CLIPTime模型有效地模拟了生物进程，提供了可解释且具有时间基础的输出，突显了视觉-语言模型在实际生物监测应用中的潜力。

Abstract: Understanding the temporal dynamics of biological growth is critical across
diverse fields such as microbiology, agriculture, and biodegradation research.
Although vision-language models like Contrastive Language Image Pretraining
(CLIP) have shown strong capabilities in joint visual-textual reasoning, their
effectiveness in capturing temporal progression remains limited. To address
this, we propose CLIPTime, a multimodal, multitask framework designed to
predict both the developmental stage and the corresponding timestamp of fungal
growth from image and text inputs. Built upon the CLIP architecture, our model
learns joint visual-textual embeddings and enables time-aware inference without
requiring explicit temporal input during testing. To facilitate training and
evaluation, we introduce a synthetic fungal growth dataset annotated with
aligned timestamps and categorical stage labels. CLIPTime jointly performs
classification and regression, predicting discrete growth stages alongside
continuous timestamps. We also propose custom evaluation metrics, including
temporal accuracy and regression error, to assess the precision of time-aware
predictions. Experimental results demonstrate that CLIPTime effectively models
biological progression and produces interpretable, temporally grounded outputs,
highlighting the potential of vision-language models in real-world biological
monitoring applications.

</details>


### [52] [PIF-Net: Ill-Posed Prior Guided Multispectral and Hyperspectral Image Fusion via Invertible Mamba and Fusion-Aware LoRA](https://arxiv.org/abs/2508.00453)
*Baisong Li,Xingwang Wang,Haixiao Xu*

Main category: cs.CV

TL;DR: 提出了PIF-Net框架和FALA模块，实现了多光谱和高光谱图像的有效融合，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多光谱和高光谱图像融合（MHIF）的目标是生成同时具有丰富光谱信息和精细空间细节的高质量图像。然而，由于光谱和空间信息之间的固有权衡及观测数据有限，这一任务在本质上是病态的。先前的研究未能有效解决由数据未对齐引起的病态问题。

Method: 提出了一种名为PIF-Net的融合框架，该框架显性地结合了病态先验以有效地融合多光谱图像和高光谱图像。采用基于可逆Mamba架构的方法以在特征变换和融合过程中保持信息一致性，保证梯度流的稳定性和过程的可逆性。同时引入了一种新颖的融合模块FALA模块，动态校准光谱和空间特征，同时保持模型的轻量级。

Result: 在多个基准数据集上的广泛实验表明，PIF-Net在保持模型效率的情况下，实现了显著优于当前最先进方法的图像恢复性能。

Conclusion: PIF-Net框架在多光谱和高光谱图像融合任务中表现优异，有效解决了数据未对齐问题，实现了更好的图像恢复性能。

Abstract: The goal of multispectral and hyperspectral image fusion (MHIF) is to
generate high-quality images that simultaneously possess rich spectral
information and fine spatial details. However, due to the inherent trade-off
between spectral and spatial information and the limited availability of
observations, this task is fundamentally ill-posed. Previous studies have not
effectively addressed the ill-posed nature caused by data misalignment. To
tackle this challenge, we propose a fusion framework named PIF-Net, which
explicitly incorporates ill-posed priors to effectively fuse multispectral
images and hyperspectral images. To balance global spectral modeling with
computational efficiency, we design a method based on an invertible Mamba
architecture that maintains information consistency during feature
transformation and fusion, ensuring stable gradient flow and process
reversibility. Furthermore, we introduce a novel fusion module called the
Fusion-Aware Low-Rank Adaptation module, which dynamically calibrates spectral
and spatial features while keeping the model lightweight. Extensive experiments
on multiple benchmark datasets demonstrate that PIF-Net achieves significantly
better image restoration performance than current state-of-the-art methods
while maintaining model efficiency.

</details>


### [53] [Semantic and Temporal Integration in Latent Diffusion Space for High-Fidelity Video Super-Resolution](https://arxiv.org/abs/2508.00471)
*Yiwen Wang,Xinning Chai,Yuhong Zhang,Zhengxue Cheng,Jun Zhao,Rong Xie,Li Song*

Main category: cs.CV

TL;DR: SeTe-VSR通过语义和时空指导，改善细节恢复和感知质量，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前视频超分辨率模型在低分辨率视频增强上虽然取得了一些进展，但在生成过程中仍难以有效控制，使得在保持时间一致性的同时，实现与低分辨率输入的高保真对齐仍然是一个重大挑战。

Method: 提出了一种名为Semantic and Temporal Guided Video Super-Resolution (SeTe-VSR)的新方法，采用语义和时空指导，在潜在扩散空间中，通过结合高层语义信息并整合空间和时间信息，实现细节恢复和时间一致性的平衡。

Result: 大量实验证明，SeTe-VSR在细节恢复和感知质量方面优于现有方法。

Conclusion: SeTe-VSR通过结合语义和时间空间指导，在恢复复杂视频细节和提升感知质量方面优于现有方法。

Abstract: Recent advancements in video super-resolution (VSR) models have demonstrated
impressive results in enhancing low-resolution videos. However, due to
limitations in adequately controlling the generation process, achieving high
fidelity alignment with the low-resolution input while maintaining temporal
consistency across frames remains a significant challenge. In this work, we
propose Semantic and Temporal Guided Video Super-Resolution (SeTe-VSR), a novel
approach that incorporates both semantic and temporal-spatio guidance in the
latent diffusion space to address these challenges. By incorporating high-level
semantic information and integrating spatial and temporal information, our
approach achieves a seamless balance between recovering intricate details and
ensuring temporal coherence. Our method not only preserves high-reality visual
content but also significantly enhances fidelity. Extensive experiments
demonstrate that SeTe-VSR outperforms existing methods in terms of detail
recovery and perceptual quality, highlighting its effectiveness for complex
video super-resolution tasks.

</details>


### [54] [HyPCV-Former: Hyperbolic Spatio-Temporal Transformer for 3D Point Cloud Video Anomaly Detection](https://arxiv.org/abs/2508.00473)
*Jiaping Cao,Kangkang Zhou,Juan Du*

Main category: cs.CV

TL;DR: HyPCV-Former, a new hyperbolic transformer, significantly improves video anomaly detection performance by effectively capturing hierarchical and spatio-temporal features in point cloud videos.


<details>
  <summary>Details</summary>
Motivation: Previous methods using Euclidean representations could not effectively capture hierarchical event structures and spatio-temporal continuity in anomaly detection tasks.

Method: The paper introduces HyPCV-Former, a hyperbolic spatio-temporal transformer. It uses a point cloud extractor to get spatial features, embeds them into Lorentzian hyperbolic space, and utilizes a hyperbolic multi-head self-attention mechanism to capture temporal dependencies.

Result: HyPCV-Former shows a 7% improvement on the TIMo dataset and a 5.6% gain on the DAD dataset, establishing new performance benchmarks.

Conclusion: HyPCV-Former achieves state-of-the-art performance in video anomaly detection, outperforming benchmarks with significant improvements on the TIMo and DAD datasets.

Abstract: Video anomaly detection is a fundamental task in video surveillance, with
broad applications in public safety and intelligent monitoring systems.
Although previous methods leverage Euclidean representations in RGB or depth
domains, such embeddings are inherently limited in capturing hierarchical event
structures and spatio-temporal continuity. To address these limitations, we
propose HyPCV-Former, a novel hyperbolic spatio-temporal transformer for
anomaly detection in 3D point cloud videos. Our approach first extracts
per-frame spatial features from point cloud sequences via point cloud
extractor, and then embeds them into Lorentzian hyperbolic space, which better
captures the latent hierarchical structure of events. To model temporal
dynamics, we introduce a hyperbolic multi-head self-attention (HMHA) mechanism
that leverages Lorentzian inner products and curvature-aware softmax to learn
temporal dependencies under non-Euclidean geometry. Our method performs all
feature transformations and anomaly scoring directly within full Lorentzian
space rather than via tangent space approximation. Extensive experiments
demonstrate that HyPCV-Former achieves state-of-the-art performance across
multiple anomaly categories, with a 7\% improvement on the TIMo dataset and a
5.6\% gain on the DAD dataset compared to benchmarks. The code will be released
upon paper acceptance.

</details>


### [55] [LAMIC: Layout-Aware Multi-Image Composition via Scalability of Multimodal Diffusion Transformer](https://arxiv.org/abs/2508.00477)
*Yuzhuo Chen,Zehua Ma,Jianhua Wang,Kai Kang,Shunyu Yao,Weiming Zhang*

Main category: cs.CV

TL;DR: LAMIC是一种无需训练的布局感知多图像合成框架，通过引入新注意力机制在多参考场景中实现了先进性能。


<details>
  <summary>Details</summary>
Motivation: 在可控图像合成中，从多个参考生成具有空间布局意识的连贯、一致的图像仍是一个开放挑战。

Method: LAMIC建立在MMDiT模型的基础上，引入了组隔离注意力（GIA）和区域调制注意力（RMA）进行图像生成，并通过三个指标进行评估。

Result: LAMIC在ID-S、BG-S、IN-R和AVG分数方面超越现有多参考基线，并在复杂合成任务中达到最佳DPG得分。

Conclusion: LAMIC在身份保持、背景保护、布局控制和提示遵循方面表现优异，并且无需训练或微调，展示了强大的零样本泛化能力。

Abstract: In controllable image synthesis, generating coherent and consistent images
from multiple references with spatial layout awareness remains an open
challenge. We present LAMIC, a Layout-Aware Multi-Image Composition framework
that, for the first time, extends single-reference diffusion models to
multi-reference scenarios in a training-free manner. Built upon the MMDiT
model, LAMIC introduces two plug-and-play attention mechanisms: 1) Group
Isolation Attention (GIA) to enhance entity disentanglement; and 2)
Region-Modulated Attention (RMA) to enable layout-aware generation. To
comprehensively evaluate model capabilities, we further introduce three
metrics: 1) Inclusion Ratio (IN-R) and Fill Ratio (FI-R) for assessing layout
control; and 2) Background Similarity (BG-S) for measuring background
consistency. Extensive experiments show that LAMIC achieves state-of-the-art
performance across most major metrics: it consistently outperforms existing
multi-reference baselines in ID-S, BG-S, IN-R and AVG scores across all
settings, and achieves the best DPG in complex composition tasks. These results
demonstrate LAMIC's superior abilities in identity keeping, background
preservation, layout control, and prompt-following, all achieved without any
training or fine-tuning, showcasing strong zero-shot generalization ability. By
inheriting the strengths of advanced single-reference models and enabling
seamless extension to multi-image scenarios, LAMIC establishes a new
training-free paradigm for controllable multi-image composition. As foundation
models continue to evolve, LAMIC's performance is expected to scale
accordingly. Our implementation is available at:
https://github.com/Suchenl/LAMIC.

</details>


### [56] [SAMSA 2.0: Prompting Segment Anything with Spectral Angles for Hyperspectral Interactive Medical Image Segmentation](https://arxiv.org/abs/2508.00493)
*Alfie Roddan,Tobias Czempiel,Chi Xu,Daniel S. Elson,Stamatia Giannarou*

Main category: cs.CV

TL;DR: SAMSA 2.0是一种用于超光谱医学成像的分割框架，通过光谱角度提示融合光谱和空间信息，在无需重新训练的情况下，提升了分割精度，尤其在少样本和零样本条件下表现优异。


<details>
  <summary>Details</summary>
Motivation: 提高超光谱医学成像的分割精度及鲁棒性，尤其是在低数据和噪声场景中展示出好的泛化能力。

Method: 引入光谱角度提示，通过光谱相似性和空间提示来引导Segment Anything Model (SAM)进行分割。

Result: SAMSA 2.0在少样本及零样本的任务中表现出色，泛化能力强，能够应对临床成像中常见的低数据和噪声环境。

Conclusion: SAMSA 2.0在无需重新训练的情况下，能显著提高分割精度，在Dice得分上相比于RGB模型提高了3.8%，相较于之前的光谱融合方法提高了3.1%。

Abstract: We present SAMSA 2.0, an interactive segmentation framework for hyperspectral
medical imaging that introduces spectral angle prompting to guide the Segment
Anything Model (SAM) using spectral similarity alongside spatial cues. This
early fusion of spectral information enables more accurate and robust
segmentation across diverse spectral datasets. Without retraining, SAMSA 2.0
achieves up to +3.8% higher Dice scores compared to RGB-only models and up to
+3.1% over prior spectral fusion methods. Our approach enhances few-shot and
zero-shot performance, demonstrating strong generalization in challenging
low-data and noisy scenarios common in clinical imaging.

</details>


### [57] [LesiOnTime -- Joint Temporal and Clinical Modeling for Small Breast Lesion Segmentation in Longitudinal DCE-MRI](https://arxiv.org/abs/2508.00496)
*Mohammed Kamran,Maria Bernathova,Raoul Varga,Christian Singer,Zsuzsanna Bago-Horvath,Thomas Helbich,Georg Langs,Philipp Seeböck*

Main category: cs.CV

TL;DR: LesiOnTime通过整合纵向图像和BI-RADS评分，提升了小病灶的分割效果。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习方法主要针对大病灶，忽略了放射科医生常用的纵向和临床信息，因此需要开发一个可以整合纵向成像和BI-RADS评分以提高小病灶检测的模型。

Method: 提出一种名为LesiOnTime的3D分割方法，结合时间序列图像和BI-RADS评分。主要组件包括时间优先注意（TPA）模块和BI-RADS一致性正则化（BCR）损失。

Result: 在经过挑选的高风险患者的DCE-MRI数据集上测试，LesiOnTime方法在Dice系数中比最先进的单时间点和纵向基线提高了5%。

Conclusion: LesiOnTime在高风险患者的DCE-MRI数据集上 outperform其他最先进的单时间点和纵向基线方法5%。

Abstract: Accurate segmentation of small lesions in Breast Dynamic Contrast-Enhanced
MRI (DCE-MRI) is critical for early cancer detection, especially in high-risk
patients. While recent deep learning methods have advanced lesion segmentation,
they primarily target large lesions and neglect valuable longitudinal and
clinical information routinely used by radiologists. In real-world screening,
detecting subtle or emerging lesions requires radiologists to compare across
timepoints and consider previous radiology assessments, such as the BI-RADS
score. We propose LesiOnTime, a novel 3D segmentation approach that mimics
clinical diagnostic workflows by jointly leveraging longitudinal imaging and
BIRADS scores. The key components are: (1) a Temporal Prior Attention (TPA)
block that dynamically integrates information from previous and current scans;
and (2) a BI-RADS Consistency Regularization (BCR) loss that enforces latent
space alignment for scans with similar radiological assessments, thus embedding
domain knowledge into the training process. Evaluated on a curated in-house
longitudinal dataset of high-risk patients with DCE-MRI, our approach
outperforms state-of-the-art single-timepoint and longitudinal baselines by 5%
in terms of Dice. Ablation studies demonstrate that both TPA and BCR contribute
complementary performance gains. These results highlight the importance of
incorporating temporal and clinical context for reliable early lesion
segmentation in real-world breast cancer screening. Our code is publicly
available at https://github.com/cirmuw/LesiOnTime

</details>


### [58] [Leveraging Convolutional and Graph Networks for an Unsupervised Remote Sensing Labelling Tool](https://arxiv.org/abs/2508.00506)
*Tulsi Patel,Mark W. Jones,Thomas Redfern*

Main category: cs.CV

TL;DR: 这项工作提出了一种无监督方法，通过卷积和图神经网络分割卫星图像以改善标签质量，减少异常值并提高标记精度。


<details>
  <summary>Details</summary>
Motivation: 机器学习用于遥感成像需要最新和精确的标签来进行模型训练和测试，然而标记遥感图像耗时且成本高昂，需要专家分析。之前的标记工具依赖于预标记数据进行训练以标记新的未知数据。

Method: 提出了一种无监督的流程，利用卷积和图神经网络进行分割，以便在Sentinel-2卫星图像中找到和标记具有类似上下文和内容的地理区域。

Result: 所提出的方法消除了之前方法的局限性，通过在编码空间中形成图像级的旋转不变语义关系，减少了标记工具中的异常值，使用户能够在更细粒度的水平进行标记。

Conclusion: 通过使用未经监督的卷积和图神经网络进行分割，提高了特征空间的鲁棒性，实现了更精细的标记和更少的异常值。

Abstract: Machine learning for remote sensing imaging relies on up-to-date and accurate
labels for model training and testing. Labelling remote sensing imagery is time
and cost intensive, requiring expert analysis. Previous labelling tools rely on
pre-labelled data for training in order to label new unseen data. In this work,
we define an unsupervised pipeline for finding and labelling geographical areas
of similar context and content within Sentinel-2 satellite imagery. Our
approach removes limitations of previous methods by utilising segmentation with
convolutional and graph neural networks to encode a more robust feature space
for image comparison. Unlike previous approaches we segment the image into
homogeneous regions of pixels that are grouped based on colour and spatial
similarity. Graph neural networks are used to aggregate information about the
surrounding segments enabling the feature representation to encode the local
neighbourhood whilst preserving its own local information. This reduces
outliers in the labelling tool, allows users to label at a granular level, and
allows a rotationally invariant semantic relationship at the image level to be
formed within the encoding space.

</details>


### [59] [Fine-grained Spatiotemporal Grounding on Egocentric Videos](https://arxiv.org/abs/2508.00518)
*Shuo Liang,Yiwu Zhong,Zi-Yuan Hu,Yeyao Tao,Liwei Wang*

Main category: cs.CV

TL;DR: 介绍了一种名为EgoMask的自中心视频的像素级基准，并创建了EgoMask-Train用于提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 探索以文本查询定位视频中的目标实体。尽管在外展视频中取得了显著进展，但在增强现实和机器人技术等应用中，随着自中心视频的重要性日益增长，其相关研究仍相对较少。

Method: 引入EgoMask，一种针对自中心视频精细的时空定位的像素级基准。通过提出的自动注释流程，对表达和对象进行短期、中期和长期视频的注释。此外，创建了EgoMask-Train，这是一种大规模训练数据集以促进模型开发。

Result: 实验表明，最先进的时空定位模型在EgoMask上表现不佳，但在EgoMask-Train上进行微调后显著提高，同时保留了在外展数据集上的性能。

Conclusion: 本文提供了关键资源和见解，推动自中心视频理解的发展。

Abstract: Spatiotemporal video grounding aims to localize target entities in videos
based on textual queries. While existing research has made significant progress
in exocentric videos, the egocentric setting remains relatively underexplored,
despite its growing importance in applications such as augmented reality and
robotics. In this work, we conduct a systematic analysis of the discrepancies
between egocentric and exocentric videos, revealing key challenges such as
shorter object durations, sparser trajectories, smaller object sizes, and
larger positional shifts. To address these challenges, we introduce EgoMask,
the first pixel-level benchmark for fine-grained spatiotemporal grounding in
egocentric videos. It is constructed by our proposed automatic annotation
pipeline, which annotates referring expressions and object masks across short-,
medium-, and long-term videos. Additionally, we create EgoMask-Train, a
large-scale training dataset to facilitate model development. Experiments
demonstrate that the state-of-the-art spatiotemporal grounding models perform
poorly on our benchmark EgoMask, but fine-tuning on EgoMask-Train yields
significant improvements, while preserving performance on exocentric datasets.
Our work thus provides essential resources and insights for advancing
egocentric video understanding. Our code is available at
https://github.com/LaVi-Lab/EgoMask .

</details>


### [60] [EPANet: Efficient Path Aggregation Network for Underwater Fish Detection](https://arxiv.org/abs/2508.00528)
*Jinsong Yang,Zeyuan Hu,Yichen Li*

Main category: cs.CV

TL;DR: 研究提出了一种新的网络EPANet，通过路径聚合和短路径瓶颈技术提高了水下鱼类的检测性能，结果显示其检测精度和速度优于现有方法且参数复杂度更低。


<details>
  <summary>Details</summary>
Motivation: 现有方法在提高小物体的检测时通常采用局部特征增强或复杂注意机制，这导致模型复杂性增加和效率降低。本研究旨在解决这些问题，提出一种准确且轻量级的水下鱼类检测方法。

Method: 提出了一种高效路径聚合网络（EPANet）。EPANet包括两个关键组件：高效路径聚合特征金字塔网络（EPA-FPN）和多尺度多样分割短路径瓶颈（MS-DDSP瓶颈）。EPA-FPN引入了跨不同尺度的长范围跳跃连接，以改善语义-空间互补性，同时采用跨层融合路径来提升特征集成效率。MS-DDSP瓶颈通过引入更细粒度的特征划分和多样的卷积操作来扩展传统瓶颈结构，从而增加局部特征的多样性和表示能力。

Result: EPANet在基准UFD数据集上的广泛实验显示，该方法在检测准确性和推理速度上优于最先进的方法，同时保持了可比甚至更低的参数复杂度。

Conclusion: EPANet超越了现有的方法，在检测准确性和推理速度方面表现出色，同时保持了相当或更低的参数复杂度。

Abstract: Underwater fish detection (UFD) remains a challenging task in computer vision
due to low object resolution, significant background interference, and high
visual similarity between targets and surroundings. Existing approaches
primarily focus on local feature enhancement or incorporate complex attention
mechanisms to highlight small objects, often at the cost of increased model
complexity and reduced efficiency. To address these limitations, we propose an
efficient path aggregation network (EPANet), which leverages complementary
feature integration to achieve accurate and lightweight UFD. EPANet consists of
two key components: an efficient path aggregation feature pyramid network
(EPA-FPN) and a multi-scale diverse-division short path bottleneck (MS-DDSP
bottleneck). The EPA-FPN introduces long-range skip connections across
disparate scales to improve semantic-spatial complementarity, while cross-layer
fusion paths are adopted to enhance feature integration efficiency. The MS-DDSP
bottleneck extends the conventional bottleneck structure by introducing
finer-grained feature division and diverse convolutional operations, thereby
increasing local feature diversity and representation capacity. Extensive
experiments on benchmark UFD datasets demonstrate that EPANet outperforms
state-of-the-art methods in terms of detection accuracy and inference speed,
while maintaining comparable or even lower parameter complexity.

</details>


### [61] [Video Color Grading via Look-Up Table Generation](https://arxiv.org/abs/2508.00548)
*Seunghyun Shin,Dongmin Shin,Jisu Shin,Hae-Gon Jeon,Joon-Young Lee*

Main category: cs.CV

TL;DR: 本研究提出了一种基于LUT的参考视频调色框架，通过扩散模型进行颜色对齐，实现快速且不损失结构细节的视频调色，并支持用户偏好调整。


<details>
  <summary>Details</summary>
Motivation: 视频调色由于过程复杂且需要专业技能，主要由专业调色师完成，因此需要简化过程以供普通用户使用。

Method: 通过扩散模型显式生成查找表（LUT）以实现参考场景和输入视频之间的颜色属性对齐。

Result: 实验结果和广泛的用户研究表明，我们的方法在视频调色方面具有高效性。

Conclusion: 我们的参考视频调色框架在保持视频结构细节的同时，实现了快速推理和色彩对齐。

Abstract: Different from color correction and transfer, color grading involves
adjusting colors for artistic or storytelling purposes in a video, which is
used to establish a specific look or mood. However, due to the complexity of
the process and the need for specialized editing skills, video color grading
remains primarily the domain of professional colorists. In this paper, we
present a reference-based video color grading framework. Our key idea is
explicitly generating a look-up table (LUT) for color attribute alignment
between reference scenes and input video via a diffusion model. As a training
objective, we enforce that high-level features of the reference scenes like
look, mood, and emotion should be similar to that of the input video. Our
LUT-based approach allows for color grading without any loss of structural
details in the whole video frames as well as achieving fast inference. We
further build a pipeline to incorporate a user-preference via text prompts for
low-level feature enhancement such as contrast and brightness, etc.
Experimental results, including extensive user studies, demonstrate the
effectiveness of our approach for video color grading. Codes are publicly
available at https://github.com/seunghyuns98/VideoColorGrading.

</details>


### [62] [Your other Left! Vision-Language Models Fail to Identify Relative Positions in Medical Images](https://arxiv.org/abs/2508.00549)
*Daniel Wolf,Heiko Hillenhagen,Billurvan Taskin,Alex Bäuerle,Meinrad Beer,Michael Götz,Timo Ropinski*

Main category: cs.CV

TL;DR: 研究发现当前视觉语言模型在医疗图像中确定相对位置的能力不足，视觉提示有助于性能提高，但仍需更多研究。


<details>
  <summary>Details</summary>
Motivation: 在临床决策中，理解解剖结构和异常的相对位置至关重要，因此需要解决视觉语言模型在医疗图像中相对位置判断能力不足的问题。

Method: 使用视觉提示（例如放置在解剖结构上的字母或颜色标记）进行实验以提高模型性能，并创建了MIRP数据集用于系统化评估。

Result: 模型在医疗图像上的相对位置判断能力有限，虽然视觉提示提供了一些改善，但性能仍然不理想。视觉语言模型往往依赖先前的解剖学知识而非图像内容。

Conclusion: 当前的视觉语言模型在确定医疗图像中相对位置的能力上表现不佳，尽管使用视觉提示可以有所改善，但结果仍显著低于自然图像中的表现。

Abstract: Clinical decision-making relies heavily on understanding relative positions
of anatomical structures and anomalies. Therefore, for Vision-Language Models
(VLMs) to be applicable in clinical practice, the ability to accurately
determine relative positions on medical images is a fundamental prerequisite.
Despite its importance, this capability remains highly underexplored. To
address this gap, we evaluate the ability of state-of-the-art VLMs, GPT-4o,
Llama3.2, Pixtral, and JanusPro, and find that all models fail at this
fundamental task. Inspired by successful approaches in computer vision, we
investigate whether visual prompts, such as alphanumeric or colored markers
placed on anatomical structures, can enhance performance. While these markers
provide moderate improvements, results remain significantly lower on medical
images compared to observations made on natural images. Our evaluations suggest
that, in medical imaging, VLMs rely more on prior anatomical knowledge than on
actual image content for answering relative position questions, often leading
to incorrect conclusions. To facilitate further research in this area, we
introduce the MIRP , Medical Imaging Relative Positioning, benchmark dataset,
designed to systematically evaluate the capability to identify relative
positions in medical images.

</details>


### [63] [DBLP: Noise Bridge Consistency Distillation For Efficient And Reliable Adversarial Purification](https://arxiv.org/abs/2508.00552)
*Chihan Huang,Belal Alsinglawi,Islam Al-qudah*

Main category: cs.CV

TL;DR: The paper presents DBLP, a fast, effective adversarial purification method achieving high accuracy and image quality, suitable for real-time application.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks are highly successful but vulnerable to adversarial perturbations. Existing purification methods are too slow for practical use, necessitating faster, efficient solutions.

Method: The paper introduces Diffusion Bridge Distillation for Purification (DBLP), employing a new noise bridge distillation objective in a latent consistency model, along with adaptive semantic enhancement using multi-scale pyramid edge maps.

Result: DBLP achieves state-of-the-art robust accuracy and image quality with around 0.2s inference time, confirmed by extensive experiments on multiple datasets.

Conclusion: DBLP significantly improves robust accuracy and image quality in adversarial purification, with a fast inference time, enhancing its practicality for real-time applications.

Abstract: Recent advances in deep neural networks (DNNs) have led to remarkable success
across a wide range of tasks. However, their susceptibility to adversarial
perturbations remains a critical vulnerability. Existing diffusion-based
adversarial purification methods often require intensive iterative denoising,
severely limiting their practical deployment. In this paper, we propose
Diffusion Bridge Distillation for Purification (DBLP), a novel and efficient
diffusion-based framework for adversarial purification. Central to our approach
is a new objective, noise bridge distillation, which constructs a principled
alignment between the adversarial noise distribution and the clean data
distribution within a latent consistency model (LCM). To further enhance
semantic fidelity, we introduce adaptive semantic enhancement, which fuses
multi-scale pyramid edge maps as conditioning input to guide the purification
process. Extensive experiments across multiple datasets demonstrate that DBLP
achieves state-of-the-art (SOTA) robust accuracy, superior image quality, and
around 0.2s inference time, marking a significant step toward real-time
adversarial purification.

</details>


### [64] [HiPrune: Training-Free Visual Token Pruning via Hierarchical Attention in Vision-Language Models](https://arxiv.org/abs/2508.00553)
*Jizhihui Liu,Feiyi Du,Guangdao Zhu,Niu Lian,Jun Li,Bin Chen*

Main category: cs.CV

TL;DR: HiPrune是一种无需训练的令牌剪枝框架，通过层次化注意力结构选择性保留重要令牌，大幅提高推理效率，同时保持任务准确性。


<details>
  <summary>Details</summary>
Motivation: 对视觉语言模型的图像令牌序列进行剪枝，以减少计算开销并提高推理效率。之前的方法依赖特殊令牌或需要任务特定训练，不利于跨架构扩展。

Method: HiPrune选择三类信息量大的令牌：(1)在以物体为中心的层中具有高注意力的锚定令牌；(2)邻近锚定令牌保证空间连续性的缓冲令牌；(3)在深层中具有强注意力用于全局总结的注册令牌。

Result: HiPrune在LLaVA-1.5、LLaVA-NeXT和Qwen2.5-VL上的大量实验表明，它实现了最先进的剪枝性能，在仅使用33.3%令牌的情况下保留了高达99.3%的任务准确度，甚至在仅使用11.1%令牌时保持了99.5%的准确度，并将推理FLOPs和延迟减少高达9倍。

Conclusion: 本文提出了一个无需训练且与模型无关的令牌剪枝框架HiPrune，通过利用视觉编码器中的层次化注意力结构来提高推理效率，同时保持任务准确性。

Abstract: Vision-Language Models (VLMs) encode images into lengthy sequences of visual
tokens, leading to excessive computational overhead and limited inference
efficiency. While prior efforts prune or merge tokens to address this issue,
they often rely on special tokens (e.g., CLS) or require task-specific
training, hindering scalability across architectures. In this paper, we propose
HiPrune, a training-free and model-agnostic token Pruning framework that
exploits the Hierarchical attention structure within vision encoders. We
identify that middle layers attend to object-centric regions, while deep layers
capture global contextual features. Based on this observation, HiPrune selects
three types of informative tokens: (1) Anchor tokens with high attention in
object-centric layers, (2) Buffer tokens adjacent to anchors for spatial
continuity, and (3) Register tokens with strong attention in deep layers for
global summarization. Our method requires no retraining and integrates
seamlessly with any ViT-based VLM. Extensive experiments on LLaVA-1.5,
LLaVA-NeXT, and Qwen2.5-VL demonstrate that HiPrune achieves state-of-the-art
pruning performance, preserving up to 99.3% task accuracy with only 33.3%
tokens, and maintaining 99.5% accuracy with just 11.1% tokens. Meanwhile, it
reduces inference FLOPs and latency by up to 9$\times$, showcasing strong
generalization across models and tasks. Code is available at
https://github.com/Danielement321/HiPrune.

</details>


### [65] [Training-Free Class Purification for Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2508.00557)
*Qi Chen,Lingxiao Yang,Yun Chen,Nailong Zhao,Jianhuang Lai,Jie Shao,Xiaohua Xie*

Main category: cs.CV

TL;DR: 提出FreeCP，一个无训练的类净化框架，解决类别冗余和视觉语言歧义问题，大幅提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有的无训练方法主要关注模型架构的修改和生成原型以提高分割性能，但忽视了类别冗余和视觉语言歧义带来的挑战。这些问题会导致次优的类激活图和亲和力精制激活图。

Method: 提出了FreeCP，一个新的无训练类净化框架，重点解决类别冗余和视觉语言歧义带来的问题。净化的类表示用于生成最终分割预测。

Result: FreeCP在结合其他OVSS方法时，作为插件模块，能够显著提升分割性能。

Conclusion: FreeCP作为插件模块，可以与其他OVSS方法结合，显著提高分割性能。

Abstract: Fine-tuning pre-trained vision-language models has emerged as a powerful
approach for enhancing open-vocabulary semantic segmentation (OVSS). However,
the substantial computational and resource demands associated with training on
large datasets have prompted interest in training-free methods for OVSS.
Existing training-free approaches primarily focus on modifying model
architectures and generating prototypes to improve segmentation performance.
However, they often neglect the challenges posed by class redundancy, where
multiple categories are not present in the current test image, and
visual-language ambiguity, where semantic similarities among categories create
confusion in class activation. These issues can lead to suboptimal class
activation maps and affinity-refined activation maps. Motivated by these
observations, we propose FreeCP, a novel training-free class purification
framework designed to address these challenges. FreeCP focuses on purifying
semantic categories and rectifying errors caused by redundancy and ambiguity.
The purified class representations are then leveraged to produce final
segmentation predictions. We conduct extensive experiments across eight
benchmarks to validate FreeCP's effectiveness. Results demonstrate that FreeCP,
as a plug-and-play module, significantly boosts segmentation performance when
combined with other OVSS methods.

</details>


### [66] [Guiding Diffusion-Based Articulated Object Generation by Partial Point Cloud Alignment and Physical Plausibility Constraints](https://arxiv.org/abs/2508.00558)
*Jens U. Kreber,Joerg Stueckler*

Main category: cs.CV

TL;DR: PhysNAP是一个基于扩散模型的方法，旨在生成与点云对齐且物理合理的可分解物体，提升了约束一致性，并在PartNet-Mobility数据集上的评估中表现出色。


<details>
  <summary>Details</summary>
Motivation: 生成与部分点云对齐的可分解物体，并提高它们的物理合理性，以在日常环境中增强物体的互动功能。

Method: 使用一种基于扩散模型的方法，运用签名距离函数（SDFs）表示部件形状，通过逆向扩散过程引导来匹配点云，并加入非穿透和移动性约束以提高物理合理性。同时，方法是类别感知的，可以使用类别信息提高对点云的匹配能力。

Result: 通过使用PartNet-Mobility数据集，对PhysNAP生成的样本进行评估，展示了其在约束一致性改进方面的有效性，并提供了与生成能力之间的权衡。

Conclusion: PhysNAP展示了在生成符合物理约束的可分解物体时，与基于启发模型的对比方式的改进。

Abstract: Articulated objects are an important type of interactable objects in everyday
environments. In this paper, we propose PhysNAP, a novel diffusion model-based
approach for generating articulated objects that aligns them with partial point
clouds and improves their physical plausibility. The model represents part
shapes by signed distance functions (SDFs). We guide the reverse diffusion
process using a point cloud alignment loss computed using the predicted SDFs.
Additionally, we impose non-penetration and mobility constraints based on the
part SDFs for guiding the model to generate more physically plausible objects.
We also make our diffusion approach category-aware to further improve point
cloud alignment if category information is available. We evaluate the
generative ability and constraint consistency of samples generated with PhysNAP
using the PartNet-Mobility dataset. We also compare it with an unguided
baseline diffusion model and demonstrate that PhysNAP can improve constraint
consistency and provides a tradeoff with generative ability.

</details>


### [67] [Weakly Supervised Virus Capsid Detection with Image-Level Annotations in Electron Microscopy Images](https://arxiv.org/abs/2508.00563)
*Hannah Kniesel,Leon Sick,Tristan Payer,Tim Bergner,Kavitha Shaga Devan,Clarissa Read,Paul Walther,Timo Ropinski*

Main category: cs.CV

TL;DR: 提出了一种新的弱监督算法，仅需图像级别标注即可实现有效的目标检测，标注效果在有限时间内超过传统方法和真实标签。


<details>
  <summary>Details</summary>
Motivation: 从数据标注成本高、需要领域专家的挑战出发，提出了一种利用易获得的图像级别标注的弱监督目标检测方法。

Method: 使用一个预训练模型的知识，用于预测图像中病毒存在与否，生成伪标签，然后用这些伪标签训练一个最先进的目标检测模型。采用收缩感受野的优化方法直接提取病毒颗粒而无需特定的网络架构。

Result: 通过广泛的研究表明，该方法生成的伪标签较易获得，且在标注时间有限的情况下，性能优于其他现存的弱标注方法甚至是真实标签。

Conclusion: 提出了一种领域特定的弱监督目标检测算法，使用较容易获得的图像级别标注代替困难的边界框标注，结果表明，该算法在有限时间内的标注效果优于现有的弱标注方法和真实标注。

Abstract: Current state-of-the-art methods for object detection rely on annotated
bounding boxes of large data sets for training. However, obtaining such
annotations is expensive and can require up to hundreds of hours of manual
labor. This poses a challenge, especially since such annotations can only be
provided by experts, as they require knowledge about the scientific domain. To
tackle this challenge, we propose a domain-specific weakly supervised object
detection algorithm that only relies on image-level annotations, which are
significantly easier to acquire. Our method distills the knowledge of a
pre-trained model, on the task of predicting the presence or absence of a virus
in an image, to obtain a set of pseudo-labels that can be used to later train a
state-of-the-art object detection model. To do so, we use an optimization
approach with a shrinking receptive field to extract virus particles directly
without specific network architectures. Through a set of extensive studies, we
show how the proposed pseudo-labels are easier to obtain, and, more
importantly, are able to outperform other existing weak labeling methods, and
even ground truth labels, in cases where the time to obtain the annotation is
limited.

</details>


### [68] [CoProU-VO: Combining Projected Uncertainty for End-to-End Unsupervised Monocular Visual Odometry](https://arxiv.org/abs/2508.00568)
*Jingchao Xie,Oussema Dhaouadi,Weirong Chen,Johannes Meier,Jacques Kaiser,Daniel Cremers*

Main category: cs.CV

TL;DR: 研究提出了一种新型端到端视觉测距法CoProU-VO，通过结合帧间不确定性解决动态场景中的位姿估计问题，实验结果表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统的视觉测距方法在处理动态对象时存在局限。本研究旨在通过不确定性传播提高动态场景中区域识别的准确性。

Method: 利用不确定性建模技术，通过结合帧间不确定性以过滤动态对象和遮挡，无需明确的运动分割。

Result: 在KITTI和nuScenes数据集上的实验显著优于之前的无监督单目端到端双帧方法，在复杂的高速公路场景中表现优异。

Conclusion: 研究提出了一种新型的端到端视觉测距方法——CoProU-VO，通过结合目标帧和参考帧的不确定性，以解决动态场景中的错误位姿估计问题。实验结果表明在KITTI和nuScenes数据集上表现优异。

Abstract: Visual Odometry (VO) is fundamental to autonomous navigation, robotics, and
augmented reality, with unsupervised approaches eliminating the need for
expensive ground-truth labels. However, these methods struggle when dynamic
objects violate the static scene assumption, leading to erroneous pose
estimations. We tackle this problem by uncertainty modeling, which is a
commonly used technique that creates robust masks to filter out dynamic objects
and occlusions without requiring explicit motion segmentation. Traditional
uncertainty modeling considers only single-frame information, overlooking the
uncertainties across consecutive frames. Our key insight is that uncertainty
must be propagated and combined across temporal frames to effectively identify
unreliable regions, particularly in dynamic scenes. To address this challenge,
we introduce Combined Projected Uncertainty VO (CoProU-VO), a novel end-to-end
approach that combines target frame uncertainty with projected reference frame
uncertainty using a principled probabilistic formulation. Built upon vision
transformer backbones, our model simultaneously learns depth, uncertainty
estimation, and camera poses. Consequently, experiments on the KITTI and
nuScenes datasets demonstrate significant improvements over previous
unsupervised monocular end-to-end two-frame-based methods and exhibit strong
performance in challenging highway scenes where other approaches often fail.
Additionally, comprehensive ablation studies validate the effectiveness of
cross-frame uncertainty propagation.

</details>


### [69] [Uncertainty-Aware Likelihood Ratio Estimation for Pixel-Wise Out-of-Distribution Detection](https://arxiv.org/abs/2508.00587)
*Marc Hölle,Walter Kellermann,Vasileios Belagiannis*

Main category: cs.CV

TL;DR: 提出了一种不确定性感知的似然比估计方法，用于处理语义分割模型中未知对象识别，取得了优异的识别性能和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在检测真实世界自动驾驶场景中的未知对象时存在困难，尤其是复杂场景中稀有对象类容易与真正未知对象混淆。

Method: 利用一种证据分类器在似然比测试中区分语义分割模型中的已知和未知像素特征，同时明确考虑不确定性。

Result: 在五个标准基准数据集上评估，该方法在保持高平均精准率（90.91%）的同时，实现了最低的平均假阳性率（2.5%），且计算开销可忽略不计。

Conclusion: 本文提出了一种不确定性感知的似然比估计方法，在语义分割模型识别未知对象时，能够有效解决现有方法在复杂场景下的不足。

Abstract: Semantic segmentation models trained on known object classes often fail in
real-world autonomous driving scenarios by confidently misclassifying unknown
objects. While pixel-wise out-of-distribution detection can identify unknown
objects, existing methods struggle in complex scenes where rare object classes
are often confused with truly unknown objects. We introduce an
uncertainty-aware likelihood ratio estimation method that addresses these
limitations. Our approach uses an evidential classifier within a likelihood
ratio test to distinguish between known and unknown pixel features from a
semantic segmentation model, while explicitly accounting for uncertainty.
Instead of producing point estimates, our method outputs probability
distributions that capture uncertainty from both rare training examples and
imperfect synthetic outliers. We show that by incorporating uncertainty in this
way, outlier exposure can be leveraged more effectively. Evaluated on five
standard benchmark datasets, our method achieves the lowest average false
positive rate (2.5%) among state-of-the-art while maintaining high average
precision (90.91%) and incurring only negligible computational overhead. Code
is available at https://github.com/glasbruch/ULRE.

</details>


### [70] [Context-based Motion Retrieval using Open Vocabulary Methods for Autonomous Driving](https://arxiv.org/abs/2508.00589)
*Stefan Englmeier,Max A. Büttner,Katharina Winter,Fabian B. Flohr*

Main category: cs.CV

TL;DR: 提出了一个可通过文本检索人类行为的上下文感知运动检索框架，并在WayMoCo数据集上大幅度提高了检索准确性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统必须在涉及不寻常或复杂行为的实时路况中可靠运行，需要在驾驶数据集中识别这些极端情况以进行鲁棒评估和泛化。

Method: 提出了一种新颖的上下文感知运动检索框架，将SMPL基础的运动序列与对应的视频帧结合，编码到与自然语言对齐的共享多模态嵌入空间。

Result: 我们的方法能够通过文本查询扩展性地检索人类行为及其上下文，并在WayMoCo数据集评估中比最先进的模型提高了27.5%的准确度。

Conclusion: 我们的框架支持在各种人类中心场景中对自动驾驶系统进行有针对性的评估。

Abstract: Autonomous driving systems must operate reliably in safety-critical
scenarios, particularly those involving unusual or complex behavior by
Vulnerable Road Users (VRUs). Identifying these edge cases in driving datasets
is essential for robust evaluation and generalization, but retrieving such rare
human behavior scenarios within the long tail of large-scale datasets is
challenging. To support targeted evaluation of autonomous driving systems in
diverse, human-centered scenarios, we propose a novel context-aware motion
retrieval framework. Our method combines Skinned Multi-Person Linear
(SMPL)-based motion sequences and corresponding video frames before encoding
them into a shared multimodal embedding space aligned with natural language.
Our approach enables the scalable retrieval of human behavior and their context
through text queries. This work also introduces our dataset WayMoCo, an
extension of the Waymo Open Dataset. It contains automatically labeled motion
and scene context descriptions derived from generated pseudo-ground-truth SMPL
sequences and corresponding image data. Our approach outperforms
state-of-the-art models by up to 27.5% accuracy in motion-context retrieval,
when evaluated on the WayMoCo dataset.

</details>


### [71] [A Novel Modeling Framework and Data Product for Extended VIIRS-like Artificial Nighttime Light Image Reconstruction (1986-2024)](https://arxiv.org/abs/2508.00590)
*Yihe Tian,Kwan Man Cheng,Zhengbo Zhang,Tao Zhang,Suju Li,Dongmei Yan,Bing Xu*

Main category: cs.CV

TL;DR: The paper introduces EVAL, a new dataset that extends VIIRS-like nighttime light data back to 1986, addressing previous limitations and showing superior performance metrics.


<details>
  <summary>Details</summary>
Motivation: Long-term time-series studies are restricted due to the temporal coverage of NPP-VIIRS sensor starting from 2012. Current methods underestimate light intensity and omit structural details.

Method: The paper proposes a novel reconstruction framework with a construction stage using a Hierarchical Fusion Decoder (HFD) and a refinement stage using a Dual Feature Refiner (DFR), along with high-resolution impervious surface masks.

Result: EVAL product extends VIIRS-like NTL time-series back to 1986 for China and demonstrates superior performance with R2 increased from 0.68 to 0.80 and RMSE reduced from 1.27 to 0.99 compared to existing products.

Conclusion: EVAL significantly outperforms existing products, with improved R2 and lowered RMSE, showing excellent temporal consistency and strong correlation with socioeconomic parameters.

Abstract: Artificial Night-Time Light (NTL) remote sensing is a vital proxy for
quantifying the intensity and spatial distribution of human activities.
Although the NPP-VIIRS sensor provides high-quality NTL observations, its
temporal coverage, which begins in 2012, restricts long-term time-series
studies that extend to earlier periods. Despite the progress in extending
VIIRS-like NTL time-series, current methods still suffer from two significant
shortcomings: the underestimation of light intensity and the structural
omission. To overcome these limitations, we propose a novel reconstruction
framework consisting of a two-stage process: construction and refinement. The
construction stage features a Hierarchical Fusion Decoder (HFD) designed to
enhance the fidelity of the initial reconstruction. The refinement stage
employs a Dual Feature Refiner (DFR), which leverages high-resolution
impervious surface masks to guide and enhance fine-grained structural details.
Based on this framework, we developed the Extended VIIRS-like Artificial
Nighttime Light (EVAL) product for China, extending the standard data record
backwards by 26 years to begin in 1986. Quantitative evaluation shows that EVAL
significantly outperforms existing state-of-the-art products, boosting the
$\text{R}^2$ from 0.68 to 0.80 while lowering the RMSE from 1.27 to 0.99.
Furthermore, EVAL exhibits excellent temporal consistency and maintains a high
correlation with socioeconomic parameters, confirming its reliability for
long-term analysis. The resulting EVAL dataset provides a valuable new resource
for the research community and is publicly available at
https://doi.org/10.11888/HumanNat.tpdc.302930.

</details>


### [72] [Wukong Framework for Not Safe For Work Detection in Text-to-Image systems](https://arxiv.org/abs/2508.00591)
*Mingrui Liu,Sixiao Zhang,Cheng Long*

Main category: cs.CV

TL;DR: 提出Wukong，通过在扩散过程中进行早期NSFW内容检测，提升了效率，优于文本过滤，匹敌图像过滤。


<details>
  <summary>Details</summary>
Motivation: 现有外部保护措施的局限性，包括文本过滤的对抗攻击弱点和图像过滤的高计算成本与延迟。

Method: Wukong使用基于transformer的NSFW检测框架，在扩散过程中操作，借助早期去噪步骤的中间输出，并重用U-Net的预训练交叉注意参数。

Result: Wukong显著优于文本过滤器，并在效率上超越图像过滤器，同时保持相当的准确性。

Conclusion: Wukong在效率上显著优于文本过滤器，与图像过滤器达到相当的准确度。

Abstract: Text-to-Image (T2I) generation is a popular AI-generated content (AIGC)
technology enabling diverse and creative image synthesis. However, some outputs
may contain Not Safe For Work (NSFW) content (e.g., violence), violating
community guidelines. Detecting NSFW content efficiently and accurately, known
as external safeguarding, is essential. Existing external safeguards fall into
two types: text filters, which analyze user prompts but overlook T2I
model-specific variations and are prone to adversarial attacks; and image
filters, which analyze final generated images but are computationally costly
and introduce latency. Diffusion models, the foundation of modern T2I systems
like Stable Diffusion, generate images through iterative denoising using a
U-Net architecture with ResNet and Transformer blocks. We observe that: (1)
early denoising steps define the semantic layout of the image, and (2)
cross-attention layers in U-Net are crucial for aligning text and image
regions. Based on these insights, we propose Wukong, a transformer-based NSFW
detection framework that leverages intermediate outputs from early denoising
steps and reuses U-Net's pre-trained cross-attention parameters. Wukong
operates within the diffusion process, enabling early detection without waiting
for full image generation. We also introduce a new dataset containing prompts,
seeds, and image-specific NSFW labels, and evaluate Wukong on this and two
public benchmarks. Results show that Wukong significantly outperforms
text-based safeguards and achieves comparable accuracy of image filters, while
offering much greater efficiency.

</details>


### [73] [GeoMoE: Divide-and-Conquer Motion Field Modeling with Mixture-of-Experts for Two-View Geometry](https://arxiv.org/abs/2508.00592)
*Jiajun Le,Jiayi Ma*

Main category: cs.CV

TL;DR: 提出了一种名为GeoMoE的框架，通过Mixture-of-Experts（MoE）策略，在两视图几何中进行运动场建模，解决复杂场景中运动场的多样性和异质性问题。


<details>
  <summary>Details</summary>
Motivation: 当前的方法在真实复杂场景中处理运动场时，缺乏针对性的建模策略，不能很好地适应极端视角变化、尺度变化以及显著深度不连续的情况。

Method: GeoMoE框架利用Mixture-of-Experts策略，通过结构感知的解构，将运动场分解为异质运动子区域。然后对每个子区域通过MoE增强的双路径校正进行增强，并将其分配给专用的专家进行建模。

Result: GeoMoE在相对姿态和单应性估计上优于现有的最新方法，具有很强的泛化能力，并提供了源码和预训练模型。

Conclusion: 通过将复杂场景下的运动场分解为异质子域并采用专用专家模型进行建模，有效解决了运动场的多样性和异质性问题，实现了更精细的运动场校正。

Abstract: Recent progress in two-view geometry increasingly emphasizes enforcing
smoothness and global consistency priors when estimating motion fields between
pairs of images. However, in complex real-world scenes, characterized by
extreme viewpoint and scale changes as well as pronounced depth
discontinuities, the motion field often exhibits diverse and heterogeneous
motion patterns. Most existing methods lack targeted modeling strategies and
fail to explicitly account for this variability, resulting in estimated motion
fields that diverge from their true underlying structure and distribution. We
observe that Mixture-of-Experts (MoE) can assign dedicated experts to motion
sub-fields, enabling a divide-and-conquer strategy for heterogeneous motion
patterns. Building on this insight, we re-architect motion field modeling in
two-view geometry with GeoMoE, a streamlined framework. Specifically, we first
devise a Probabilistic Prior-Guided Decomposition strategy that exploits inlier
probability signals to perform a structure-aware decomposition of the motion
field into heterogeneous sub-fields, sharply curbing outlier-induced bias.
Next, we introduce an MoE-Enhanced Bi-Path Rectifier that enhances each
sub-field along spatial-context and channel-semantic paths and routes it to a
customized expert for targeted modeling, thereby decoupling heterogeneous
motion regimes, suppressing cross-sub-field interference and representational
entanglement, and yielding fine-grained motion-field rectification. With this
minimalist design, GeoMoE outperforms prior state-of-the-art methods in
relative pose and homography estimation and shows strong generalization. The
source code and pre-trained models are available at
https://github.com/JiajunLe/GeoMoE.

</details>


### [74] [DPoser-X: Diffusion Model as Robust 3D Whole-body Human Pose Prior](https://arxiv.org/abs/2508.00599)
*Junzhe Lu,Jing Lin,Hongkun Dou,Ailing Zeng,Yue Deng,Xian Liu,Zhongang Cai,Lei Yang,Yulun Zhang,Haoqian Wang,Ziwei Liu*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的全身体人类姿态建模方法DPoser-X，并通过实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于人类姿态的复杂性以及高质量全身体姿态数据集的稀缺，构建一个通用且稳健的全身体人类姿态先验仍然具有挑战性。

Method: DPoser-X方法采用了变分扩散采样，通过引入一个新的截断时间步调度方法和一个遮蔽的训练机制，提高了在下游应用中的性能。

Result: 实验表明，DPoser-X能够有效捕捉身体部位之间的相互依赖关系，同时避免特定动作的过拟合，展示了其在身体、手、脸及全身体姿态建模方面的坚固性和多功能性。

Conclusion: DPoser-X模型在多个基准测试中表现出色，设立了全身体人类姿态建模的新基准，并超越了现有的最先进替代方案。

Abstract: We present DPoser-X, a diffusion-based prior model for 3D whole-body human
poses. Building a versatile and robust full-body human pose prior remains
challenging due to the inherent complexity of articulated human poses and the
scarcity of high-quality whole-body pose datasets. To address these
limitations, we introduce a Diffusion model as body Pose prior (DPoser) and
extend it to DPoser-X for expressive whole-body human pose modeling. Our
approach unifies various pose-centric tasks as inverse problems, solving them
through variational diffusion sampling. To enhance performance on downstream
applications, we introduce a novel truncated timestep scheduling method
specifically designed for pose data characteristics. We also propose a masked
training mechanism that effectively combines whole-body and part-specific
datasets, enabling our model to capture interdependencies between body parts
while avoiding overfitting to specific actions. Extensive experiments
demonstrate DPoser-X's robustness and versatility across multiple benchmarks
for body, hand, face, and full-body pose modeling. Our model consistently
outperforms state-of-the-art alternatives, establishing a new benchmark for
whole-body human pose prior modeling.

</details>


### [75] [Backdoor Attacks on Deep Learning Face Detection](https://arxiv.org/abs/2508.00620)
*Quentin Le Roux,Yannick Teglia,Teddy Furon,Philippe Loubet-Moundi*

Main category: cs.CV

TL;DR: 论文研究了人脸识别系统中的人脸检测模块受物体生成攻击和地标偏移攻击影响，提供了应对这些漏洞的缓解方案。


<details>
  <summary>Details</summary>
Motivation: 人脸识别系统在不受约束的环境中由于光照不一致和脸部姿态多样性带来的挑战，需要一个回归边界框和地标坐标的人脸检测模块进行人脸对齐。不过，该模块可能受到攻击，因此需要研究其潜在漏洞并提供解决方案。

Method: 该研究通过展示物体生成攻击（称为人脸生成攻击）对人脸检测的有效性，首次演示了一种后门攻击，这种攻击能够影响由人脸检测器执行的坐标回归任务。研究者还提供了应对此类攻击的缓解措施。

Result: 该研究证明了物体生成攻击能够有效影响人脸检测，首次展示了如何通过地标偏移攻击影响坐标回归任务。此外，还提供了应对这些攻击的缓解方案，增强了人脸识别系统的鲁棒性。

Conclusion: 该论文展示了在不受约束的环境下，人脸识别系统在面临多种挑战时，尤其是面对人脸检测模块时，可能受到物体生成攻击的影响，并首次提出了地标偏移攻击。作者还提供了针对这些漏洞的缓解方案。

Abstract: Face Recognition Systems that operate in unconstrained environments capture
images under varying conditions,such as inconsistent lighting, or diverse face
poses. These challenges require including a Face Detection module that
regresses bounding boxes and landmark coordinates for proper Face Alignment.
This paper shows the effectiveness of Object Generation Attacks on Face
Detection, dubbed Face Generation Attacks, and demonstrates for the first time
a Landmark Shift Attack that backdoors the coordinate regression task performed
by face detectors. We then offer mitigations against these vulnerabilities.

</details>


### [76] [Minimum Data, Maximum Impact: 20 annotated samples for explainable lung nodule classification](https://arxiv.org/abs/2508.00639)
*Luisa Gallée,Catharina Silvia Lisson,Christoph Gerhard Lisson,Daniela Drees,Felix Weig,Daniel Vogele,Meinrad Beer,Michael Götz*

Main category: cs.CV

TL;DR: 研究通过生成模型合成属性标注数据，提高医学图像诊断可解释模型的性能，解决数据集规模限制问题。


<details>
  <summary>Details</summary>
Motivation: 提高可解释分类模型在医学图像诊断中的透明度和模型输出的显性验证。由于缺乏大规模的带有这些属性标注的医学图像数据集，因此需要合成属性标注数据。

Method: 使用生成模型合成属性标注数据，增强扩散模型的属性条件，并仅使用 LIDC-IDRI 数据集中的 20 个带属性标签的肺结节样本进行训练。

Result: 将生成图像纳入可解释模型的训练中，提高属性预测准确性13.4%和目标预测准确性1.8%。

Conclusion: 合成数据有潜力克服数据集的局限性，加强可解释模型在医学图像分析中的适用性。

Abstract: Classification models that provide human-interpretable explanations enhance
clinicians' trust and usability in medical image diagnosis. One research focus
is the integration and prediction of pathology-related visual attributes used
by radiologists alongside the diagnosis, aligning AI decision-making with
clinical reasoning. Radiologists use attributes like shape and texture as
established diagnostic criteria and mirroring these in AI decision-making both
enhances transparency and enables explicit validation of model outputs.
However, the adoption of such models is limited by the scarcity of large-scale
medical image datasets annotated with these attributes. To address this
challenge, we propose synthesizing attribute-annotated data using a generative
model. We enhance the Diffusion Model with attribute conditioning and train it
using only 20 attribute-labeled lung nodule samples from the LIDC-IDRI dataset.
Incorporating its generated images into the training of an explainable model
boosts performance, increasing attribute prediction accuracy by 13.4% and
target prediction accuracy by 1.8% compared to training with only the small
real attribute-annotated dataset. This work highlights the potential of
synthetic data to overcome dataset limitations, enhancing the applicability of
explainable models in medical image analysis.

</details>


### [77] [Revisiting Adversarial Patch Defenses on Object Detectors: Unified Evaluation, Large-Scale Dataset, and New Insights](https://arxiv.org/abs/2508.00649)
*Junhao Zheng,Jiahao Sun,Chenhao Lin,Zhengyu Zhao,Chen Ma,Chong Zhang,Cong Wang,Qian Wang,Chao Shen*

Main category: cs.CV

TL;DR: 我们提出了第一个补丁防御基准，分析揭示了自然化补丁防御的挑战在于数据分布。新数据集提高了防御性能，并发现平均精度与防御性能一致，复杂或随机模型的防御更稳健。


<details>
  <summary>Details</summary>
Motivation: 现有的防御评估缺乏统一和综合的框架，导致对当前方法的评估不一致和不完整。

Method: 我们重新审视了11种代表性防御，并提出了第一个补丁防御基准，涉及两种攻击目标、13种补丁攻击、11种对象检测器和4种不同的度量标准。

Result: 使用我们多样化补丁分布的新数据集可以提高现有防御性能15.09% AP@0.5。平均精度与防御性能高度一致。复杂或随机模型的防御相对稳健。

Conclusion: 我们进行了全面分析，并提出了新的见解，即应对自然化攻击的挑战更多来自数据分布，而不是通常认为的高频率。通过我们的数据集，可以提高现有防御的性能。

Abstract: Developing reliable defenses against patch attacks on object detectors has
attracted increasing interest. However, we identify that existing defense
evaluations lack a unified and comprehensive framework, resulting in
inconsistent and incomplete assessments of current methods. To address this
issue, we revisit 11 representative defenses and present the first patch
defense benchmark, involving 2 attack goals, 13 patch attacks, 11 object
detectors, and 4 diverse metrics. This leads to the large-scale adversarial
patch dataset with 94 types of patches and 94,000 images. Our comprehensive
analyses reveal new insights: (1) The difficulty in defending against
naturalistic patches lies in the data distribution, rather than the commonly
believed high frequencies. Our new dataset with diverse patch distributions can
be used to improve existing defenses by 15.09% AP@0.5. (2) The average
precision of the attacked object, rather than the commonly pursued patch
detection accuracy, shows high consistency with defense performance. (3)
Adaptive attacks can substantially bypass existing defenses, and defenses with
complex/stochastic models or universal patch properties are relatively robust.
We hope that our analyses will serve as guidance on properly evaluating patch
attacks/defenses and advancing their design. Code and dataset are available at
https://github.com/Gandolfczjh/APDE, where we will keep integrating new
attacks/defenses.

</details>


### [78] [Can Large Pretrained Depth Estimation Models Help With Image Dehazing?](https://arxiv.org/abs/2508.00698)
*Hongfei Zhang,Kun Zhou,Ruizheng Wu,Jiangbo Lu*

Main category: cs.CV

TL;DR: 提出了一种新的RGB-D融合模块用于图像去雾，证明其在多个基准测试上有效且广泛适用。


<details>
  <summary>Details</summary>
Motivation: 现有方法的架构设计限制了在不同场景中的适应性，旨在解决多样场景的精度和效率问题。

Method: 进行系统实验，验证预训练深度表征的泛化能力，并提出RGB-D融合模块。

Result: 实验证明所提出的方法在多个基准上都是有效的和广泛适用的。

Conclusion: 提出了一种可插拔的RGB-D融合模块，可与多种去雾架构集成，验证了其有效性和广泛适用性。

Abstract: Image dehazing remains a challenging problem due to the spatially varying
nature of haze in real-world scenes. While existing methods have demonstrated
the promise of large-scale pretrained models for image dehazing, their
architecture-specific designs hinder adaptability across diverse scenarios with
different accuracy and efficiency requirements. In this work, we systematically
investigate the generalization capability of pretrained depth
representations-learned from millions of diverse images-for image dehazing. Our
empirical analysis reveals that the learned deep depth features maintain
remarkable consistency across varying haze levels. Building on this insight, we
propose a plug-and-play RGB-D fusion module that seamlessly integrates with
diverse dehazing architectures. Extensive experiments across multiple
benchmarks validate both the effectiveness and broad applicability of our
approach.

</details>


### [79] [D3: Training-Free AI-Generated Video Detection Using Second-Order Features](https://arxiv.org/abs/2508.00701)
*Chende Zheng,Ruiqi suo,Chenhao Lin,Zhengyu Zhao,Le Yang,Shuai Liu,Minghui Yang,Cong Wang,Chao Shen*

Main category: cs.CV

TL;DR: 研究提出一种新的无训练检测方法D3，通过分析时间伪影来检测AI生成视频，其在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法对合成视频中的时间伪影探索不足，因此需要新的方法来更好地检测高保真AI生成视频中的合成内容。

Method: 该研究通过牛顿力学下的二阶动力分析建立理论框架，并扩展用于时间伪影检测的二阶中心差分特征，利用这些特征提出了一种无训练检测方法D3。

Result: D3在4个开源数据集的40个子集上进行了验证，结果显示在GenVideo数据集上，D3的平均精度比之前最好的方法提高了10.39%（绝对值）。此外，实验也验证了D3的计算效率和鲁棒性。

Conclusion: 研究提出了一种新的视频检测方法D3，它在多个开源数据集上的性能优于现有方法，并表现出良好的计算效率和鲁棒性。

Abstract: The evolution of video generation techniques, such as Sora, has made it
increasingly easy to produce high-fidelity AI-generated videos, raising public
concern over the dissemination of synthetic content. However, existing
detection methodologies remain limited by their insufficient exploration of
temporal artifacts in synthetic videos. To bridge this gap, we establish a
theoretical framework through second-order dynamical analysis under Newtonian
mechanics, subsequently extending the Second-order Central Difference features
tailored for temporal artifact detection. Building on this theoretical
foundation, we reveal a fundamental divergence in second-order feature
distributions between real and AI-generated videos. Concretely, we propose
Detection by Difference of Differences (D3), a novel training-free detection
method that leverages the above second-order temporal discrepancies. We
validate the superiority of our D3 on 4 open-source datasets (Gen-Video,
VideoPhy, EvalCrafter, VidProM), 40 subsets in total. For example, on GenVideo,
D3 outperforms the previous best method by 10.39% (absolute) mean Average
Precision. Additional experiments on time cost and post-processing operations
demonstrate D3's exceptional computational efficiency and strong robust
performance. Our code is available at https://github.com/Zig-HS/D3.

</details>


### [80] [MIHBench: Benchmarking and Mitigating Multi-Image Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2508.00726)
*Jiale Li,Mingrui Wu,Zixiang Jin,Hao Chen,Jiayi Ji,Xiaoshuai Sun,Liujuan Cao,Rongrong Ji*

Main category: cs.CV

TL;DR: 提出MIHBench基准和动态注意力平衡机制，减少多图像幻觉并增强语义理解。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在单幅图像的幻觉问题上，缺乏对多图像场景中幻觉的探索。

Method: 使用MIHBench基准，通过三个核心任务系统性研究多图像幻觉问题，并提出动态注意力平衡机制调整跨图像注意力分布。

Result: 实验表明，所提方法有效减少了多图像幻觉的发生，并改进了语义整合和推理的稳定性。

Conclusion: MIHBench以及动态注意力平衡机制显著减少了多图像情况下的幻觉发生，并增强了语义整合和推理稳定性。

Abstract: Despite growing interest in hallucination in Multimodal Large Language
Models, existing studies primarily focus on single-image settings, leaving
hallucination in multi-image scenarios largely unexplored. To address this gap,
we conduct the first systematic study of hallucinations in multi-image MLLMs
and propose MIHBench, a benchmark specifically tailored for evaluating
object-related hallucinations across multiple images. MIHBench comprises three
core tasks: Multi-Image Object Existence Hallucination, Multi-Image Object
Count Hallucination, and Object Identity Consistency Hallucination, targeting
semantic understanding across object existence, quantity reasoning, and
cross-view identity consistency. Through extensive evaluation, we identify key
factors associated with the occurrence of multi-image hallucinations,
including: a progressive relationship between the number of image inputs and
the likelihood of hallucination occurrences; a strong correlation between
single-image hallucination tendencies and those observed in multi-image
contexts; and the influence of same-object image ratios and the positional
placement of negative samples within image sequences on the occurrence of
object identity consistency hallucination. To address these challenges, we
propose a Dynamic Attention Balancing mechanism that adjusts inter-image
attention distributions while preserving the overall visual attention
proportion. Experiments across multiple state-of-the-art MLLMs demonstrate that
our method effectively reduces hallucination occurrences and enhances semantic
integration and reasoning stability in multi-image scenarios.

</details>


### [81] [YOLO-Count: Differentiable Object Counting for Text-to-Image Generation](https://arxiv.org/abs/2508.00728)
*Guanning Zeng,Xiang Zhang,Zirui Wang,Haiyang Xu,Zeyuan Chen,Bingnan Li,Zhuowen Tu*

Main category: cs.CV

TL;DR: YOLO-Count是一种创新的可微分开集词汇数量计数模型，能够高效地为文本到图像生成提供数量控制。


<details>
  <summary>Details</summary>
Motivation: 解决通用计数难题并实现文本到图像生成中的精确数量控制。

Method: 该模型采用了新的“基数”图作为回归目标，结合表示对齐和强-弱混合监督方案，实现了梯度优化。

Result: 实验表明，YOLO-Count在计数准确性上达到了当前最好的效果，并能有效控制文本到图像生成系统中的物体数量。

Conclusion: YOLO-Count在物体计数的准确性上达到了最先进的水平，同时有效地为文本到图像生成系统提供了数量控制。

Abstract: We propose YOLO-Count, a differentiable open-vocabulary object counting model
that tackles both general counting challenges and enables precise quantity
control for text-to-image (T2I) generation. A core contribution is the
'cardinality' map, a novel regression target that accounts for variations in
object size and spatial distribution. Leveraging representation alignment and a
hybrid strong-weak supervision scheme, YOLO-Count bridges the gap between
open-vocabulary counting and T2I generation control. Its fully differentiable
architecture facilitates gradient-based optimization, enabling accurate object
count estimation and fine-grained guidance for generative models. Extensive
experiments demonstrate that YOLO-Count achieves state-of-the-art counting
accuracy while providing robust and effective quantity control for T2I systems.

</details>


### [82] [Rethinking Backbone Design for Lightweight 3D Object Detection in LiDAR](https://arxiv.org/abs/2508.00744)
*Adwait Chandorkar,Hasan Tercan,Tobias Meisen*

Main category: cs.CV

TL;DR: 提出了轻量级的Dense Backbone，减少参数和延迟，稍微降低准确率，易于集成。


<details>
  <summary>Details</summary>
Motivation: 现有的3D目标检测多使用VGG或ResNet作为特征提取主干网络，导致模型复杂度较高。需要探索轻量化的3D目标检测主干网络，以提高处理速度及检测效果。

Method: 本文采用轻量化的Dense Backbone作为3D目标检测的主干网络，并通过适配多种最先进的3D目标检测器（例如PillarNet），验证其在保留大部分检测能力的同时显著降低计算成本。

Result: DensePillarNet在nuScenes测试集上实现了参数量减少29%、延迟减少28%，而检测准确率仅下降2%。

Conclusion: 本文提出了一种适用于3D目标检测的轻量级Dense Backbone，能够显著减少模型参数和计算延迟，同时仅略微降低检测准确率。Dense BackBone可以方便地集成到现有架构中，无需修改其他网络组件。

Abstract: Recent advancements in LiDAR-based 3D object detection have significantly
accelerated progress toward the realization of fully autonomous driving in
real-world environments. Despite achieving high detection performance, most of
the approaches still rely on a VGG-based or ResNet-based backbone for feature
exploration, which increases the model complexity. Lightweight backbone design
is well-explored for 2D object detection, but research on 3D object detection
still remains limited. In this work, we introduce Dense Backbone, a lightweight
backbone that combines the benefits of high processing speed, lightweight
architecture, and robust detection accuracy. We adapt multiple SoTA 3d object
detectors, such as PillarNet, with our backbone and show that with our
backbone, these models retain most of their detection capability at a
significantly reduced computational cost. To our knowledge, this is the first
dense-layer-based backbone tailored specifically for 3D object detection from
point cloud data. DensePillarNet, our adaptation of PillarNet, achieves a 29%
reduction in model parameters and a 28% reduction in latency with just a 2%
drop in detection accuracy on the nuScenes test set. Furthermore, Dense
Backbone's plug-and-play design allows straightforward integration into
existing architectures, requiring no modifications to other network components.

</details>


### [83] [GECO: Geometrically Consistent Embedding with Lightspeed Inference](https://arxiv.org/abs/2508.00746)
*Regine Hartwig,Dominik Muhle,Riccardo Marin,Daniel Cremers*

Main category: cs.CV

TL;DR: GECO通过一种基于最优传输的框架，提升了自监督视觉模型的几何一致性和语义区分能力，显著提高了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 虽然自监督视视觉基础模型能够捕捉语义对应关系，但常常缺乏对3D几何结构的理解。GECO解决了这一问题。

Method: 基于最优传输的训练框架，允许在关键点之外进行监督。

Result: GECO在PFPascal、APK和CUB数据集上取得了最先进的性能，比以前的方法提高了98.2%的帧率，PCK提升了分别是6.0%、6.2%、和4.1%。

Conclusion: 我们引入了GECO，这是一种能够生成几何一致性特征的训练框架。GECO不仅能够在可见性变化下通过几何区分语义部分，同时还具备较高的执行效率和准确性。

Abstract: Recent advances in feature learning have shown that self-supervised vision
foundation models can capture semantic correspondences but often lack awareness
of underlying 3D geometry. GECO addresses this gap by producing geometrically
coherent features that semantically distinguish parts based on geometry (e.g.,
left/right eyes, front/back legs). We propose a training framework based on
optimal transport, enabling supervision beyond keypoints, even under occlusions
and disocclusions. With a lightweight architecture, GECO runs at 30 fps, 98.2%
faster than prior methods, while achieving state-of-the-art performance on
PFPascal, APK, and CUB, improving PCK by 6.0%, 6.2%, and 4.1%, respectively.
Finally, we show that PCK alone is insufficient to capture geometric quality
and introduce new metrics and insights for more geometry-aware feature
learning. Link to project page:
https://reginehartwig.github.io/publications/geco/

</details>


### [84] [Is It Really You? Exploring Biometric Verification Scenarios in Photorealistic Talking-Head Avatar Videos](https://arxiv.org/abs/2508.00748)
*Laura Pedrouzo-Rodriguez,Pedro Delgado-DeRobles,Luis F. Gomez,Ruben Tolosana,Ruben Vera-Rodriguez,Aythami Morales,Julian Fierrez*

Main category: cs.CV

TL;DR: 虚拟头像带来安全风险，本文探讨通过面部动作进行身份验证的方法，以应对头像冒充问题。


<details>
  <summary>Details</summary>
Motivation: 虚拟头像在沟通中越来越普遍，但带来了安全风险。其中一种风险是冒充攻击，通过窃取用户的头像来假装其身份。本文探讨了在此类头像交流情况下的生物识别验证问题。

Method: 引入一个新的数据集，其中包含使用先进的单次头像生成模型GAGAvatar创建的真实头像视频，以及伪造者头像视频。提出了一种轻量级、可解释的时空图卷积网络架构，结合时间注意力池化，采用面部地标来建模动态面部动作。

Result: 实验结果表明，面部动作线索可以实现有意义的身份验证，AUC值接近80%。

Conclusion: 在虚拟头像交流系统中，面部动作可以作为可靠的行为生物特征进行身份验证。这种验证方法可以在视觉上与拥有者相似的头像中有效运行。提出的方法提供了较高的身份验证准确性。

Abstract: Photorealistic talking-head avatars are becoming increasingly common in
virtual meetings, gaming, and social platforms. These avatars allow for more
immersive communication, but they also introduce serious security risks. One
emerging threat is impersonation: an attacker can steal a user's
avatar-preserving their appearance and voice-making it nearly impossible to
detect its fraudulent usage by sight or sound alone. In this paper, we explore
the challenge of biometric verification in such avatar-mediated scenarios. Our
main question is whether an individual's facial motion patterns can serve as
reliable behavioral biometrics to verify their identity when the avatar's
visual appearance is a facsimile of its owner. To answer this question, we
introduce a new dataset of realistic avatar videos created using a
state-of-the-art one-shot avatar generation model, GAGAvatar, with genuine and
impostor avatar videos. We also propose a lightweight, explainable
spatio-temporal Graph Convolutional Network architecture with temporal
attention pooling, that uses only facial landmarks to model dynamic facial
gestures. Experimental results demonstrate that facial motion cues enable
meaningful identity verification with AUC values approaching 80%. The proposed
benchmark and biometric system are available for the research community in
order to bring attention to the urgent need for more advanced behavioral
biometric defenses in avatar-based communication systems.

</details>


### [85] [SU-ESRGAN: Semantic and Uncertainty-Aware ESRGAN for Super-Resolution of Satellite and Drone Imagery with Fine-Tuning for Cross Domain Evaluation](https://arxiv.org/abs/2508.00750)
*Prerana Ramkumar*

Main category: cs.CV

TL;DR: 提出了SU-ESRGAN超分辨率框架，以改善遥感应用中图像的语义一致性与不确定性评估。


<details>
  <summary>Details</summary>
Motivation: 现有GAN技术在图像超分辨率方面缺乏语义一致性和每像素置信度，这限制了其在关键遥感应用中的可信度。

Method: 引入ESRGAN与DeepLabv3结合的分割损失及Monte Carlo dropout来生成像素级不确定性图。

Result: SU-ESRGAN在PSNR、SSIM、LPIPS指标上与基线ESRGAN相当，且在不同领域应用中表现出色。

Conclusion: SU-ESRGAN在卫星图像的超分辨率任务中表现出色，特别是在语义一致性和不确定性评估方面。

Abstract: Generative Adversarial Networks (GANs) have achieved realistic
super-resolution (SR) of images however, they lack semantic consistency and
per-pixel confidence, limiting their credibility in critical remote sensing
applications such as disaster response, urban planning and agriculture. This
paper introduces Semantic and Uncertainty-Aware ESRGAN (SU-ESRGAN), the first
SR framework designed for satellite imagery to integrate the ESRGAN,
segmentation loss via DeepLabv3 for class detail preservation and Monte Carlo
dropout to produce pixel-wise uncertainty maps. The SU-ESRGAN produces results
(PSNR, SSIM, LPIPS) comparable to the Baseline ESRGAN on aerial imagery. This
novel model is valuable in satellite systems or UAVs that use wide
field-of-view (FoV) cameras, trading off spatial resolution for coverage. The
modular design allows integration in UAV data pipelines for on-board or
post-processing SR to enhance imagery resulting due to motion blur, compression
and sensor limitations. Further, the model is fine-tuned to evaluate its
performance on cross domain applications. The tests are conducted on two drone
based datasets which differ in altitude and imaging perspective. Performance
evaluation of the fine-tuned models show a stronger adaptation to the Aerial
Maritime Drone Dataset, whose imaging characteristics align with the training
data, highlighting the importance of domain-aware training in SR-applications.

</details>


### [86] [Sample-Aware Test-Time Adaptation for Medical Image-to-Image Translation](https://arxiv.org/abs/2508.00766)
*Irene Iele,Francesco Di Feola,Valerio Guarrasi,Paolo Soda*

Main category: cs.CV

TL;DR: 提出了一种用于医学图像翻译的测试时间适应框架，通过动态调整模型来处理域转变问题，在多个任务上表现优于基线和现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在处理分布外样本时性能下降的问题，通过动态适应样本特性提高图像翻译模型的鲁棒性。

Method: 引入重建模块量化域转变，并使用动态适应块选择性地修改预训练模型的内部特征，以减少域转变对模型性能的影响。

Result: 在低剂量CT去噪和T1到T2 MRI翻译这两个医学图像翻译任务上，该方法较基线翻译模型和现有TTA方法取得了一致的性能提升。

Conclusion: 提出了一种新的测试时间适应（TTA）框架，可以动态调整图像翻译过程，从而更好地处理医学图像翻译中的域转变问题。

Abstract: Image-to-image translation has emerged as a powerful technique in medical
imaging, enabling tasks such as image denoising and cross-modality conversion.
However, it suffers from limitations in handling out-of-distribution samples
without causing performance degradation. To address this limitation, we propose
a novel Test-Time Adaptation (TTA) framework that dynamically adjusts the
translation process based on the characteristics of each test sample. Our
method introduces a Reconstruction Module to quantify the domain shift and a
Dynamic Adaptation Block that selectively modifies the internal features of a
pretrained translation model to mitigate the shift without compromising the
performance on in-distribution samples that do not require adaptation. We
evaluate our approach on two medical image-to-image translation tasks: low-dose
CT denoising and T1 to T2 MRI translation, showing consistent improvements over
both the baseline translation model without TTA and prior TTA methods. Our
analysis highlights the limitations of the state-of-the-art that uniformly
apply the adaptation to both out-of-distribution and in-distribution samples,
demonstrating that dynamic, sample-specific adjustment offers a promising path
to improve model resilience in real-world scenarios. The code is available at:
https://github.com/cosbidev/Sample-Aware_TTA.

</details>


### [87] [Zero-Shot Anomaly Detection with Dual-Branch Prompt Learning](https://arxiv.org/abs/2508.00777)
*Zihan Wang,Samira Ebrahimi Kahou,Narges Armanfard*

Main category: cs.CV

TL;DR: PILOT 框架通过双分支提示学习和无标签适应策略，在领域转移的异常检测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本异常检测方法，在领域转移时表现不佳，因为其训练数据来源于有限的训练领域，无法很好地推广到新的分布。

Method: 引入PILOT框架，通过双分支提示学习机制和无标签测试时适应策略，动态集成可学习提示和结构化语义属性，更新提示参数以适应新输入。

Result: 在13个工业和医疗基准测试中，PILOT在异常检测和定位中表现达到最新的技术水平，特别是在领域转移下。

Conclusion: 提出的PILOT框架解决了现有方法在领域转移中的局限性，通过无标签测试时间适应，显著提升了检测性能。

Abstract: Zero-shot anomaly detection (ZSAD) enables identifying and localizing defects
in unseen categories by relying solely on generalizable features rather than
requiring any labeled examples of anomalies. However, existing ZSAD methods,
whether using fixed or learned prompts, struggle under domain shifts because
their training data are derived from limited training domains and fail to
generalize to new distributions. In this paper, we introduce PILOT, a framework
designed to overcome these challenges through two key innovations: (1) a novel
dual-branch prompt learning mechanism that dynamically integrates a pool of
learnable prompts with structured semantic attributes, enabling the model to
adaptively weight the most relevant anomaly cues for each input image; and (2)
a label-free test-time adaptation strategy that updates the learnable prompt
parameters using high-confidence pseudo-labels from unlabeled test data.
Extensive experiments on 13 industrial and medical benchmarks demonstrate that
PILOT achieves state-of-the-art performance in both anomaly detection and
localization under domain shift.

</details>


### [88] [Cross-Dataset Semantic Segmentation Performance Analysis: Unifying NIST Point Cloud City Datasets for 3D Deep Learning](https://arxiv.org/abs/2508.00822)
*Alexander Nikitas Dimopoulos,Joseph Grasso*

Main category: cs.CV

TL;DR: 研究异构标注点云数据集的语义分割性能差异，结果表明，大型几何对象较易识别，而小型关键特征存在识别困难，需标准化标注协议以改善识别性能。


<details>
  <summary>Details</summary>
Motivation: 研究的动机是分析异构标注点云数据集在公共安全应用中语义分割性能的差异，特别是针对激光雷达扫描的预事件规划系统。

Method: 研究采用KPConv架构的分级模式，通过IoU指标评估安全相关特征的语义分割性能。

Result: 结果显示，大型几何对象（如楼梯、窗户）具有较高的分割性能，而较小的安全关键特征识别率较低。表现受类别不平衡和小型物体在典型激光雷达扫描中的几何区分有限影响，显示出当前点云方法在检测某些安全相关特征方面的局限性。

Conclusion: 为了实现可靠的公共安全点云语义分割，需要标准化的标注协议和改进的标注技术，以应对数据异构性和检测小型安全关键元素的挑战。

Abstract: This study analyzes semantic segmentation performance across heterogeneously
labeled point-cloud datasets relevant to public safety applications, including
pre-incident planning systems derived from lidar scans. Using NIST's Point
Cloud City dataset (Enfield and Memphis collections), we investigate challenges
in unifying differently labeled 3D data. Our methodology employs a graded
schema with the KPConv architecture, evaluating performance through IoU metrics
on safety-relevant features. Results indicate performance variability:
geometrically large objects (e.g. stairs, windows) achieve higher segmentation
performance, suggesting potential for navigational context, while smaller
safety-critical features exhibit lower recognition rates. Performance is
impacted by class imbalance and the limited geometric distinction of smaller
objects in typical lidar scans, indicating limitations in detecting certain
safety-relevant features using current point-cloud methods. Key identified
challenges include insufficient labeled data, difficulties in unifying class
labels across datasets, and the need for standardization. Potential directions
include automated labeling and multi-dataset learning strategies. We conclude
that reliable point-cloud semantic segmentation for public safety necessitates
standardized annotation protocols and improved labeling techniques to address
data heterogeneity and the detection of small, safety-critical elements.

</details>


### [89] [IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation](https://arxiv.org/abs/2508.00823)
*Wenxuan Guo,Xiuwei Xu,Hang Yin,Ziwei Wang,Jianjiang Feng,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 提出了IGL-Nav、一种增量式3D高斯定位框架，以应对图像目标导航问题，效果显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法完全建模探索的3D环境与目标图像之间的几何关系，因此需要一种高效且准确的目标图像定位系统。

Method: 介绍了一种名为IGL-Nav的增量式3D高斯定位框架，以增量更新场景表示，并利用几何信息进行离散空间匹配，当代理靠近目标时，通过差异化渲染解决精确目标姿态。

Result: IGL-Nav在各种实验配置中表现优异，能够在自由视角图像目标设置中发挥作用，并可以在真实机器人平台上部署。

Conclusion: IGL-Nav能够在各种实验配置中显著超越现有的方法，并可以处理更具挑战性的自由视角图像目标设置，还能在现实世界的机器人平台上使用手机捕获任意姿态的目标图像。

Abstract: Visual navigation with an image as goal is a fundamental and challenging
problem. Conventional methods either rely on end-to-end RL learning or
modular-based policy with topological graph or BEV map as memory, which cannot
fully model the geometric relationship between the explored 3D environment and
the goal image. In order to efficiently and accurately localize the goal image
in 3D space, we build our navigation system upon the renderable 3D gaussian
(3DGS) representation. However, due to the computational intensity of 3DGS
optimization and the large search space of 6-DoF camera pose, directly
leveraging 3DGS for image localization during agent exploration process is
prohibitively inefficient. To this end, we propose IGL-Nav, an Incremental 3D
Gaussian Localization framework for efficient and 3D-aware image-goal
navigation. Specifically, we incrementally update the scene representation as
new images arrive with feed-forward monocular prediction. Then we coarsely
localize the goal by leveraging the geometric information for discrete space
matching, which can be equivalent to efficient 3D convolution. When the agent
is close to the goal, we finally solve the fine target pose with optimization
via differentiable rendering. The proposed IGL-Nav outperforms existing
state-of-the-art methods by a large margin across diverse experimental
configurations. It can also handle the more challenging free-view image-goal
setting and be deployed on real-world robotic platform using a cellphone to
capture goal image at arbitrary pose. Project page:
https://gwxuan.github.io/IGL-Nav/.

</details>
