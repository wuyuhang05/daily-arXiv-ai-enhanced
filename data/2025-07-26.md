<div id=toc></div>

# Table of Contents

- [cs.IR](#cs.IR) [Total: 9]


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [1] [VERIRAG: Healthcare Claim Verification via Statistical Audit in Retrieval-Augmented Generation](https://arxiv.org/abs/2507.17948)
*Shubham Mohole,Hongjun Choi,Shusen Liu,Christine Klymko,Shashank Kushwaha,Derek Shi,Wesam Sakla,Sainyam Galhotra,Ruben Glatt*

Main category: cs.IR

TL;DR: VERIRAG框架通过评估证据质量和多样性，显著提升了检索增强生成系统在医学支持中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成系统在循证医学中无法评估证据的科学质量。

Method: 提出VERIRAG框架，于源材料应用11点核查清单评估方法学严谨性，结合多样性权重聚合器Hard-to-Vary (HV) 分数及动态接纳阈值。

Result: VERIRAG在四个数据集中的F1分数为0.53至0.65，相对其它方法提升10到14个点。

Conclusion: VERIRAG提升了循证医学的可信度及可靠性，并在各数据集上展现优越性能。

Abstract: Retrieval-augmented generation (RAG) systems are increasingly adopted in
clinical decision support, yet they remain methodologically blind-they retrieve
evidence but cannot vet its scientific quality. A paper claiming "Antioxidant
proteins decreased after alloferon treatment" and a rigorous multi-laboratory
replication study will be treated as equally credible, even if the former
lacked scientific rigor or was even retracted. To address this challenge, we
introduce VERIRAG, a framework that makes three notable contributions: (i) the
Veritable, an 11-point checklist that evaluates each source for methodological
rigor, including data integrity and statistical validity; (ii) a Hard-to-Vary
(HV) Score, a quantitative aggregator that weights evidence by its quality and
diversity; and (iii) a Dynamic Acceptance Threshold, which calibrates the
required evidence based on how extraordinary a claim is. Across four
datasets-comprising retracted, conflicting, comprehensive, and settled science
corpora-the VERIRAG approach consistently outperforms all baselines, achieving
absolute F1 scores ranging from 0.53 to 0.65, representing a 10 to 14 point
improvement over the next-best method in each respective dataset. We will
release all materials necessary for reproducing our results.

</details>


### [2] [Failure Prediction in Conversational Recommendation Systems](https://arxiv.org/abs/2507.17976)
*Maria Vlachou*

Main category: cs.IR

TL;DR: 提出了一种用于对话图像推荐中的性能预测的新方法，实验表明其在系统故障预测中的有效性，但在目录故障预测中存在挑战。


<details>
  <summary>Details</summary>
Motivation: 为了解决建议系统在多轮对话中可能无法返回用户目标项目的问题，尤其是当项目不在目录中时，提出了一种受监督的对话性能预测任务。

Method: 基于AutoEncoder的预测器学习了已检索图像项的多轮语义信息的压缩表示，并使用分类标签来预测评估轮。

Result: 实验表明，提出的预测器在系统失效的场景中表现良好，但在目录失效场景中，预测性能明显下降。

Conclusion: 提出的预测器在系统故障预测中表现良好，而在目录故障预测中表现较差。

Abstract: In a Conversational Image Recommendation task, users can provide natural
language feedback on a recommended image item, which leads to an improved
recommendation in the next turn. While typical instantiations of this task
assume that the user's target item will (eventually) be returned, this might
often not be true, for example, the item the user seeks is not within the item
catalogue. Failing to return a user's desired item can lead to user
frustration, as the user needs to interact with the system for an increased
number of turns. To mitigate this issue, in this paper, we introduce the task
of Supervised Conversational Performance Prediction, inspired by Query
Performance Prediction (QPP) for predicting effectiveness in response to a
search engine query. In this regard, we propose predictors for conversational
performance that detect conversation failures using multi-turn semantic
information contained in the embedded representations of retrieved image items.
Specifically, our AutoEncoder-based predictor learns a compressed
representation of top-retrieved items of the train turns and uses the
classification labels to predict the evaluation turn. Our evaluation scenario
addressed two recommendation scenarios, by differentiating between system
failure, where the system is unable to find the target, and catalogue failure,
where the target does not exist in the item catalogue. In our experiments using
the Shoes and FashionIQ Dresses datasets, we measure the accuracy of predictors
for both system and catalogue failures. Our results demonstrate the promise of
our proposed predictors for predicting system failures (existing evaluation
scenario), while we detect a considerable decrease in predictive performance in
the case of catalogue failure prediction (when inducing a missing item
scenario) compared to system failures.

</details>


### [3] [Fashion-AlterEval: A Dataset for Improved Evaluation of Conversational Recommendation Systems with Alternative Relevant Items](https://arxiv.org/abs/2507.18017)
*Maria Vlachou*

Main category: cs.IR

TL;DR: 引入Fashion-AlterEval数据集和新型模拟器，提高对话推荐系统评估精准度，更好响应用户需求。


<details>
  <summary>Details</summary>
Motivation: 现有用户模拟器的局限性在于只关注单一目标商品，且用户在多个回合中表现出无限耐心，因此需要更真实地模拟用户的行为和偏好。

Method: 提出Fashion-AlterEval数据集，并在现有数据集中增加新注释，开发两种新型元用户模拟器，允许模拟用户表达对替代商品的偏好，并调整其耐心等级进行评价。

Result: 实验表明，利用模拟器获取的替代商品知识可以显著影响现有对话推荐系统模型的评价，现有的单一目标商品评价低估了模型的有效性。通过考虑相关的替代商品，系统可以更迅速地满足用户需求。

Conclusion: 通过使用新的Fashion-AlterEval数据集和两个新型元用户模拟器，可以更准确地评估现有的对话推荐系统模型，更好地满足用户需求。

Abstract: In Conversational Recommendation Systems (CRS), a user provides feedback on
recommended items at each turn, leading the CRS towards improved
recommendations. Due to the need for a large amount of data, a user simulator
is employed for both training and evaluation. Such user simulators critique the
current retrieved item based on knowledge of a single target item. However,
system evaluation in offline settings with simulators is limited by the focus
on a single target item and their unlimited patience over a large number of
turns. To overcome these limitations of existing simulators, we propose
Fashion-AlterEval, a new dataset that contains human judgments for a selection
of alternative items by adding new annotations in common fashion CRS datasets.
Consequently, we propose two novel meta-user simulators that use the collected
judgments and allow simulated users not only to express their preferences about
alternative items to their original target, but also to change their mind and
level of patience. In our experiments using the Shoes and Fashion IQ as the
original datasets and three CRS models, we find that using the knowledge of
alternatives by the simulator can have a considerable impact on the evaluation
of existing CRS models, specifically that the existing single-target evaluation
underestimates their effectiveness, and when simulatedusers are allowed to
instead consider alternative relevant items, the system can rapidly respond to
more quickly satisfy the user.

</details>


### [4] [RecPS: Privacy Risk Scoring for Recommender Systems](https://arxiv.org/abs/2507.18365)
*Jiajie He,Yuechun Gu,Keke Chen*

Main category: cs.IR

TL;DR: 研究提出了RecPS隐私得分方法，用于评估推荐系统中的隐私风险，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 量化推荐系统训练数据的隐私风险是启用隐私感知的推荐系统模型开发和部署的关键步骤。

Method: 提出了一种基于成员推理攻击（MIA）的隐私得分方法，名为RecPS，以衡量交互和用户层面的隐私风险，包括RecLiRA交互级MIA方法以提供高质量的成员预测。

Result: 通过在著名的基准数据集和推荐系统模型上进行大量实验，验证了RecPS得分方法在隐私风险评估和推荐系统模型学习中的独特功能和优势。

Conclusion: 在推荐系统的隐私风险评估和模型学习中，RecPS得分方法展示了其独特的功能和优势。

Abstract: Recommender systems (RecSys) have become an essential component of many web
applications. The core of the system is a recommendation model trained on
highly sensitive user-item interaction data. While privacy-enhancing techniques
are actively studied in the research community, the real-world model
development still depends on minimal privacy protection, e.g., via controlled
access. Users of such systems should have the right to choose \emph{not} to
share highly sensitive interactions. However, there is no method allowing the
user to know which interactions are more sensitive than others. Thus,
quantifying the privacy risk of RecSys training data is a critical step to
enabling privacy-aware RecSys model development and deployment. We propose a
membership-inference attack (MIA)- based privacy scoring method, RecPS, to
measure privacy risks at both the interaction and user levels. The RecPS
interaction-level score definition is motivated and derived from differential
privacy, which is then extended to the user-level scoring method. A critical
component is the interaction-level MIA method RecLiRA, which gives high-quality
membership estimation. We have conducted extensive experiments on well-known
benchmark datasets and RecSys models to show the unique features and benefits
of RecPS scoring in risk assessment and RecSys model unlearning. Our code is
available at https://anonymous.4open.science/r/RsLiRA-4BD3/readme.md.

</details>


### [5] [LLM-based Embedders for Prior Case Retrieval](https://arxiv.org/abs/2507.18455)
*Damith Premasiri,Tharindu Ranasinghe,Ruslan Mitkov*

Main category: cs.IR

TL;DR: 通过使用LLM-based文本嵌入器解决PCR中长度受限和训练数据不足的问题，表现优于传统的BM25和监督型变压器模型。


<details>
  <summary>Details</summary>
Motivation: 在普通法系统中，法律专业人士依赖前例来建立论点。随着案件数量的大量增长，有效检索先前案件变得至关重要。现有的PCR方法大多依赖传统的信息检索方法，如BM25，而深度学习的IR方法在PCR中尚未取得成功。

Method: 利用LLM-based文本嵌入器进行PCR。LLM-based嵌入器支持更长的输入文本长度，无需训练数据，从而解决了输入文本长度受限和缺乏法律训练数据这两个挑战。

Result: 在四个PCR基准数据集上评估了最先进的LLM-based文本嵌入器，结果显示其性能优于BM25和监督型的基于变压器的模型。

Conclusion: LLM-based文本嵌入器在PCR任务中能够克服传统方法的两个关键挑战，并在几个基准数据集上表现出色，优于现有的传统和监督型方法。

Abstract: In common law systems, legal professionals such as lawyers and judges rely on
precedents to build their arguments. As the volume of cases has grown massively
over time, effectively retrieving prior cases has become essential. Prior case
retrieval (PCR) is an information retrieval (IR) task that aims to
automatically identify the most relevant court cases for a specific query from
a large pool of potential candidates. While IR methods have seen several
paradigm shifts over the last few years, the vast majority of PCR methods
continue to rely on traditional IR methods, such as BM25. The state-of-the-art
deep learning IR methods have not been successful in PCR due to two key
challenges: i. Lengthy legal text limitation; when using the powerful
BERT-based transformer models, there is a limit of input text lengths, which
inevitably requires to shorten the input via truncation or division with a loss
of legal context information. ii. Lack of legal training data; due to data
privacy concerns, available PCR datasets are often limited in size, making it
difficult to train deep learning-based models effectively. In this research, we
address these challenges by leveraging LLM-based text embedders in PCR.
LLM-based embedders support longer input lengths, and since we use them in an
unsupervised manner, they do not require training data, addressing both
challenges simultaneously. In this paper, we evaluate state-of-the-art
LLM-based text embedders in four PCR benchmark datasets and show that they
outperform BM25 and supervised transformer-based models.

</details>


### [6] [How Well Do LLMs Predict Prerequisite Skills? Zero-Shot Comparison to Expert-Defined Concepts](https://arxiv.org/abs/2507.18479)
*Ngoc Luyen Le,Marie-Hélène Abel*

Main category: cs.IR

TL;DR: 大型语言模型(LLMs)能够在无需任务特定微调的情况下有效预测前置技能，可能推动个性化学习和智能教学应用的发展。


<details>
  <summary>Details</summary>
Motivation: 前置技能对于支持有效的学习、评估和技能差距分析非常重要。然而，传统上由领域专家策划的方法维护成本高且难以扩展。本研究探讨了大型语言模型能否在无需任务特定微调的情况下，仅使用自然语言描述预测前置技能。

Method: 本研究引入了ESCO-PrereqSkill基准数据集，该数据集由3,196项技能及其专家定义的前置技能链接组成。通过标准化的提示策略，评估了包括GPT-4, Claude 3, Gemini, LLaMA 4, Qwen2, 和DeepSeek在内的13个最新LLM模型，采用语义相似度、BERTScore和推理延迟作为评价标准。

Result: 研究结果显示，模型如LLaMA4-Maverick, Claude-3-7-Sonnet, 和Qwen2-72B能够生成与专家标准高度一致的预测，表现出强大的语义推理能力。

Conclusion: 本研究表明，大型语言模型(LLMs)在无需任务特定微调的情况下，可以在预测前置技能方面表现出色，与专家的定义标准高度一致，显示出在个性化学习、智能辅导和基于技能的推荐系统中的潜力。

Abstract: Prerequisite skills - foundational competencies required before mastering
more advanced concepts - are important for supporting effective learning,
assessment, and skill-gap analysis. Traditionally curated by domain experts,
these relationships are costly to maintain and difficult to scale. This paper
investigates whether large language models (LLMs) can predict prerequisite
skills in a zero-shot setting, using only natural language descriptions and
without task-specific fine-tuning. We introduce ESCO-PrereqSkill, a benchmark
dataset constructed from the ESCO taxonomy, comprising 3,196 skills and their
expert-defined prerequisite links. Using a standardized prompting strategy, we
evaluate 13 state-of-the-art LLMs, including GPT-4, Claude 3, Gemini, LLaMA 4,
Qwen2, and DeepSeek, across semantic similarity, BERTScore, and inference
latency. Our results show that models such as LLaMA4-Maverick,
Claude-3-7-Sonnet, and Qwen2-72B generate predictions that closely align with
expert ground truth, demonstrating strong semantic reasoning without
supervision. These findings highlight the potential of LLMs to support scalable
prerequisite skill modeling for applications in personalized learning,
intelligent tutoring, and skill-based recommender systems.

</details>


### [7] [The Best is Yet to Come: Graph Convolution in the Testing Phase for Multimodal Recommendation](https://arxiv.org/abs/2507.18489)
*Jinfeng Xu,Zheyu Chen,Shuo Yang,Jinze Li,Edith C. H. Ngai*

Main category: cs.IR

TL;DR: 提出了一种高效的多模态推荐框架FastMMRec，仅在测试阶段使用图卷积，以提高效率和可扩展性，缓解模态隔离问题，并优于其他基准模型。


<details>
  <summary>Details</summary>
Motivation: 现有的图卷积网络在多模态推荐系统上的训练效率和可扩展性较低，并且加剧了不同模态之间的隔离，导致推荐精度下降。

Method: 在测试阶段部署图卷积，而不是在训练阶段。

Result: 采用FastMMRec能够显著提高模型的效率和可扩展性，并缓解模态隔离问题。在三个公共数据集上的实验显示，其性能优于竞争对手。

Conclusion: FastMMRec模型能在保持推荐性能的同时，提高效率和规模化能力。

Abstract: The efficiency and scalability of graph convolution networks (GCNs) in
training recommender systems remain critical challenges, hindering their
practical deployment in real-world scenarios. In the multimodal recommendation
(MMRec) field, training GCNs requires more expensive time and space costs and
exacerbates the gap between different modalities, resulting in sub-optimal
recommendation accuracy. This paper critically points out the inherent
challenges associated with adopting GCNs during the training phase in MMRec,
revealing that GCNs inevitably create unhelpful and even harmful pairs during
model optimization and isolate different modalities. To this end, we propose
FastMMRec, a highly efficient multimodal recommendation framework that deploys
graph convolutions exclusively during the testing phase, bypassing their use in
training. We demonstrate that adopting GCNs solely in the testing phase
significantly improves the model's efficiency and scalability while alleviating
the modality isolation problem often caused by using GCNs during the training
phase. We conduct extensive experiments on three public datasets, consistently
demonstrating the performance superiority of FastMMRec over competitive
baselines while achieving efficiency and scalability.

</details>


### [8] [Transform Before You Query: A Privacy-Preserving Approach for Vector Retrieval with Embedding Space Alignment](https://arxiv.org/abs/2507.18518)
*Ruiqi He,Zekun Fei,Jiaqi Li,Xinyuan Zhu,Biao Yi,Siyi Lv,Weijie Liu,Zheli Liu*

Main category: cs.IR

TL;DR: STEER提供了一种保护隐私的向量检索框架，避免泄露敏感信息，同时保持检索高准确性。


<details>
  <summary>Details</summary>
Motivation: 现有VDB服务提供商使用专有黑箱模型，用户必须通过API暴露原始查询文本，导致金融和医疗等领域的敏感信息泄露。

Method: 使用不同嵌入模型语义空间之间的对齐关系来推导查询文本的近似嵌入，无需修改服务器端，在原始VDB中执行检索。

Result: STEER能够防止专有模型提供者通过嵌入反转攻击恢复查询文本，其召回率在广泛实验中表现出色，召回率达到或超过基线。

Conclusion: STEER能有效保护查询文本隐私，并保持检索精度，实验结果显示其召回率比当前基线高20%。

Abstract: Vector Database (VDB) can efficiently index and search high-dimensional
vector embeddings from unstructured data, crucially enabling fast semantic
similarity search essential for modern AI applications like generative AI and
recommendation systems. Since current VDB service providers predominantly use
proprietary black-box models, users are forced to expose raw query text to them
via API in exchange for the vector retrieval services. Consequently, if query
text involves confidential records from finance or healthcare domains, this
mechanism inevitably leads to critical leakage of user's sensitive information.
To address this issue, we introduce STEER (\textbf{S}ecure \textbf{T}ransformed
\textbf{E}mbedding v\textbf{E}ctor\textbf{ R}etrieval), a private vector
retrieval framework that leverages the alignment relationship between the
semantic spaces of different embedding models to derive approximate embeddings
for the query text. STEER performs the retrieval using the approximate
embeddings within the original VDB and requires no modifications to the server
side. Our theoretical and experimental analyses demonstrate that STEER
effectively safeguards query text privacy while maintaining the retrieval
accuracy. Even though approximate embeddings are approximations of the
embeddings from proprietary models, they still prevent the providers from
recovering the query text through Embedding Inversion Attacks (EIAs). Extensive
experimental results show that Recall@100 of STEER can basically achieve a
decrease of less than 5\%. Furthermore, even when searching within a text
corpus of millions of entries, STEER achieves a Recall@20 accuracy 20\% higher
than current baselines.

</details>


### [9] [DR.EHR: Dense Retrieval for Electronic Health Record with Knowledge Injection and Synthetic Data](https://arxiv.org/abs/2507.18583)
*Zhengyun Zhao,Huaiyuan Ying,Yue Zhong,Sheng Yu*

Main category: cs.IR

TL;DR: 提出了一种名为DR.EHR的新型密集检索模式，专为电子健康记录（EHR）检索而设计，显著提升了检索性能，超越了现有的最先进技术。


<details>
  <summary>Details</summary>
Motivation: 现有的模型在一般领域和生物医学领域由于医学知识不足或训练语料不匹配而效果不佳，因此需要一个专门针对EHR检索的密集检索模型。

Method: 提出了一个两阶段的训练流程，利用MIMIC-IV出院摘要来进行训练。第一阶段涉及医学实体提取和生物医学知识图谱的知识注入；第二阶段使用大型语言模型生成多样的训练数据。

Result: 在CliniQ基准测试中，我们的模型显著优于现有的所有密集检索器，达到了最先进的结果。详细分析证明了我们的模型在各种匹配和查询类型中的优越性，特别是在复杂语义匹配如蕴含和缩写中。

Conclusion: 本文显著提升了EHR检索技术，为临床应用提供了一种强大的解决方案。

Abstract: Electronic Health Records (EHRs) are pivotal in clinical practices, yet their
retrieval remains a challenge mainly due to semantic gap issues. Recent
advancements in dense retrieval offer promising solutions but existing models,
both general-domain and biomedical-domain, fall short due to insufficient
medical knowledge or mismatched training corpora. This paper introduces
\texttt{DR.EHR}, a series of dense retrieval models specifically tailored for
EHR retrieval. We propose a two-stage training pipeline utilizing MIMIC-IV
discharge summaries to address the need for extensive medical knowledge and
large-scale training data. The first stage involves medical entity extraction
and knowledge injection from a biomedical knowledge graph, while the second
stage employs large language models to generate diverse training data. We train
two variants of \texttt{DR.EHR}, with 110M and 7B parameters, respectively.
Evaluated on the CliniQ benchmark, our models significantly outperforms all
existing dense retrievers, achieving state-of-the-art results. Detailed
analyses confirm our models' superiority across various match and query types,
particularly in challenging semantic matches like implication and abbreviation.
Ablation studies validate the effectiveness of each pipeline component, and
supplementary experiments on EHR QA datasets demonstrate the models'
generalizability on natural language questions, including complex ones with
multiple entities. This work significantly advances EHR retrieval, offering a
robust solution for clinical applications.

</details>
